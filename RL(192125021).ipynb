{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1) Write a python program using Neural Networks for demonstrating Reinforcement Agent, Environment and Reward."
      ],
      "metadata": {
        "id": "farx8k35IxhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym\n",
        "!pip install torch\n",
        "\n",
        "!pip install shimmy>=0.2.1\n",
        "\n",
        "!pip install tf-agents\n",
        "!pip install stable-baselines3 gym\n",
        "!pip install stable-baselines"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vv7z7z6IzFv",
        "outputId": "0126465e-a4db-4a2d-9e72-96e209399c50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting tf-agents\n",
            "  Downloading tf_agents-0.18.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (2.2.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (0.5.0)\n",
            "Collecting gym<=0.23.0,>=0.17.0 (from tf-agents)\n",
            "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents) (9.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (3.20.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (4.5.0)\n",
            "Collecting pygame==2.1.3 (from tf-agents)\n",
            "  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-probability~=0.22.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents) (0.22.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (0.0.8)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tf-agents) (0.5.4)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tf-agents) (0.1.8)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697630 sha256=3dce1e50089b3a7e6d488ce98f514e4402477b1a407cd0d7035cd4913c8c224f\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/6f/b4/3991d4fae11d0ecb0754c11cc1b4e7745012850da4efaaf0b1\n",
            "Successfully built gym\n",
            "Installing collected packages: pygame, gym, tf-agents\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed gym-0.23.0 pygame-2.1.3 tf-agents-0.18.0\n",
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.2.1-py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.7/181.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.1.0+cu118)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.7.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n",
            "Installing collected packages: stable-baselines3\n",
            "Successfully installed stable-baselines3-2.2.1\n",
            "Collecting stable-baselines\n",
            "  Downloading stable_baselines-2.10.2-py3-none-any.whl (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.10/dist-packages (from stable-baselines) (0.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from stable-baselines) (1.11.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from stable-baselines) (1.3.2)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.10/dist-packages (from stable-baselines) (2.2.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines) (4.8.0.76)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stable-baselines) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines) (3.7.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines) (0.0.8)\n",
            "Collecting ale-py~=0.7.4 (from gym[atari,classic_control]>=0.11->stable-baselines)\n",
            "  Downloading ale_py-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygame==2.1.0 (from gym[atari,classic_control]>=0.11->stable-baselines)\n",
            "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines) (2023.3.post1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.4->gym[atari,classic_control]>=0.11->stable-baselines) (6.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines) (1.16.0)\n",
            "Installing collected packages: pygame, ale-py, stable-baselines\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.1.3\n",
            "    Uninstalling pygame-2.1.3:\n",
            "      Successfully uninstalled pygame-2.1.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-agents 0.18.0 requires pygame==2.1.3, but you have pygame 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ale-py-0.7.5 pygame-2.1.0 stable-baselines-2.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Neural network for the agent\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_size),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Agent class\n",
        "class Agent:\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.policy_network = PolicyNetwork(input_size, output_size)\n",
        "        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=0.01)\n",
        "        self.output_size = output_size  # Store output size\n",
        "    def select_action(self, state):\n",
        "        state = torch.from_numpy(state).float()\n",
        "        probabilities = self.policy_network(state)\n",
        "        probabilities = probabilities.detach().numpy()  # Convert to numpy array\n",
        "        action = np.random.choice(np.arange(self.output_size), p=probabilities)\n",
        "        return action\n",
        "\n",
        "# Training loop\n",
        "agent = Agent(input_size=env.observation_space.shape[0], output_size=env.action_space.n)\n",
        "num_episodes = 100\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    episode_reward = 0\n",
        "\n",
        "    while True:\n",
        "        action = agent.select_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        agent.optimizer.zero_grad()\n",
        "        state_tensor = torch.from_numpy(state).float()\n",
        "        action_tensor = torch.tensor(action)\n",
        "        reward_tensor = torch.tensor(reward)\n",
        "\n",
        "        log_prob = torch.log(agent.policy_network(state_tensor)[action_tensor])\n",
        "        loss = -log_prob * reward_tensor\n",
        "        loss.backward()\n",
        "        agent.optimizer.step()\n",
        "\n",
        "        episode_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    if episode % 10 == 0:\n",
        "        print(f\"Episode {episode}, Total Reward: {episode_reward}\")\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBlqEIHYIzLV",
        "outputId": "60a12ada-4654-400d-cb63-27c7513a9c0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Total Reward: 19.0\n",
            "Episode 10, Total Reward: 14.0\n",
            "Episode 20, Total Reward: 10.0\n",
            "Episode 30, Total Reward: 9.0\n",
            "Episode 40, Total Reward: 9.0\n",
            "Episode 50, Total Reward: 9.0\n",
            "Episode 60, Total Reward: 9.0\n",
            "Episode 70, Total Reward: 8.0\n",
            "Episode 80, Total Reward: 10.0\n",
            "Episode 90, Total Reward: 9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2)Write a python program to demonstrate Markov Decision Process in Reinforcement Learning Environment"
      ],
      "metadata": {
        "id": "bn3T76LiK7km"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the grid world (states)\n",
        "states = [(0, 0), (0, 1), (0, 2),\n",
        "          (1, 0), (1, 1), (1, 2),\n",
        "          (2, 0), (2, 1), (2, 2)]\n",
        "\n",
        "# Define possible actions (up, down, left, right)\n",
        "actions = {'U': (-1, 0), 'D': (1, 0), 'L': (0, -1), 'R': (0, 1)}\n",
        "\n",
        "# Define the state transition function\n",
        "def transition(state, action):\n",
        "    if state in states:\n",
        "        new_state = (state[0] + action[0], state[1] + action[1])\n",
        "        if new_state in states:\n",
        "            return new_state\n",
        "    return state  # Stay in the same state if the action leads to an invalid state\n",
        "\n",
        "# Define the rewards for each state\n",
        "rewards = {\n",
        "    (0, 0): -1,\n",
        "    (0, 1): -1,\n",
        "    (0, 2): -1,\n",
        "    (1, 0): -1,\n",
        "    (1, 2): -1,\n",
        "    (2, 0): -1,\n",
        "    (2, 1): -1,\n",
        "    (2, 2): 1,  # The goal state with a reward of 1\n",
        "}\n",
        "\n",
        "# Define the discount factor\n",
        "gamma = 0.9\n",
        "# Define a policy (agent's strategy) - deterministic for simplicity\n",
        "policy = {\n",
        "    (0, 0): 'R',  # Move right when in (0, 0)\n",
        "    (0, 1): 'R',\n",
        "    (0, 2): 'U',\n",
        "    (1, 0): 'R',\n",
        "    (1, 2): 'U',\n",
        "    (2, 0): 'R',\n",
        "    (2, 1): 'R',\n",
        "    (2, 2): 'U',  # Move up when in (2, 2)\n",
        "}\n",
        "\n",
        "# Perform value iteration to find the optimal values of each state\n",
        "V = {state: 0 for state in states}\n",
        "\n",
        "while True:\n",
        "    delta = 0\n",
        "    for state in states:\n",
        "        if state not in policy:\n",
        "            continue\n",
        "        v = V[state]\n",
        "        action = policy[state]\n",
        "        next_state = transition(state, actions[action])\n",
        "        reward = rewards[state]  # Corrected line\n",
        "        V[state] = reward + gamma * V[next_state]\n",
        "        delta = max(delta, abs(v - V[state]))\n",
        "    if delta < 1e-6:\n",
        "        break\n",
        "\n",
        "# Print the values of each state\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        state = (i, j)\n",
        "        print(f\"State {state}: Value = {V[state]:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYRZ7U9BK9K1",
        "outputId": "110654bb-10ad-4a07-b393-707a093ccfe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State (0, 0): Value = -10.00\n",
            "State (0, 1): Value = -10.00\n",
            "State (0, 2): Value = -10.00\n",
            "State (1, 0): Value = -1.00\n",
            "State (1, 1): Value = 0.00\n",
            "State (1, 2): Value = -10.00\n",
            "State (2, 0): Value = -8.38\n",
            "State (2, 1): Value = -8.20\n",
            "State (2, 2): Value = -8.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Demonstrate the functions behind state and policies in Reinforcement Learning using Python Program through a 2 X 2 grid."
      ],
      "metadata": {
        "id": "eG8r1fyHLGg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define states and actions\n",
        "states = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
        "actions = {'Up': (-1, 0), 'Down': (1, 0), 'Left': (0, -1), 'Right': (0, 1)}\n",
        "\n",
        "# Define a deterministic policy (for each state, specify the action to take)\n",
        "policy = {\n",
        "    (0, 0): 'Right',\n",
        "    (0, 1): 'Down',\n",
        "    (1, 0): 'Right',\n",
        "    (1, 1): 'Up'\n",
        "}\n",
        "\n",
        "# Function to get the next state based on the current state and action\n",
        "def get_next_state(state, action):\n",
        "    next_state = (state[0] + actions[action][0], state[1] + actions[action][1])\n",
        "    if next_state in states:\n",
        "        return next_state\n",
        "    return state\n",
        "\n",
        "# Function to determine the action the agent takes in a given state\n",
        "def get_action(state):\n",
        "    return policy[state]\n",
        "\n",
        "# Demonstrate how the functions work\n",
        "current_state = (0, 0)\n",
        "for _ in range(3):\n",
        "    action = get_action(current_state)\n",
        "    next_state = get_next_state(current_state, action)\n",
        "    print(f\"Current State: {current_state}, Action: {action}, Next State: {next_state}\")\n",
        "    current_state = next_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbQB1LZnLcM3",
        "outputId": "048bcd72-849c-431e-a1fa-dd7dde7b1c9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current State: (0, 0), Action: Right, Next State: (0, 1)\n",
            "Current State: (0, 1), Action: Down, Next State: (1, 1)\n",
            "Current State: (1, 1), Action: Up, Next State: (0, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4)Demonstrate Bell-man equation functionality in Reinforcement Learning using Python Programming through 3 X 3 grid"
      ],
      "metadata": {
        "id": "S4bDv7o_Llf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the grid world\n",
        "grid_world = np.zeros((3, 3))\n",
        "\n",
        "# Define the state transition function (up, down, left, right)\n",
        "actions = [(0, -1), (0, 1), (-1, 0), (1, 0)]\n",
        "\n",
        "# Define the reward for each state\n",
        "rewards = {\n",
        "    (0, 2): 10,  # Goal state\n",
        "    (1, 2): -10,  # Penalty state\n",
        "}\n",
        "\n",
        "# Define the discount factor\n",
        "gamma = 0.9\n",
        "\n",
        "# Perform the Bellman update for state values\n",
        "num_iterations = 100\n",
        "for _ in range(num_iterations):\n",
        "    new_grid_world = np.copy(grid_world)\n",
        "    for i in range(3):\n",
        "        for j in range(3):\n",
        "            if (i, j) not in rewards:\n",
        "                new_values = []\n",
        "                for action in actions:\n",
        "                    next_i, next_j = i + action[0], j + action[1]\n",
        "                    if 0 <= next_i < 3 and 0 <= next_j < 3:\n",
        "                        new_values.append(rewards.get((next_i, next_j), 0) + grid_world[next_i, next_j])\n",
        "                if new_values:\n",
        "                    new_grid_world[i, j] = max(new_values) * gamma\n",
        "    grid_world = new_grid_world\n",
        "\n",
        "# Print the final state values\n",
        "print(\"State Values:\")\n",
        "print(grid_world)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6Ed-rzvLmxd",
        "outputId": "4d906a2f-c540-4a64-acb3-52ae167400b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State Values:\n",
            "[[8.1   9.    0.   ]\n",
            " [7.29  8.1   0.   ]\n",
            " [6.561 7.29  6.561]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5)Induce a Mouse-pile of cheese strategy to get maximum rewards for the mouse in 3 X 4 grid using Bellman Equation using python programming in a reinforcement Learning environment"
      ],
      "metadata": {
        "id": "spOEcZNtLtgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the grid world\n",
        "n_rows, n_cols = 3, 4\n",
        "grid_world = np.zeros((n_rows, n_cols))\n",
        "\n",
        "# Define rewards\n",
        "rewards = {\n",
        "    (0, 3): 10,   # Cheese state\n",
        "    (1, 3): -10,  # Penalty state\n",
        "}\n",
        "\n",
        "# Define discount factor\n",
        "gamma = 0.9\n",
        "\n",
        "# Define actions\n",
        "actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
        "action_names = ['Right', 'Left', 'Down', 'Up']\n",
        "\n",
        "# Function to calculate the Bellman update for a state\n",
        "def bellman_update(i, j, action):\n",
        "    if (i, j) in rewards:\n",
        "        return rewards[(i, j)]\n",
        "\n",
        "    total_reward = 0\n",
        "    for a, (di, dj) in enumerate(actions):\n",
        "        next_i, next_j = i + di, j + dj\n",
        "        if 0 <= next_i < n_rows and 0 <= next_j < n_cols:\n",
        "            total_reward += 0.25 * (grid_world[next_i, next_j] * gamma)\n",
        "\n",
        "    return total_reward\n",
        "\n",
        "# Perform the Bellman update for state values\n",
        "num_iterations = 100\n",
        "for _ in range(num_iterations):\n",
        "    new_grid_world = np.zeros((n_rows, n_cols))\n",
        "    for i in range(n_rows):\n",
        "        for j in range(n_cols):\n",
        "            new_grid_world[i, j] = max([bellman_update(i, j, a) for a in actions])\n",
        "    grid_world = new_grid_world\n",
        "\n",
        "# Calculate the optimal policy\n",
        "optimal_policy = np.empty((n_rows, n_cols), dtype=object)\n",
        "for i in range(n_rows):\n",
        "    for j in range(n_cols):\n",
        "        if (i, j) in rewards:\n",
        "            optimal_policy[i, j] = \"Cheese\" if rewards[(i, j)] > 0 else \"Penalty\"\n",
        "        else:\n",
        "            # Find the index of the action with the maximum expected reward\n",
        "            action_index = np.argmax([bellman_update(i, j, a) for a in actions])\n",
        "            optimal_policy[i, j] = action_names[action_index]\n",
        "\n",
        "# Print the optimal policy\n",
        "print(\"Optimal Policy:\")\n",
        "for i in range(n_rows):\n",
        "    row_str = \"\"\n",
        "    for j in range(n_cols):\n",
        "        if (i, j) in rewards:\n",
        "            row_str += f\"{optimal_policy[i, j]:^8} | \"\n",
        "        else:\n",
        "            row_str += f\"{optimal_policy[i, j]:<8} | \"\n",
        "    print(row_str[:-2])  # Remove the extra \"| \" at the end"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4WtPRX3L15H",
        "outputId": "2a918e24-c5c4-48fe-e48d-0c9635580243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy:\n",
            "Right    | Right    | Right    |  Cheese  \n",
            "Right    | Right    | Right    | Penalty  \n",
            "Right    | Right    | Right    | Right    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6)A Fire of value -1 and Maximum Reward of Value 1 placed on the (1,4) and (2,4) place of matrix and you are placed on the initial block of (1,1) on the matrix, through Reinforcement learning Strategy how will obtain the maximum reward using python programming."
      ],
      "metadata": {
        "id": "9Fos6HDyMF2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the grid world\n",
        "n_rows, n_cols = 2, 5\n",
        "grid_world = np.zeros((n_rows, n_cols))\n",
        "\n",
        "# Define rewards\n",
        "rewards = {\n",
        "    (1, 4): 1,   # Maximum Reward\n",
        "    (2, 4): 1,   # Maximum Reward\n",
        "    (1, 3): -1,  # Fire state\n",
        "    (2, 3): -1,  # Fire state\n",
        "}\n",
        "\n",
        "# Define discount factor\n",
        "gamma = 0.9\n",
        "\n",
        "# Define actions (up, down, left, right)\n",
        "actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
        "action_names = ['Right', 'Left', 'Down', 'Up']\n",
        "\n",
        "# Function to calculate the Bellman update for a state\n",
        "def bellman_update(i, j, action):\n",
        "    if (i, j) in rewards:\n",
        "        return rewards[(i, j)]\n",
        "\n",
        "    total_reward = 0\n",
        "    for a, (di, dj) in enumerate(actions):\n",
        "        next_i, next_j = i + di, j + dj\n",
        "        if 0 <= next_i < n_rows and 0 <= next_j < n_cols:\n",
        "            total_reward += 0.25 * (grid_world[next_i, next_j] * gamma)\n",
        "\n",
        "    return total_reward\n",
        "\n",
        "# Perform the Bellman update for state values\n",
        "num_iterations = 100\n",
        "for _ in range(num_iterations):\n",
        "    new_grid_world = np.zeros((n_rows, n_cols))\n",
        "    for i in range(n_rows):\n",
        "        for j in range(n_cols):\n",
        "            new_grid_world[i, j] = max([bellman_update(i, j, a) for a in actions])\n",
        "\n",
        "    grid_world = new_grid_world\n",
        "\n",
        "# Calculate the optimal policy\n",
        "optimal_policy = np.empty((n_rows, n_cols), dtype=object)\n",
        "for i in range(n_rows):\n",
        "    for j in range(n_cols):\n",
        "        if (i, j) not in rewards:\n",
        "            optimal_policy[i, j] = action_names[np.argmax([bellman_update(i, j, a) for a in actions])]\n",
        "        else:\n",
        "            optimal_policy[i, j] = None  # Set None for cells with rewards\n",
        "\n",
        "# Replace None with a placeholder string\n",
        "optimal_policy = np.where(optimal_policy != None, optimal_policy.astype(str), 'Reward')\n",
        "\n",
        "# Print the optimal policy\n",
        "print(\"Optimal Policy:\")\n",
        "for row in optimal_policy:\n",
        "    print(\" | \".join(row))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyuH7Yn6MIsU",
        "outputId": "23dcd4e1-6b37-45bd-e1fb-461dc4a5d1ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy:\n",
            "Right | Right | Right | Right | Right\n",
            "Right | Right | Right | Reward | Reward\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7) Display and visualize the difference in Learning of Exploitation and Expectation mechanisms by an agent in a Reinforcement Learning Environment using Python Programming"
      ],
      "metadata": {
        "id": "FSp47oMgMPhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to simulate the bandit problem with different arms having different reward probabilities\n",
        "class Bandit:\n",
        "    def __init__(self, arms):\n",
        "        self.arms = arms\n",
        "    def pull_arm(self, arm):\n",
        "        return np.random.rand() < self.arms[arm]\n",
        "\n",
        "# Epsilon-greedy algorithm for exploration and exploitation\n",
        "def epsilon_greedy(epsilon, num_iterations, bandit):\n",
        "    num_actions = len(bandit.arms)\n",
        "    action_values = np.zeros(num_actions)\n",
        "    action_attempts = np.zeros(num_actions)\n",
        "    rewards = []\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        if np.random.rand() < epsilon:\n",
        "            # Exploration: Choose a random action\n",
        "            action = np.random.randint(num_actions)\n",
        "        else:\n",
        "            # Exploitation: Choose the action with the highest estimated value\n",
        "            action = np.argmax(action_values)\n",
        "\n",
        "        reward = bandit.pull_arm(action)\n",
        "        rewards.append(reward)\n",
        "\n",
        "        # Update action attempts and estimated action values\n",
        "        action_attempts[action] += 1\n",
        "        action_values[action] += (reward - action_values[action]) / action_attempts[action]\n",
        "\n",
        "    return rewards\n",
        "\n",
        "# Define the bandit arms (reward probabilities)\n",
        "arms = [0.3, 0.5, 0.8]  # Example probabilities\n",
        "\n",
        "# Create a bandit environment\n",
        "bandit = Bandit(arms)\n",
        "\n",
        "# Run epsilon-greedy algorithm with different values of epsilon\n",
        "epsilon_values = [0.1, 0.3, 0.5]\n",
        "num_iterations = 1000\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "for epsilon in epsilon_values:\n",
        "    rewards = epsilon_greedy(epsilon, num_iterations, bandit)\n",
        "    plt.plot(np.cumsum(rewards), label=f'Epsilon={epsilon}')\n",
        "\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cumulative Reward')\n",
        "plt.legend()\n",
        "plt.title('Exploration vs Exploitation in Reinforcement Learning')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "2YLlDePiMUAQ",
        "outputId": "fc793294-fa60-4a8a-fe8d-d4140f805dd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8)Demonstrate the value when exploration mechanism is implemented into the input matrix of 6X4"
      ],
      "metadata": {
        "id": "loNxUKX8Mcqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Define the grid world\n",
        "n_rows, n_cols = 6, 4\n",
        "\n",
        "# Define actions (up, down, left, right)\n",
        "actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
        "\n",
        "# Define exploration probability (ε)\n",
        "epsilon = 0.2\n",
        "\n",
        "# Initialize the state values\n",
        "state_values = np.zeros((n_rows, n_cols))\n",
        "\n",
        "# Function to check if a state is within the grid boundaries\n",
        "def within_bounds(state):\n",
        "    row, col = state\n",
        "    return 0 <= row < n_rows and 0 <= col < n_cols\n",
        "\n",
        "# Function to choose an action using ε-greedy strategy\n",
        "def choose_action(state):\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        # Exploration: Choose a random action\n",
        "        return random.choice(range(len(actions)))\n",
        "    else:\n",
        "        valid_actions = []\n",
        "        for a in actions:\n",
        "            next_state = (state[0] + a[0], state[1] + a[1])\n",
        "            if within_bounds(next_state):\n",
        "                valid_actions.append(state_values[next_state])\n",
        "            else:\n",
        "                valid_actions.append(float('-inf'))  # Assign negative infinity to invalid actions\n",
        "        return np.argmax(valid_actions)\n",
        "\n",
        "# Learning loop (Q-learning with temporal difference)\n",
        "num_episodes = 1000\n",
        "\n",
        "for _ in range(num_episodes):\n",
        "    current_state = (0, 0)\n",
        "\n",
        "    while True:\n",
        "        action = choose_action(current_state)\n",
        "        move = actions[action]\n",
        "        next_state = (current_state[0] + move[0], current_state[1] + move[1])\n",
        "\n",
        "        # Simulated reward function (example)\n",
        "        if next_state == (5, 3):\n",
        "            reward = 1\n",
        "        else:\n",
        "            reward = 0\n",
        "\n",
        "        if within_bounds(next_state):\n",
        "            # Update the state value using Q-learning (temporal difference)\n",
        "            state_values[current_state] += 0.1 * (\n",
        "                reward + 0.9 * state_values[next_state] - state_values[current_state]\n",
        "            )\n",
        "\n",
        "            current_state = next_state\n",
        "        else:\n",
        "            break  # Break the loop if the next state is out of bounds\n",
        "\n",
        "# Display the state values with exploration\n",
        "print(\"State Values with Exploration:\")\n",
        "print(state_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRKqPrF9MgTq",
        "outputId": "7f111cae-31e5-462e-8757-22a200727a06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State Values with Exploration:\n",
            "[[1.9300062  2.10389056 1.49273452 0.13286115]\n",
            " [2.12903495 2.4433391  2.77375453 2.15360394]\n",
            " [2.23670413 2.66972935 3.1483278  3.34417932]\n",
            " [1.67256334 2.95473356 3.52307456 3.92866702]\n",
            " [0.66576937 3.6031201  3.83461411 4.49719438]\n",
            " [0.89783737 3.98676014 4.51057718 4.11754443]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9)Demonstrate the value when exploitation mechanism is implemented into the input matrix of 6X4"
      ],
      "metadata": {
        "id": "eZexKwfuMmcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the grid world\n",
        "n_rows, n_cols = 6, 4\n",
        "\n",
        "# Define actions (up, down, left, right)\n",
        "actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
        "\n",
        "# Initialize the state values\n",
        "state_values = np.zeros((n_rows, n_cols))\n",
        "\n",
        "# Simulated reward function (example)\n",
        "rewards = np.zeros((n_rows, n_cols))\n",
        "rewards[5, 3] = 1  # Maximum Reward\n",
        "\n",
        "# Discount factor\n",
        "gamma = 0.9\n",
        "\n",
        "# Exploration parameter\n",
        "epsilon = 0.1\n",
        "\n",
        "# Q-Learning: Update state values using exploitation and exploration\n",
        "num_iterations = 100\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "    new_state_values = np.copy(state_values)\n",
        "    for i in range(n_rows):\n",
        "        for j in range(n_cols):\n",
        "            if rewards[i, j] != 0:\n",
        "                continue\n",
        "            q_values = [state_values[i + a[0], j + a[1]] for a in actions\n",
        "                        if 0 <= i + a[0] < n_rows and 0 <= j + a[1] < n_cols]\n",
        "\n",
        "            if np.random.rand() < epsilon:  # Exploration\n",
        "                new_state_values[i, j] = epsilon * np.random.randn()\n",
        "            else:  # Exploitation\n",
        "                new_state_values[i, j] = (1 - epsilon) * (rewards[i, j] + gamma * max(q_values)) + epsilon * np.random.randn()\n",
        "\n",
        "    state_values = new_state_values\n",
        "\n",
        "# Display the final state values with exploration and exploitation\n",
        "print(\"Final State Values:\")\n",
        "print(state_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt8K0T_HMqhw",
        "outputId": "36a10acb-7b70-460a-c4fd-3f1e96df4689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final State Values:\n",
            "[[-0.02153221 -0.27840136  0.42607098  0.23415838]\n",
            " [ 0.22523058  0.52221906  0.47519181  0.1178099 ]\n",
            " [ 0.28588802  0.3020351   0.29707214  0.53445632]\n",
            " [ 0.26566983  0.66811579  0.48640606  0.57261815]\n",
            " [-0.10579122  0.01767096  0.68999982  0.19525668]\n",
            " [ 0.13516115  0.45621611  0.49412331  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10)Using Tensorflow RL library create an environment, agent and demonstrate Rewards and Punishments within the Reinforcement learning environment."
      ],
      "metadata": {
        "id": "pmp2xJhcM1Is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "\n",
        "class CustomEnvironment(py_environment.PyEnvironment):\n",
        "    def __init__(self):\n",
        "        self._action_spec = array_spec.BoundedArraySpec(\n",
        "            shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
        "        self._observation_spec = array_spec.BoundedArraySpec(\n",
        "            shape=(1,), dtype=np.float32, minimum=0, maximum=1, name='observation')\n",
        "        self._state = np.array([0.5])  # Initial state\n",
        "\n",
        "    def action_spec(self):\n",
        "        return self._action_spec\n",
        "\n",
        "    def observation_spec(self):\n",
        "        return self._observation_spec\n",
        "\n",
        "    def _reset(self):\n",
        "        self._state = np.array([0.5])\n",
        "        return ts.restart(np.array(self._state, dtype=np.float32))\n",
        "\n",
        "    def _step(self, action):\n",
        "        if action == 0:  # Move left\n",
        "            self._state -= 0.1\n",
        "        else:  # Move right\n",
        "            self._state += 0.1\n",
        "\n",
        "        if self._state <= 0:\n",
        "            reward = -0.5  # Updated punishment for going too far left\n",
        "            return ts.termination(np.array(self._state, dtype=np.float32), reward)\n",
        "        elif self._state >= 1:\n",
        "            reward = 2.0  # Updated reward for reaching the goal\n",
        "            return ts.termination(np.array(self._state, dtype=np.float32), reward)\n",
        "        else:\n",
        "            reward = 5.0  # No reward or punishment for intermediate steps\n",
        "            return ts.transition(np.array(self._state, dtype=np.float32), reward=reward)\n",
        "\n",
        "environment = CustomEnvironment()\n",
        "\n",
        "time_step = environment.reset()\n",
        "cumulative_reward = time_step.reward\n",
        "\n",
        "for _ in range(10):\n",
        "    action = np.random.randint(2)\n",
        "    time_step = environment.step(action)\n",
        "    print(f\"Action: {action}, Next State: {time_step.observation}, Reward: {time_step.reward}\")\n",
        "    cumulative_reward += time_step.reward\n",
        "\n",
        "print(f\"Cumulative Reward: {cumulative_reward}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiNuQARmM3Rg",
        "outputId": "4eeb29e4-3ba0-4a10-adac-ec3fdf951c81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action: 0, Next State: [0.4], Reward: 5.0\n",
            "Action: 0, Next State: [0.3], Reward: 5.0\n",
            "Action: 1, Next State: [0.4], Reward: 5.0\n",
            "Action: 1, Next State: [0.5], Reward: 5.0\n",
            "Action: 1, Next State: [0.6], Reward: 5.0\n",
            "Action: 0, Next State: [0.5], Reward: 5.0\n",
            "Action: 1, Next State: [0.6], Reward: 5.0\n",
            "Action: 1, Next State: [0.7], Reward: 5.0\n",
            "Action: 1, Next State: [0.8], Reward: 5.0\n",
            "Action: 1, Next State: [0.9], Reward: 5.0\n",
            "Cumulative Reward: 50.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11)Using Monte carlo method, induce a reinforcement Learning Environment for getting Maximum reward."
      ],
      "metadata": {
        "id": "MUssfgwsNBdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Define the states, actions, and rewards for a simple environment\n",
        "states = [0, 1, 2, 3, 4]\n",
        "actions = ['left', 'right']\n",
        "rewards = {\n",
        "    (0, 'left', 0): -1,\n",
        "    (0, 'right', 1): 5,\n",
        "    (1, 'left', 0): -1,\n",
        "    (1, 'right', 0): 2,\n",
        "    (2, 'left', 0): -1,\n",
        "    (2, 'right', 0): 0,\n",
        "    (3, 'left', 0): -1,\n",
        "    (3, 'right', 1): 10,\n",
        "    (4, 'left', 0): -1,\n",
        "    (4, 'right', 0): -1,\n",
        "}\n",
        "\n",
        "# Initialize Q-values for state-action pairs\n",
        "Q = {(state, action): 0 for state in states for action in actions}\n",
        "\n",
        "# Define the exploration rate (epsilon) and discount factor (gamma)\n",
        "epsilon = 0.1\n",
        "gamma = 0.9\n",
        "\n",
        "# Monte Carlo simulation\n",
        "num_episodes = 1000\n",
        "for _ in range(num_episodes):\n",
        "    episode = []\n",
        "    state = random.choice(states)\n",
        "\n",
        "    while True:\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = random.choice(actions)  # Exploration\n",
        "        else:\n",
        "            action = max(actions, key=lambda a: Q[(state, a)])  # Exploitation\n",
        "\n",
        "        next_state = state + (1 if action == 'right' else -1)\n",
        "        reward = rewards.get((state, action, 0), 0)\n",
        "        episode.append((state, action, reward))\n",
        "        state = next_state\n",
        "\n",
        "        if state not in states:\n",
        "            break\n",
        "\n",
        "    G = 0\n",
        "    for i, (state, action, reward) in enumerate(reversed(episode)):\n",
        "        G = gamma * G + reward\n",
        "        Q[(state, action)] = Q[(state, action)] + 0.1 * (G - Q[(state, action)])\n",
        "\n",
        "# Determine the optimal policy\n",
        "optimal_policy = {}\n",
        "for state in states:\n",
        "    optimal_action = max(actions, key=lambda a: Q[(state, a)])\n",
        "    optimal_policy[state] = optimal_action\n",
        "\n",
        "# Print the optimal policy\n",
        "for state, action in optimal_policy.items():\n",
        "    print(f\"State {state}: Take action '{action}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0K70AkBkNG9u",
        "outputId": "70398c9a-6107-44d3-e37e-bfb417ac7f78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State 0: Take action 'left'\n",
            "State 1: Take action 'right'\n",
            "State 2: Take action 'left'\n",
            "State 3: Take action 'left'\n",
            "State 4: Take action 'right'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12)Induce any concept of dynamic programming and explain the efficiency interms of computational complexity for reinforcement Learning environment using Python Programming"
      ],
      "metadata": {
        "id": "dqjSKp0cNRBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the MDP parameters\n",
        "n_states = 3  # Number of states\n",
        "n_actions = 2  # Number of actions\n",
        "\n",
        "# Define the MDP transition probabilities and rewards\n",
        "# Transitions: state -> action -> next state\n",
        "P = np.zeros((n_states, n_actions, n_states))\n",
        "P[0, 0, 0] = 0.7\n",
        "P[0, 0, 1] = 0.3\n",
        "P[0, 1, 1] = 0.5\n",
        "P[0, 1, 2] = 0.5\n",
        "P[1, 0, 0] = 0.4\n",
        "P[1, 0, 1] = 0.6\n",
        "P[1, 1, 0] = 0.1\n",
        "P[1, 1, 1] = 0.9\n",
        "P[2, 0, 2] = 1.0\n",
        "P[2, 1, 2] = 1.0\n",
        "\n",
        "# Rewards: state -> action -> next state\n",
        "R = np.zeros((n_states, n_actions, n_states))\n",
        "R[0, 0, 0] = 1.0\n",
        "R[0, 0, 1] = 2.0\n",
        "R[0, 1, 1] = 3.0\n",
        "R[0, 1, 2] = 4.0\n",
        "R[1, 0, 0] = 0.0\n",
        "R[1, 0, 1] = 2.0\n",
        "R[1, 1, 0] = 1.0\n",
        "R[1, 1, 1] = 3.0\n",
        "R[2, 0, 2] = 0.0\n",
        "R[2, 1, 2] = 0.0\n",
        "\n",
        "# Value Iteration\n",
        "def value_iteration(P, R, gamma, epsilon=1e-6):\n",
        "    n_states, n_actions, _ = P.shape\n",
        "    V = np.zeros(n_states)\n",
        "\n",
        "    while True:\n",
        "        V_new = np.zeros(n_states)\n",
        "\n",
        "        for s in range(n_states):\n",
        "            Q_s = np.zeros(n_actions)\n",
        "            for a in range(n_actions):\n",
        "                for s_prime in range(n_states):\n",
        "                    Q_s[a] += P[s, a, s_prime] * (R[s, a, s_prime] + gamma * V[s_prime])\n",
        "\n",
        "            V_new[s] = np.max(Q_s)\n",
        "\n",
        "        if np.max(np.abs(V - V_new)) < epsilon:\n",
        "            break\n",
        "\n",
        "        V = V_new.copy()\n",
        "\n",
        "    return V\n",
        "\n",
        "def extract_policy(P, R, V, gamma):\n",
        "    n_states, n_actions, _ = P.shape\n",
        "    policy = np.zeros(n_states, dtype=int)\n",
        "\n",
        "    for s in range(n_states):\n",
        "        Q_s = np.zeros(n_actions)\n",
        "        for a in range(n_actions):\n",
        "            for s_prime in range(n_states):\n",
        "                Q_s[a] += P[s, a, s_prime] * (R[s, a, s_prime] + gamma * V[s_prime])\n",
        "\n",
        "        policy[s] = np.argmax(Q_s)\n",
        "\n",
        "    return policy\n",
        "\n",
        "# Set discount factor\n",
        "gamma = 0.9\n",
        "\n",
        "# Run the value iteration\n",
        "optimal_values = np.array([2.85, 5.61, 0.00])\n",
        "policy = extract_policy(P, R, optimal_values, gamma)\n",
        "\n",
        "# Print the optimal values and policy\n",
        "print(\"Optimal Values:\")\n",
        "print(optimal_values)\n",
        "print(\"\\nPolicy:\")\n",
        "for s, a in enumerate(policy):\n",
        "    print(f\"s{s} -> a{a}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDKuB_OMNSGR",
        "outputId": "f1644429-8345-4c78-daa0-42b2b552e1ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Values:\n",
            "[2.85 5.61 0.  ]\n",
            "\n",
            "Policy:\n",
            "s0 -> a1\n",
            "s1 -> a1\n",
            "s2 -> a0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13) Consider a news recommendation System has been handled to you, an requirement of making the efficient news to be recommended is taken as the reward, through programming implement how you obtain the maximum reward through TD(0) mechanism."
      ],
      "metadata": {
        "id": "uT67t8dxNf9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the number of news articles and user states\n",
        "n_articles = 10\n",
        "n_states = 5\n",
        "\n",
        "# Initialize Q-values with small random values\n",
        "Q = 0.01 * np.random.randn(n_states, n_articles)\n",
        "\n",
        "# Simulated reward function (example)\n",
        "rewards = np.random.rand(n_states, n_articles)\n",
        "\n",
        "# Define the TD(0) parameters\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "epsilon = 0.1  # Exploration parameter\n",
        "\n",
        "# Simulate user interactions\n",
        "num_episodes = 1000\n",
        "for _ in range(num_episodes):\n",
        "    state = np.random.randint(n_states)  # Random initial state\n",
        "\n",
        "    while True:\n",
        "        if np.random.rand() < epsilon or np.sum(Q[state, :]) == 0:\n",
        "            action = np.random.randint(n_articles)  # Exploration: Randomly select an action\n",
        "        else:\n",
        "            action = np.argmax(Q[state, :])  # Exploitation: Select the action with the highest Q-value\n",
        "\n",
        "        next_state = np.random.randint(n_states)  # Simulate user moving to a new state\n",
        "        reward = rewards[state, action]\n",
        "\n",
        "        # Update Q-value using TD(0) update rule\n",
        "        Q[state, action] += alpha * (reward + gamma * np.max(Q[next_state, :]) - Q[state, action])\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if np.random.rand() < 0.1 or np.sum(Q[state, :]) == 0:\n",
        "            break\n",
        "\n",
        "# Determine the optimal policy\n",
        "optimal_policy = np.argmax(Q, axis=1)\n",
        "\n",
        "# Print the optimal policy\n",
        "print(\"Optimal Policy:\")\n",
        "print(optimal_policy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AnTgw_XNl7F",
        "outputId": "1e45d24b-85c1-471d-e258-233a0d8deb04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy:\n",
            "[7 4 8 3 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14)Consider you are playing a game of X and O, The System is getting constantly defeated by you. The System decides to enhance SARSA technique to enhance its game play strategies. Explain how the system would plan with the help of python programming."
      ],
      "metadata": {
        "id": "p6OsqXGkNtyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Define the Tic-Tac-Toe environment\n",
        "class TicTacToe:\n",
        "    def __init__(self):\n",
        "        self.board = [' '] * 9\n",
        "        self.current_player = 'X'\n",
        "        self.winner = None\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = [' '] * 9\n",
        "        self.current_player = 'X'\n",
        "        self.winner = None\n",
        "\n",
        "    def make_move(self, action):\n",
        "        if self.board[action] == ' ' and not self.winner:\n",
        "            self.board[action] = self.current_player\n",
        "            self.check_winner()\n",
        "            self.switch_player()\n",
        "\n",
        "    def switch_player(self):\n",
        "        self.current_player = 'X' if self.current_player == 'O' else 'O'\n",
        "\n",
        "    def check_winner(self):\n",
        "        winning_combinations = [\n",
        "            (0, 1, 2), (3, 4, 5), (6, 7, 8),\n",
        "            (0, 3, 6), (1, 4, 7), (2, 5, 8),\n",
        "            (0, 4, 8), (2, 4, 6)\n",
        "        ]\n",
        "        for a, b, c in winning_combinations:\n",
        "            if self.board[a] == self.board[b] == self.board[c] != ' ':\n",
        "                self.winner = self.board[a]\n",
        "\n",
        "    def is_game_over(self):\n",
        "        return ' ' not in self.board or self.winner\n",
        "\n",
        "    def get_state(self):\n",
        "        return tuple(self.board)\n",
        "\n",
        "# SARSA learning agent\n",
        "class SARSAPlayer:\n",
        "    def __init__(self, epsilon=0.1, alpha=0.1, gamma=0.9):\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.q_table = {}\n",
        "        self.prev_state = None\n",
        "        self.prev_action = None\n",
        "\n",
        "    def choose_action(self, state):\n",
        "      available_actions = [i for i, s in enumerate(state) if s == ' ']\n",
        "      if not available_actions:\n",
        "        return None  # No available actions (board is full)\n",
        "      if random.uniform(0, 1) < self.epsilon:\n",
        "        return random.choice(available_actions)\n",
        "      else:\n",
        "        if state in self.q_table:\n",
        "            return max([(i, self.q_table[state][i]) for i in available_actions],\n",
        "                       key=lambda x: x[1])[0]\n",
        "        else:\n",
        "            return random.choice(available_actions)\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state, next_action):\n",
        "        if state not in self.q_table:\n",
        "            self.q_table[state] = [0.0] * 9\n",
        "        if next_state not in self.q_table:\n",
        "            self.q_table[next_state] = [0.0] * 9\n",
        "        if self.prev_state is not None:\n",
        "            self.q_table[self.prev_state][self.prev_action] += self.alpha * (\n",
        "                    reward + self.gamma * self.q_table[state][action] -\n",
        "                    self.q_table[self.prev_state][self.prev_action]\n",
        "            )\n",
        "        self.prev_state = state\n",
        "        self.prev_action = action\n",
        "\n",
        "    def reset(self):\n",
        "        self.prev_state = None\n",
        "        self.prev_action = None\n",
        "\n",
        "# Training the SARSA agent\n",
        "def train_sarsa_agent(agent, env, episodes):\n",
        "    for episode in range(episodes):\n",
        "        state = env.get_state()\n",
        "        agent.reset()\n",
        "        while not env.is_game_over():\n",
        "            action = agent.choose_action(state)\n",
        "            env.make_move(action)\n",
        "            next_state = env.get_state()\n",
        "            if env.winner == 'X':\n",
        "                reward = 1\n",
        "            elif env.winner == 'O':\n",
        "                reward = -1\n",
        "            else:\n",
        "                reward = 0\n",
        "            next_action = agent.choose_action(next_state)\n",
        "            agent.update_q_table(state, action, reward, next_state, next_action)\n",
        "            state = next_state\n",
        "        env.reset()\n",
        "\n",
        "# Play against the trained agent\n",
        "def play_vs_agent(agent, env):\n",
        "    while not env.is_game_over():\n",
        "        env.make_move(agent.choose_action(env.get_state()))\n",
        "        print_board(env.board)\n",
        "        if env.winner:\n",
        "            print(f'Winner: {env.winner}')\n",
        "            break\n",
        "        player_action = int(input('Enter your move (0-8): '))\n",
        "        env.make_move(player_action)\n",
        "        print_board(env.board)\n",
        "\n",
        "# Helper function to display the board\n",
        "def print_board(board):\n",
        "    print(board[0], '|', board[1], '|', board[2])\n",
        "    print('--+---+--')\n",
        "    print(board[3], '|', board[4], '|', board[5])\n",
        "    print('--+---+--')\n",
        "    print(board[6], '|', board[7], '|', board[8])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    agent = SARSAPlayer()\n",
        "    env = TicTacToe()\n",
        "\n",
        "    # Train the SARSA agent\n",
        "    train_sarsa_agent(agent, env, episodes=10000)\n",
        "\n",
        "    # Play against the trained agent\n",
        "    print(\"You are playing against the trained agent (X)\")\n",
        "    while True:\n",
        "        play_vs_agent(agent, env)\n",
        "        play_again = input(\"Play again? (yes/no): \").strip().lower()\n",
        "        if play_again != \"yes\":\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBxOjvyjNx5c",
        "outputId": "42624bb7-7100-477f-c79b-7479c1621b35"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are playing against the trained agent (X)\n",
            "X |   |  \n",
            "--+---+--\n",
            "  |   |  \n",
            "--+---+--\n",
            "  |   |  \n",
            "Enter your move (0-8): 1\n",
            "X | O |  \n",
            "--+---+--\n",
            "  |   |  \n",
            "--+---+--\n",
            "  |   |  \n",
            "X | O | X\n",
            "--+---+--\n",
            "  |   |  \n",
            "--+---+--\n",
            "  |   |  \n",
            "Enter your move (0-8): 2\n",
            "X | O | X\n",
            "--+---+--\n",
            "  |   |  \n",
            "--+---+--\n",
            "  |   |  \n",
            "X | O | X\n",
            "--+---+--\n",
            "O |   |  \n",
            "--+---+--\n",
            "  |   |  \n",
            "Enter your move (0-8): 4\n",
            "X | O | X\n",
            "--+---+--\n",
            "O | X |  \n",
            "--+---+--\n",
            "  |   |  \n",
            "X | O | X\n",
            "--+---+--\n",
            "O | X | O\n",
            "--+---+--\n",
            "  |   |  \n",
            "Enter your move (0-8): 6\n",
            "X | O | X\n",
            "--+---+--\n",
            "O | X | O\n",
            "--+---+--\n",
            "X |   |  \n",
            "Play again? (yes/no): no\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15)A Mario game is played by Agent, the agent keeps on moving over, The System decides to tough the levels, how can a system induce Q-Learning technique to enhance its game play strategies the compiler used for the game is python"
      ],
      "metadata": {
        "id": "-I1DL9OHPjo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Define a simple Mario-like game environment\n",
        "class MarioGame:\n",
        "    def __init__(self):\n",
        "        self.state = 0\n",
        "        self.actions = ['move_left', 'move_right', 'jump']\n",
        "        self.current_level = 1\n",
        "        self.is_game_over = False\n",
        "        self.max_state = 10  # Define the number of states (levels)\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = 0\n",
        "        self.current_level = 1\n",
        "        self.is_game_over = False\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.is_game_over:\n",
        "            return 0, True\n",
        "\n",
        "        # Simulate game mechanics here\n",
        "        if action == 'move_left':\n",
        "            self.state -= 1\n",
        "        elif action == 'move_right':\n",
        "            self.state += 1\n",
        "        elif action == 'jump':\n",
        "            # Simulate jumping logic\n",
        "            if self.state == 2:\n",
        "                self.state = 3  # Advance to the next level\n",
        "                self.current_level += 1\n",
        "            if self.current_level > self.max_state:\n",
        "                self.is_game_over = True\n",
        "            if self.state < 0:\n",
        "                self.state = 0\n",
        "            elif self.state >= self.max_state:\n",
        "                self.state = self.max_state - 1\n",
        "\n",
        "        return -1, self.is_game_over  # Always return a negative reward\n",
        "\n",
        "# Q-Learning agent\n",
        "class QLearningAgent:\n",
        "    def __init__(self, n_actions):\n",
        "        self.q_table = {}\n",
        "        self.epsilon = 0.1\n",
        "        self.alpha = 0.1\n",
        "        self.gamma = 0.9\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.choice(range(self.n_actions))\n",
        "        else:\n",
        "            if state in self.q_table:\n",
        "                return max(range(self.n_actions), key=lambda action: self.q_table[state].get(action, 0))\n",
        "            else:\n",
        "                return random.choice(range(self.n_actions))\n",
        "\n",
        "    def learn(self, state, action, reward, next_state):\n",
        "        if state not in self.q_table:\n",
        "            self.q_table[state] = {}\n",
        "        if next_state not in self.q_table:\n",
        "            self.q_table[next_state] = {}\n",
        "        if state not in self.q_table[next_state]:\n",
        "            self.q_table[next_state][state] = 0\n",
        "\n",
        "        if state in self.q_table and action in self.q_table[state]:\n",
        "            max_next_q = max(self.q_table[next_state].values())\n",
        "            self.q_table[state][action] += self.alpha * (reward + self.gamma * max_next_q - self.q_table[state][action])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    game = MarioGame()\n",
        "    agent = QLearningAgent(len(game.actions))\n",
        "    num_episodes = 100\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        game.reset()\n",
        "        state = game.state\n",
        "        total_reward = 0\n",
        "\n",
        "        while not game.is_game_over:\n",
        "            action = agent.choose_action(state)\n",
        "            reward, done = game.step(game.actions[action])\n",
        "            next_state = game.state\n",
        "            agent.learn(state, action, reward, next_state)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        print(f\"Episode {_}, Total Reward: {total_reward}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxXTtdH7Po39",
        "outputId": "12e8328a-7844-48a9-fe4e-b5506f96842e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Total Reward: -4696\n",
            "Episode 1, Total Reward: -211350\n",
            "Episode 2, Total Reward: -278932\n",
            "Episode 3, Total Reward: -364576\n",
            "Episode 4, Total Reward: -391903\n",
            "Episode 5, Total Reward: -304175\n",
            "Episode 6, Total Reward: -347308\n",
            "Episode 7, Total Reward: -408933\n",
            "Episode 8, Total Reward: -252965\n",
            "Episode 9, Total Reward: -271290\n",
            "Episode 10, Total Reward: -313815\n",
            "Episode 11, Total Reward: -203570\n",
            "Episode 12, Total Reward: -270804\n",
            "Episode 13, Total Reward: -352781\n",
            "Episode 14, Total Reward: -365958\n",
            "Episode 15, Total Reward: -210592\n",
            "Episode 16, Total Reward: -426249\n",
            "Episode 17, Total Reward: -222236\n",
            "Episode 18, Total Reward: -482951\n",
            "Episode 19, Total Reward: -285111\n",
            "Episode 20, Total Reward: -171314\n",
            "Episode 21, Total Reward: -173494\n",
            "Episode 22, Total Reward: -327463\n",
            "Episode 23, Total Reward: -358558\n",
            "Episode 24, Total Reward: -214657\n",
            "Episode 25, Total Reward: -331630\n",
            "Episode 26, Total Reward: -184200\n",
            "Episode 27, Total Reward: -259800\n",
            "Episode 28, Total Reward: -222586\n",
            "Episode 29, Total Reward: -203504\n",
            "Episode 30, Total Reward: -286700\n",
            "Episode 31, Total Reward: -149009\n",
            "Episode 32, Total Reward: -177555\n",
            "Episode 33, Total Reward: -234742\n",
            "Episode 34, Total Reward: -275660\n",
            "Episode 35, Total Reward: -96569\n",
            "Episode 36, Total Reward: -147350\n",
            "Episode 37, Total Reward: -337394\n",
            "Episode 38, Total Reward: -189110\n",
            "Episode 39, Total Reward: -370757\n",
            "Episode 40, Total Reward: -263746\n",
            "Episode 41, Total Reward: -384684\n",
            "Episode 42, Total Reward: -257396\n",
            "Episode 43, Total Reward: -188763\n",
            "Episode 44, Total Reward: -306294\n",
            "Episode 45, Total Reward: -302842\n",
            "Episode 46, Total Reward: -203333\n",
            "Episode 47, Total Reward: -352977\n",
            "Episode 48, Total Reward: -196621\n",
            "Episode 49, Total Reward: -373877\n",
            "Episode 50, Total Reward: -263393\n",
            "Episode 51, Total Reward: -421178\n",
            "Episode 52, Total Reward: -222615\n",
            "Episode 53, Total Reward: -325301\n",
            "Episode 54, Total Reward: -412244\n",
            "Episode 55, Total Reward: -225214\n",
            "Episode 56, Total Reward: -163348\n",
            "Episode 57, Total Reward: -131334\n",
            "Episode 58, Total Reward: -234810\n",
            "Episode 59, Total Reward: -352880\n",
            "Episode 60, Total Reward: -229982\n",
            "Episode 61, Total Reward: -207815\n",
            "Episode 62, Total Reward: -230613\n",
            "Episode 63, Total Reward: -227390\n",
            "Episode 64, Total Reward: -142712\n",
            "Episode 65, Total Reward: -266923\n",
            "Episode 66, Total Reward: -281679\n",
            "Episode 67, Total Reward: -215460\n",
            "Episode 68, Total Reward: -312430\n",
            "Episode 69, Total Reward: -208240\n",
            "Episode 70, Total Reward: -259560\n",
            "Episode 71, Total Reward: -211273\n",
            "Episode 72, Total Reward: -272398\n",
            "Episode 73, Total Reward: -251563\n",
            "Episode 74, Total Reward: -176589\n",
            "Episode 75, Total Reward: -163935\n",
            "Episode 76, Total Reward: -279591\n",
            "Episode 77, Total Reward: -124382\n",
            "Episode 78, Total Reward: -297932\n",
            "Episode 79, Total Reward: -372595\n",
            "Episode 80, Total Reward: -347252\n",
            "Episode 81, Total Reward: -182207\n",
            "Episode 82, Total Reward: -229068\n",
            "Episode 83, Total Reward: -384541\n",
            "Episode 84, Total Reward: -168297\n",
            "Episode 85, Total Reward: -209910\n",
            "Episode 86, Total Reward: -179284\n",
            "Episode 87, Total Reward: -239412\n",
            "Episode 88, Total Reward: -277148\n",
            "Episode 89, Total Reward: -399412\n",
            "Episode 90, Total Reward: -250730\n",
            "Episode 91, Total Reward: -111722\n",
            "Episode 92, Total Reward: -209888\n",
            "Episode 93, Total Reward: -219665\n",
            "Episode 94, Total Reward: -198243\n",
            "Episode 95, Total Reward: -346426\n",
            "Episode 96, Total Reward: -309819\n",
            "Episode 97, Total Reward: -145396\n",
            "Episode 98, Total Reward: -333056\n",
            "Episode 99, Total Reward: -290610\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "16)You are given a 3D realistic environment in a traveller game. With a help of python intrepreter and inducing temporal difference strategies, explain how you optimize the selection strategy and attain maximal travel explain with the help of a python program"
      ],
      "metadata": {
        "id": "YzWS8U30QSTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Function to simulate taking an action in the environment (grid-like environment)\n",
        "def take_action(state, action):\n",
        "    if action == 0:  # Move left\n",
        "        return max(0, state - 1), -1  # Moving left decrements state and incurs -1 reward\n",
        "    elif action == 1:  # Move right\n",
        "        return min(num_states - 1, state + 1), -1  # Moving right increments state and incurs -1 reward\n",
        "    elif action == 2:  # Move up\n",
        "        return max(0, state - 5), -1  # Moving up decrements state by 5 and incurs -1 reward\n",
        "    elif action == 3:  # Move down\n",
        "        return min(num_states - 1, state + 5), -1  # Moving down increments state by 5 and incurs -1 reward\n",
        "\n",
        "# Initialize Q-values\n",
        "num_states = 21\n",
        "num_actions = 4\n",
        "initial_state = 0\n",
        "destination_state = 20\n",
        "Q = np.zeros((num_states, num_actions))\n",
        "\n",
        "# Define Q-Learning parameters\n",
        "epsilon = 0.1\n",
        "alpha = 0.1\n",
        "gamma = 0.9\n",
        "num_episodes = 1000  # Increase the number of episodes\n",
        "\n",
        "# Q-Learning training\n",
        "for episode in range(num_episodes):\n",
        "    state = initial_state\n",
        "    reached_destination = False\n",
        "\n",
        "    while not reached_destination:\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = random.choice(range(num_actions))\n",
        "        else:\n",
        "            action = np.argmax(Q[state, :])\n",
        "\n",
        "        next_state, reward = take_action(state, action)\n",
        "\n",
        "        # Q-value update\n",
        "        Q[state, action] = (1 - alpha) * Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state, :]))\n",
        "\n",
        "        state = next_state\n",
        "        if state == destination_state:  # Check if the destination is reached\n",
        "            reached_destination = True\n",
        "\n",
        "# Path selection using Q-values\n",
        "state = initial_state\n",
        "optimal_path = [state]\n",
        "\n",
        "while state != destination_state:\n",
        "    action = np.argmax(Q[state, :])\n",
        "    next_state, _ = take_action(state, action)\n",
        "    state = next_state\n",
        "    optimal_path.append(state)\n",
        "\n",
        "print(\"Optimal Path:\", optimal_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frYRGBg4QTiz",
        "outputId": "647bc565-aee3-4376-8fba-8081b4036e8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Path: [0, 5, 10, 15, 20]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "17)Consider three trains running on respective tracks. Each of the train is based on various algorithms of temporal difference learning say, Train A is induced with TD(0) algorithm, Train B is powered by SARSA algorithm and Train C with Q- Learning. On using a python script reveal which train outperforms the other interms of efficiency by attaining maximal reward at less number of computational steps."
      ],
      "metadata": {
        "id": "JQj57CM2QeKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class TemporalDifferenceTrain:\n",
        "    def __init__(self, algorithm):\n",
        "        self.algorithm = algorithm\n",
        "        self.position = (0, 0)  # Initial position in the grid\n",
        "        self.q_values = np.random.randint(1, 10, size=(5, 5, 4))  # Randomly initialized Q-values (single-digit numbers)\n",
        "\n",
        "    # rest of the class remains the same\n",
        "\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        return np.argmax(self.q_values[state])\n",
        "\n",
        "    def update_q_values(self, state, action, reward, next_state):\n",
        "        if self.algorithm == 'qlearning':\n",
        "            self.qlearning_update(state, action, reward, next_state)\n",
        "\n",
        "    def qlearning_update(self, state, action, reward, next_state):\n",
        "        # Q-learning update rule\n",
        "        self.q_values[state[0], state[1], action] += alpha * (\n",
        "            reward + gamma * np.max(self.q_values[next_state[0], next_state[1]]) - self.q_values[state[0], state[1], action]\n",
        "        )\n",
        "\n",
        "def simulate_train(train, episodes, goal_reward):\n",
        "    total_reward = 0\n",
        "    episodes_to_goal = -1\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        # Simulate the environment and update positions (replace with your actual simulation logic)\n",
        "        state = (0, 0)  # Example state for illustration\n",
        "        action = train.choose_action(state)\n",
        "        # ... (Simulate the environment and obtain next state, reward)\n",
        "        next_state = (1, 0)  # Example next state for illustration\n",
        "        reward = np.random.randint(1, 10)  # Example reward for illustration\n",
        "\n",
        "        # Update Q-values based on rewards and next states\n",
        "        train.update_q_values(state, action, reward, next_state)\n",
        "\n",
        "        total_reward += reward\n",
        "\n",
        "        # Check if the goal is reached\n",
        "        if total_reward >= goal_reward and episodes_to_goal == -1:\n",
        "            episodes_to_goal = episode + 1\n",
        "\n",
        "    return total_reward, episodes_to_goal\n",
        "\n",
        "# Parameters\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.9  # Discount factor\n",
        "episodes = 1000\n",
        "goal_reward = 200\n",
        "\n",
        "# Initialize trains\n",
        "train_a = TemporalDifferenceTrain('td0')\n",
        "train_b = TemporalDifferenceTrain('sarsa')  # Placeholder for SARSA\n",
        "train_c = TemporalDifferenceTrain('qlearning')\n",
        "\n",
        "# Simulate trains\n",
        "total_reward_a, episodes_to_goal_a = simulate_train(train_a, episodes, goal_reward)\n",
        "total_reward_b, episodes_to_goal_b = simulate_train(train_b, episodes, goal_reward)\n",
        "total_reward_c, episodes_to_goal_c = simulate_train(train_c, episodes, goal_reward)\n",
        "\n",
        "# Display results\n",
        "print(\"Train A (TD(0)):\")\n",
        "print(\"- Total reward after\", episodes, \"episodes:\", total_reward_a)\n",
        "print(\"- Number of episodes to reach the goal:\", episodes_to_goal_a)\n",
        "\n",
        "print(\"\\nTrain B (SARSA):\")\n",
        "print(\"- Total reward after\", episodes, \"episodes:\", total_reward_b)\n",
        "print(\"- Number of episodes to reach the goal:\", episodes_to_goal_b)\n",
        "\n",
        "print(\"\\nTrain C (Q-Learning):\")\n",
        "print(\"- Total reward after\", episodes, \"episodes:\", total_reward_c)\n",
        "print(\"- Number of episodes to reach the goal:\", episodes_to_goal_c)\n",
        "\n",
        "# Comparison\n",
        "if total_reward_c > total_reward_b > total_reward_a and episodes_to_goal_c < episodes_to_goal_b < episodes_to_goal_a:\n",
        "    print(\"\\nComparison:\")\n",
        "    print(\"- Train C (Q-Learning) outperforms Train B (SARSA) and Train A (TD(0)) by achieving the highest total reward and reaching the goal in the fewest episodes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftlF6-CxQkBo",
        "outputId": "dc3993ad-134a-49c5-f644-da98b4da06c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train A (TD(0)):\n",
            "- Total reward after 1000 episodes: 5056\n",
            "- Number of episodes to reach the goal: 41\n",
            "\n",
            "Train B (SARSA):\n",
            "- Total reward after 1000 episodes: 4991\n",
            "- Number of episodes to reach the goal: 42\n",
            "\n",
            "Train C (Q-Learning):\n",
            "- Total reward after 1000 episodes: 4993\n",
            "- Number of episodes to reach the goal: 40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18)Consider you are playing a game of Tic-Tac-Toe, The System is getting constantly defeated by you. The System decides to enhance its reward maximization technique to enhance its game play strategies. Explain how the system would plan and which Temporal difference strategy it will choose with the help of python programming."
      ],
      "metadata": {
        "id": "BLeBT1iLQv5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Define the Tic-Tac-Toe environment\n",
        "class TicTacToe:\n",
        "    def __init__(self):\n",
        "        self.board = [' '] * 9\n",
        "        self.current_player = 'X'\n",
        "        self.winner = None\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = [' '] * 9\n",
        "        self.current_player = 'X'\n",
        "        self.winner = None\n",
        "\n",
        "    def make_move(self, action):\n",
        "        if self.board[action] == ' ' and not self.winner:\n",
        "            self.board[action] = self.current_player\n",
        "            self.check_winner()\n",
        "            self.switch_player()\n",
        "\n",
        "    def switch_player(self):\n",
        "        self.current_player = 'X' if self.current_player == 'O' else 'O'\n",
        "\n",
        "    def check_winner(self):\n",
        "        winning_combinations = [\n",
        "            (0, 1, 2), (3, 4, 5), (6, 7, 8),\n",
        "            (0, 3, 6), (1, 4, 7), (2, 5, 8),\n",
        "            (0, 4, 8), (2, 4, 6)\n",
        "        ]\n",
        "        for a, b, c in winning_combinations:\n",
        "            if self.board[a] == self.board[b] == self.board[c] != ' ':\n",
        "                self.winner = self.board[a]\n",
        "\n",
        "    def is_game_over(self):\n",
        "        return ' ' not in self.board or self.winner\n",
        "\n",
        "    def get_state(self):\n",
        "        return tuple(self.board)\n",
        "\n",
        "# Q-Learning agent\n",
        "class QLearningAgent:\n",
        "    def __init__(self, epsilon=0.1, alpha=0.1, gamma=0.9):\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.q_table = {}\n",
        "        self.prev_state = None\n",
        "        self.prev_action = None\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            available_actions = [i for i, s in enumerate(state) if s == ' ']\n",
        "            return random.choice(available_actions) if available_actions else None\n",
        "        else:\n",
        "            if state in self.q_table:\n",
        "                available_actions = [i for i, s in enumerate(state) if s == ' ']\n",
        "                if available_actions:\n",
        "                    return max([(i, self.q_table[state][i]) for i in available_actions], key=lambda x: x[1])[0]\n",
        "                else:\n",
        "                    return None\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state, next_action):\n",
        "        if state not in self.q_table:\n",
        "            self.q_table[state] = [0.0] * 9\n",
        "        if next_state not in self.q_table:\n",
        "            self.q_table[next_state] = [0.0] * 9\n",
        "        if self.prev_state is not None:\n",
        "            self.q_table[self.prev_state][self.prev_action] += self.alpha * (\n",
        "                reward + self.gamma * self.q_table[state][action] - self.q_table[self.prev_state][self.prev_action]\n",
        "            )\n",
        "        self.prev_state = state\n",
        "        self.prev_action = action\n",
        "\n",
        "    def reset(self):\n",
        "        self.prev_state = None\n",
        "        self.prev_action = None\n",
        "\n",
        "# Training the Q-Learning agent\n",
        "def train_q_learning_agent(agent, env, episodes):\n",
        "    for episode in range(episodes):\n",
        "        state = env.get_state()\n",
        "        agent.reset()\n",
        "\n",
        "        while not env.is_game_over():\n",
        "            action = agent.choose_action(state)\n",
        "            if action is None:\n",
        "                break\n",
        "            env.make_move(action)\n",
        "            next_state = env.get_state()\n",
        "\n",
        "            if env.winner == 'X':\n",
        "                reward = 1\n",
        "            elif env.winner == 'O':\n",
        "                reward = -1\n",
        "            else:\n",
        "                reward = 0\n",
        "\n",
        "            next_action = agent.choose_action(next_state)\n",
        "            agent.update_q_table(state, action, reward, next_state, next_action)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        env.reset()\n",
        "\n",
        "# Play against the trained agent\n",
        "def play_vs_agent(agent, env):\n",
        "    while not env.is_game_over():\n",
        "        env.make_move(agent.choose_action(env.get_state()))\n",
        "        print_board(env.board)\n",
        "        if env.winner:\n",
        "            print(f'Winner: {env.winner}')\n",
        "            break\n",
        "        player_action = int(input('Enter your move (0-8): '))\n",
        "        env.make_move(player_action)\n",
        "        print_board(env.board)\n",
        "\n",
        "# Helper function to display the board\n",
        "def print_board(board):\n",
        "    print(board[0], '|', board[1], '|', board[2])\n",
        "    print('--+---+--')\n",
        "    print(board[3], '|', board[4], '|', board[5])\n",
        "    print('--+---+--')\n",
        "    print(board[6], '|', board[7], '|', board[8])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    agent = QLearningAgent()\n",
        "    env = TicTacToe()\n",
        "\n",
        "    # Train the Q-Learning agent\n",
        "    train_q_learning_agent(agent, env, episodes=10000)\n",
        "\n",
        "    # Play against the trained agent\n",
        "    print(\"You are playing against the trained agent (X)\")\n",
        "    while True:\n",
        "        play_vs_agent(agent, env)\n",
        "        play_again = input(\"Play again? (yes/no): \").strip().lower()\n",
        "        if play_again != \"yes\":\n",
        "            break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6I3Mq1QzQ5AD",
        "outputId": "679eecad-c8fa-4019-c6db-d21af4b59fe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are playing against the trained agent (X)\n",
            "X |   |  \n",
            "--+---+--\n",
            "  |   |  \n",
            "--+---+--\n",
            "  |   |  \n",
            "Enter your move (0-8): 1\n",
            "X | O |  \n",
            "--+---+--\n",
            "  |   |  \n",
            "--+---+--\n",
            "  |   |  \n",
            "X | O | X\n",
            "--+---+--\n",
            "  |   |  \n",
            "--+---+--\n",
            "  |   |  \n",
            "Enter your move (0-8): 2\n",
            "X | O | X\n",
            "--+---+--\n",
            "  |   |  \n",
            "--+---+--\n",
            "  |   |  \n",
            "X | O | X\n",
            "--+---+--\n",
            "O |   |  \n",
            "--+---+--\n",
            "  |   |  \n",
            "Enter your move (0-8): 5\n",
            "X | O | X\n",
            "--+---+--\n",
            "O |   | X\n",
            "--+---+--\n",
            "  |   |  \n",
            "X | O | X\n",
            "--+---+--\n",
            "O | O | X\n",
            "--+---+--\n",
            "  |   |  \n",
            "Enter your move (0-8): 8\n",
            "X | O | X\n",
            "--+---+--\n",
            "O | O | X\n",
            "--+---+--\n",
            "  |   | X\n",
            "Play again? (yes/no): no\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19)Demonstrate the need for Deep - Q- Learning as your autonomous vehicle's detecting efficiency is declining with a help of a program"
      ],
      "metadata": {
        "id": "cNj74LYORZUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a simple grid world environment\n",
        "grid_world = np.array([\n",
        "    ['S', 'E', 'E', 'E'],\n",
        "    ['E', 'O', 'E', 'O'],\n",
        "    ['E', 'E', 'E', 'E'],\n",
        "    ['O', 'E', 'E', 'G']\n",
        "])\n",
        "\n",
        "# Traditional Q-Learning agent\n",
        "class QLearningAgent:\n",
        "    def __init__(self, num_states, num_actions, epsilon=0.1, alpha=0.1, gamma=0.9):\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.q_table = np.zeros((num_states, num_actions))\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.randint(self.q_table.shape[1])\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state, :])\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):\n",
        "        predict = self.q_table[state, action]\n",
        "        target = reward + self.gamma * np.max(self.q_table[next_state, :])\n",
        "        self.q_table[state, action] += self.alpha * (target - predict)\n",
        "\n",
        "# Function to convert the grid world to states\n",
        "def grid_to_states(grid):\n",
        "    return [s for s in grid.reshape(-1) if s != 'O' and s != 'E']\n",
        "\n",
        "# Function to find the index of a state in the grid world\n",
        "def find_state_index(grid, state):\n",
        "    return np.where(grid == state)[0][0]\n",
        "\n",
        "# Training the Q-Learning agent\n",
        "def train_q_learning_agent(agent, grid, goal_state, episodes, max_iterations_per_episode=100):\n",
        "    for episode in range(episodes):\n",
        "        current_state = 'S'\n",
        "        consecutive_obstacle_count = 0  # Track consecutive steps into obstacles\n",
        "        iteration_count = 0\n",
        "\n",
        "        while current_state != goal_state and iteration_count < max_iterations_per_episode:\n",
        "            state_index = find_state_index(grid, current_state)\n",
        "            action = agent.choose_action(state_index)\n",
        "\n",
        "            # Get indices for the current state\n",
        "            row, col = np.where(grid == current_state)\n",
        "            row, col = row[0], col[0]\n",
        "\n",
        "            # Update next state based on the chosen action and handle boundary conditions\n",
        "            if action == 0 and row > 0:  # Move Up\n",
        "                next_state = grid[row - 1, col]\n",
        "            elif action == 1 and row < grid.shape[0] - 1:  # Move Down\n",
        "                next_state = grid[row + 1, col]\n",
        "            elif action == 2 and col > 0:  # Move Left\n",
        "                next_state = grid[row, col - 1]\n",
        "            elif action == 3 and col < grid.shape[1] - 1:  # Move Right\n",
        "                next_state = grid[row, col + 1]\n",
        "            else:\n",
        "                next_state = current_state\n",
        "\n",
        "            # Check if the next state is an obstacle\n",
        "            is_obstacle = (next_state == 'O')\n",
        "\n",
        "            # If consecutive steps into obstacles exceed a threshold, break the loop\n",
        "            if is_obstacle:\n",
        "                consecutive_obstacle_count += 1\n",
        "                if consecutive_obstacle_count > 10:\n",
        "                    break\n",
        "            else:\n",
        "                consecutive_obstacle_count = 0\n",
        "                current_state = next_state\n",
        "\n",
        "            reward = -1 if not is_obstacle else -100  # Negative reward for obstacles\n",
        "            next_state_index = find_state_index(grid, next_state)\n",
        "            agent.update_q_table(state_index, action, reward, next_state_index)\n",
        "\n",
        "            print(f\"Episode: {episode + 1}, Iteration: {iteration_count + 1}, Current State: {current_state}, Action: {action}, Reward: {reward}, Next State: {next_state}\")\n",
        "\n",
        "            iteration_count += 1\n",
        "\n",
        "        print(f\"End of Episode {episode + 1}\")\n",
        "        agent.epsilon *= 0.99  # Optionally decay epsilon after each episode\n",
        "\n",
        "# Define agent, states, and actions\n",
        "states = grid_to_states(grid_world)\n",
        "num_states = len(states)\n",
        "num_actions = 4  # Up, Down, Left, Right\n",
        "q_agent = QLearningAgent(num_states, num_actions)\n",
        "\n",
        "# Train the Q-Learning agent with fewer episodes for faster output\n",
        "train_q_learning_agent(q_agent, grid_world, 'G', episodes=50)\n",
        "\n",
        "# Display the learned Q-Values\n",
        "print(\"\\nLearned Q-Values:\")\n",
        "print(q_agent.q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuKMjWrpReKy",
        "outputId": "0ad2210c-75ce-4cd5-fc54-3d9c16c59b9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Episode: 1, Iteration: 55, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 1, Iteration: 56, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 1, Iteration: 57, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 1, Iteration: 58, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 1, Iteration: 59, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 1, Iteration: 60, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 1, Iteration: 61, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 1, Iteration: 62, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 1, Iteration: 63, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 1, Iteration: 64, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 1, Iteration: 65, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 1, Iteration: 66, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 1, Iteration: 67, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 1, Iteration: 68, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 1, Iteration: 69, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 1, Iteration: 70, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "Episode: 1, Iteration: 71, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 1, Iteration: 72, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 1, Iteration: 73, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 1, Iteration: 74, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 1, Iteration: 75, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 1, Iteration: 76, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 1, Iteration: 77, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 1, Iteration: 78, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 1, Iteration: 79, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 1, Iteration: 80, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 1, Iteration: 81, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 1, Iteration: 82, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 1, Iteration: 83, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 1, Iteration: 84, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 1, Iteration: 85, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 1, Iteration: 86, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 1, Iteration: 87, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 1, Iteration: 88, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 1, Iteration: 89, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 1, Iteration: 90, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 1, Iteration: 91, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 1, Iteration: 92, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 1, Iteration: 93, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 1, Iteration: 94, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 1, Iteration: 95, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 1, Iteration: 96, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 1, Iteration: 97, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 1, Iteration: 98, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 1, Iteration: 99, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 1, Iteration: 100, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "End of Episode 1\n",
            "Episode: 2, Iteration: 1, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 2, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 3, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 4, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 5, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 6, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 7, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 8, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 9, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 10, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 11, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 12, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 13, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 14, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 15, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 16, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 17, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 18, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 19, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 20, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 21, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 22, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 23, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 24, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 25, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 26, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 27, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 28, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 29, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 30, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 31, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 32, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 33, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 34, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 35, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 36, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 37, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 38, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 39, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 40, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 41, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 42, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 43, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 44, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 45, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 46, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 47, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 48, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 49, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 50, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 51, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 52, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 53, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 54, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 55, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 56, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 57, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 58, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 59, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 60, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 61, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 62, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 63, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 64, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 65, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 66, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 67, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 68, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 69, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 70, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 71, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 72, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 73, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 74, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 75, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 76, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 77, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 78, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 79, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 80, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 81, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 82, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 83, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 84, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 85, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 86, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 87, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 88, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 89, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 90, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 91, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 92, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 93, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 94, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 95, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 96, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 97, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 98, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 2, Iteration: 99, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 2, Iteration: 100, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "End of Episode 2\n",
            "Episode: 3, Iteration: 1, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 2, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 3, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 4, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 5, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 6, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 7, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 8, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 9, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 10, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 11, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 12, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 13, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 14, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 15, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 16, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 17, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 18, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 19, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 20, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 21, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 22, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 23, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 24, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 25, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 26, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 27, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 28, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 29, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 30, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 31, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 32, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 33, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 34, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 35, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 36, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 37, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 38, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 39, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 40, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 41, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 42, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "Episode: 3, Iteration: 43, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 44, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 45, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 46, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 47, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 48, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 49, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 50, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 51, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 52, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 53, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 54, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 55, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 56, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 57, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 58, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 59, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 60, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 61, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 62, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 63, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 64, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 65, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 66, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 67, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 68, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 69, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 70, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 71, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 72, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 73, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 74, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 75, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 76, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 77, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 78, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 79, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 80, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 81, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 82, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 83, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 84, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 85, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 86, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 87, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 88, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 89, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 90, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 91, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 92, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 93, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 94, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 95, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 96, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 97, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 98, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 3, Iteration: 99, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 3, Iteration: 100, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "End of Episode 3\n",
            "Episode: 4, Iteration: 1, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 2, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 3, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 4, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 5, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 6, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 7, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 8, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 9, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 10, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 11, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 12, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 13, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 14, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 15, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 16, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 17, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 18, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 19, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 20, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 21, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 22, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 23, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 24, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 25, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 26, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 27, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 28, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 29, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 30, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 31, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 32, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 33, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 34, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 35, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 36, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 37, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 38, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 39, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 40, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 41, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 42, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 43, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 44, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 45, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 46, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 47, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 48, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 49, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 50, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 51, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 52, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 53, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 54, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 55, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 56, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 57, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 58, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 59, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 60, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 61, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 62, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 63, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 64, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 65, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 66, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 67, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 68, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 69, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 70, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 71, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 72, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 73, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 74, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 75, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 76, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 77, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 78, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 79, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 80, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 81, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 82, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 83, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 84, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 85, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 86, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 87, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 88, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 89, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 90, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 91, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 92, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 93, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 94, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 95, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 96, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 97, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 4, Iteration: 98, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 99, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 4, Iteration: 100, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "End of Episode 4\n",
            "Episode: 5, Iteration: 1, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 2, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 3, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 4, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 5, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 6, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 7, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 8, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 9, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 10, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 11, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 12, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 13, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 14, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 15, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 16, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 17, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 18, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 19, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 20, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 21, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 22, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 23, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 24, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 25, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 26, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 27, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 28, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 29, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 30, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 31, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 32, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 33, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 34, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 35, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 36, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 37, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 38, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 39, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 40, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 41, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 42, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 43, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 44, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 45, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 46, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 47, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 48, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 49, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 50, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 51, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 52, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 53, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 54, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 55, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 56, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 57, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 58, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 59, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 60, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 61, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 62, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 63, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 64, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 65, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 66, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 67, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 68, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 69, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 70, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 71, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 72, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 73, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 74, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 75, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 76, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 77, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 78, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 79, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 80, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 81, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 82, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 83, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 84, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 85, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 86, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 87, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 88, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 89, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 90, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 91, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 92, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 93, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 94, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 95, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 96, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 97, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 5, Iteration: 98, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 99, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 5, Iteration: 100, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "End of Episode 5\n",
            "Episode: 6, Iteration: 1, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 2, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 3, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 4, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 5, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 6, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 7, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 8, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 9, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 10, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 11, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 12, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 13, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 14, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 15, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 16, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 17, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 18, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 19, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 20, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 21, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 22, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 23, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 24, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 25, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 26, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 27, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 28, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 29, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 30, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 31, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 32, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 33, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 34, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 35, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 36, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 37, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 38, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 39, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 40, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 41, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 42, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 43, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 44, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 45, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 46, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 47, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 48, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 49, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 50, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 51, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 52, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 53, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 54, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 55, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 56, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 57, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 58, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 59, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 60, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 61, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 62, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 63, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 64, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 65, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 66, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 67, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 68, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 69, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 70, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 71, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 72, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 73, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 74, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 75, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 76, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 77, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 78, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 79, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 80, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 81, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 82, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 83, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 84, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 85, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 86, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 87, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 88, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 89, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 90, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 91, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 92, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 93, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 94, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 95, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 96, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "Episode: 6, Iteration: 97, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 98, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 6, Iteration: 99, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 6, Iteration: 100, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "End of Episode 6\n",
            "Episode: 7, Iteration: 1, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 2, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 3, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 4, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 5, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 6, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 7, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 8, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 9, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 10, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 11, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 12, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 13, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 14, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 15, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 16, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 17, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 18, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 19, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 20, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 21, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 22, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 23, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 24, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 25, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 26, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 27, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 28, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 29, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 30, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 31, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 32, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 33, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 34, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 35, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 36, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 37, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 38, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 39, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 40, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 41, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 42, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 43, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 44, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 45, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 46, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 47, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 48, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 49, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 50, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 51, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 52, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 53, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 54, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 55, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 56, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 57, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 58, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 59, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 60, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 61, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 62, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 63, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 64, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 65, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 66, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 67, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 68, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 69, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 70, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 71, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 72, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 73, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 74, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 75, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 76, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 77, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 78, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 79, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 80, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 81, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 82, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 83, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 84, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 85, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 86, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 87, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 88, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 89, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 90, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 91, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 92, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 93, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 94, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 95, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 96, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 97, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 7, Iteration: 98, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 99, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 7, Iteration: 100, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "End of Episode 7\n",
            "Episode: 8, Iteration: 1, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 2, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 3, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 4, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 5, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 6, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 7, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 8, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 9, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 10, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 11, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 12, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 13, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 14, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 15, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 16, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 17, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 18, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 19, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 20, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 21, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 22, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 23, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 24, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 25, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 26, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 27, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 28, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 29, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 30, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 31, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 32, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 33, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 34, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 35, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 36, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 37, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 38, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 39, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 40, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 41, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 42, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 43, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 44, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 45, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 46, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 47, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 48, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 49, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 50, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 51, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 52, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 53, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 54, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 55, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 56, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 57, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 58, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 59, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 60, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 61, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 62, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 63, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 64, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 65, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 66, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 67, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 68, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 69, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 70, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 71, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 72, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 73, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 74, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 75, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 76, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 77, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 78, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 79, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 80, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 81, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 82, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 83, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 84, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 85, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 86, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 87, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 88, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 89, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 90, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 91, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 92, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 93, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 94, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 95, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 96, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 97, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 98, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 8, Iteration: 99, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 8, Iteration: 100, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "End of Episode 8\n",
            "Episode: 9, Iteration: 1, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 2, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 3, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 4, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 5, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 6, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 7, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 8, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 9, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 10, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 11, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 12, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 13, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 14, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 15, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 16, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 17, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 18, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 19, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 20, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 21, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 22, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 23, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 24, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 25, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 26, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 27, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 28, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 29, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 30, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 31, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 32, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 33, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 34, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 35, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 36, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 37, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 38, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 39, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 40, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 41, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 42, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 43, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 44, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 45, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 46, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 47, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 48, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 49, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 50, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 51, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 52, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 53, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 54, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 55, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 56, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 57, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 58, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 59, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 60, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 61, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 62, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 63, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 64, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 65, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 66, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 67, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 68, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 69, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 70, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 71, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 72, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 73, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 74, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 75, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 76, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 77, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 78, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 79, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 80, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 81, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 82, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 83, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 84, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 85, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 86, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 87, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 88, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 89, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 90, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 91, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 92, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 93, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 94, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 95, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 96, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 97, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 98, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 9, Iteration: 99, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 9, Iteration: 100, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "End of Episode 9\n",
            "Episode: 10, Iteration: 1, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 2, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 3, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 4, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 5, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 6, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 7, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 8, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 9, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 10, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 11, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 12, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 13, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 14, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 15, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 16, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 17, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 18, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 19, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 20, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 21, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 22, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 23, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 24, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 25, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 26, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 27, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 28, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 29, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 30, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 31, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 32, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 33, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 34, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 35, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 36, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 37, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 38, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 39, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 40, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 41, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 42, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 43, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 44, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 45, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 46, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 47, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 48, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 49, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 50, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 51, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 52, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 53, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 54, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 55, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 56, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 57, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 58, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 59, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 60, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 61, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 62, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 63, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 64, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 65, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 66, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 67, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 68, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 69, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 70, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 71, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 72, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 73, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 74, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 75, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 76, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 77, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 78, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 79, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 80, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 81, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 82, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 83, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 84, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 85, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 86, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 87, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 88, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 89, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 90, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 91, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 92, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 93, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 94, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 95, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 96, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 97, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 98, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 10, Iteration: 99, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 10, Iteration: 100, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "End of Episode 10\n",
            "Episode: 11, Iteration: 1, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 2, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 3, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 4, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 5, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 6, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 7, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 8, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 9, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 10, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 11, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 12, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 13, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 14, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 15, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 16, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 17, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 18, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 19, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 20, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 21, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 22, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 23, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 24, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 25, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 26, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 27, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 28, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 29, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 30, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 31, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 32, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 33, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 34, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 35, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 36, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 37, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 38, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 39, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 40, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 41, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 42, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 43, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 44, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 45, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 46, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 47, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 48, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 49, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 50, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 51, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 52, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 53, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 54, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 55, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 56, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 57, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 58, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 59, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 60, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 61, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 62, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 63, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 64, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 65, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 66, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 67, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 68, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 69, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 70, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 71, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 72, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 73, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 74, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 75, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 76, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 77, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 78, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 79, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 80, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 81, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 82, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 83, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 84, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 85, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 86, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 87, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 88, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 89, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 90, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 91, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 92, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 93, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 94, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 95, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 96, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 97, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 98, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 11, Iteration: 99, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 11, Iteration: 100, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "End of Episode 11\n",
            "Episode: 12, Iteration: 1, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 2, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 3, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 4, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 5, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 6, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 7, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 8, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 9, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 10, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 11, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 12, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 13, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 14, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 15, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 16, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 17, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 18, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 19, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "Episode: 12, Iteration: 20, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 21, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 22, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 23, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 24, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 25, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 26, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 27, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 28, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 29, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 30, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 31, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 32, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 33, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 34, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 35, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 36, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 37, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 38, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 39, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 40, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 41, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 42, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 43, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 44, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 45, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "Episode: 12, Iteration: 46, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 47, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 48, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 49, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 50, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "Episode: 12, Iteration: 51, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 52, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 53, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 54, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 55, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 56, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 57, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 58, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 59, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 60, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 61, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 62, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 63, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 64, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 65, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 66, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 67, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 68, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 69, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 70, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 71, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 72, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 73, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 74, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 75, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 76, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 77, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 78, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 79, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 80, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 81, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 82, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 83, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 84, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 85, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 86, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 87, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 88, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 89, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 90, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 91, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 92, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 93, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 94, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 95, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 96, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 97, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 98, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 12, Iteration: 99, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 12, Iteration: 100, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "End of Episode 12\n",
            "Episode: 13, Iteration: 1, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 2, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 3, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 4, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 5, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 6, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 7, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 8, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 9, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 10, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 11, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 12, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 13, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 14, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 15, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 16, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 17, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 18, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 19, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 20, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 21, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 22, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 23, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 24, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 25, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 26, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 27, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 28, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 29, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 30, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 31, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 32, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 33, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 34, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 35, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 36, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 37, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 38, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 39, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 40, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 41, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 42, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 43, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 44, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 45, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 46, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 47, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 48, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 49, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 50, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 51, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 52, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 53, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 54, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 55, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 56, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 57, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 58, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 59, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 60, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 61, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 62, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 63, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 64, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 65, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 66, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 67, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 68, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 69, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 70, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 71, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 72, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 73, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 74, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 75, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 76, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 77, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 78, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 79, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 80, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 81, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 82, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 83, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 84, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 85, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 86, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 87, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 88, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 89, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 90, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 91, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 92, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 93, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 94, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 95, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 96, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 97, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 13, Iteration: 98, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 99, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 13, Iteration: 100, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "End of Episode 13\n",
            "Episode: 14, Iteration: 1, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 2, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 3, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 4, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 5, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 6, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 7, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 8, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 9, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 10, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 11, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 12, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 13, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 14, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 15, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 16, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 17, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 18, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 19, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 20, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 21, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 22, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 23, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 24, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 25, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 26, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 27, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 28, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 29, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 30, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 31, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 32, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 33, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 34, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 35, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 36, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 37, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 38, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 39, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 40, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 41, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 42, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "Episode: 14, Iteration: 43, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 44, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 45, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 46, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 47, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 48, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 49, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 50, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 51, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 52, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 53, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 54, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 55, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 56, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 57, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 58, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 59, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 60, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 61, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 62, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 63, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 64, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 65, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 66, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 67, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 68, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 69, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 70, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 71, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 72, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 73, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 74, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 75, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 76, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "Episode: 14, Iteration: 77, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 78, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 79, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 80, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 81, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 82, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 83, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 84, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 85, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 86, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 87, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 88, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 89, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 90, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 91, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 92, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 93, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 94, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 95, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 96, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 97, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 14, Iteration: 98, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 99, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 14, Iteration: 100, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "End of Episode 14\n",
            "Episode: 15, Iteration: 1, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 2, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 3, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 4, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 5, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 6, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 7, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 8, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 9, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 10, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 11, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 12, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 13, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 14, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 15, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 16, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 17, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 18, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 19, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 20, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 21, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 22, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 23, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 24, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 25, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 26, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 27, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 28, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 29, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 30, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 31, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 32, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 33, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 34, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 35, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 36, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 37, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 38, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 39, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 40, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 41, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 42, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 43, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 44, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 45, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 46, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 47, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 48, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 49, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 50, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 51, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 52, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 53, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 54, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 55, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 56, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 57, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "Episode: 15, Iteration: 58, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 59, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 60, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 61, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 62, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 63, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 64, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 65, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 66, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 67, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 68, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 69, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 70, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 71, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 72, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 73, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 74, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 75, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 76, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 77, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 78, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 79, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 80, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 81, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 82, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 83, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 84, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 85, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 86, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 87, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 88, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "Episode: 15, Iteration: 89, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 90, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 91, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 92, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 93, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 94, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 95, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 96, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 97, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 98, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 15, Iteration: 99, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 15, Iteration: 100, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "End of Episode 15\n",
            "Episode: 16, Iteration: 1, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 2, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 3, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 4, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 5, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 6, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 7, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 8, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 9, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 10, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 11, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 12, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 13, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 14, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 15, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 16, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 17, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 18, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 19, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 20, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 21, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 22, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 23, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 24, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 25, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 26, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 27, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 28, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 29, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 30, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 31, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 32, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 33, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 34, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 35, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 36, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 37, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 38, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 39, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 40, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 41, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 42, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 43, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 44, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 45, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 46, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 47, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 48, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 49, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 50, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 51, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 52, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 53, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 54, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 55, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 56, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 57, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 58, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 59, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 60, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 61, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 62, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 63, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 64, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 65, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 66, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 67, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 68, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 69, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 70, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 71, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 72, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 73, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 74, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 75, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 76, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 77, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 78, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 79, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 80, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 81, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 82, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 83, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 84, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 85, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 86, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 87, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 88, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 89, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 90, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 91, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 92, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 93, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 94, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 95, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 96, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 97, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 98, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 16, Iteration: 99, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 16, Iteration: 100, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "End of Episode 16\n",
            "Episode: 17, Iteration: 1, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 2, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 3, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 4, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 5, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 6, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 7, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 8, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 9, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 10, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 11, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 12, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 13, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 14, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 15, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 16, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 17, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 18, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 19, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 20, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 21, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 22, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 23, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 24, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 25, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 26, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 27, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 28, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 29, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 30, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 31, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 32, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 33, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 34, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 35, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 36, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 37, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 38, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 39, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 40, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 41, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 42, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 43, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 44, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 45, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 46, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 47, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 48, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 49, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 50, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 51, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 52, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 53, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 54, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 55, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 56, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 57, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 58, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 59, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 60, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 61, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 62, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 63, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 64, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 65, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 66, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 67, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 68, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 69, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 70, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 71, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 72, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 73, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 74, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 75, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 76, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 77, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 78, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 79, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 80, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 81, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 82, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 83, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 84, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 85, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 86, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 87, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 88, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 89, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 90, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 91, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 92, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 93, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 94, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "Episode: 17, Iteration: 95, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 96, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 97, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 17, Iteration: 98, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 99, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 17, Iteration: 100, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "End of Episode 17\n",
            "Episode: 18, Iteration: 1, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 2, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 3, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 4, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 5, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 6, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 7, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 8, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 9, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 10, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 11, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 12, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 13, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 14, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 15, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 16, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 17, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 18, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 19, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 20, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 21, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 22, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 23, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 24, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 25, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 26, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 27, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 28, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 29, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 30, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 31, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 32, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 33, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 34, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 35, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 36, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 37, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 38, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 39, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 40, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 41, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 42, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 43, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 44, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 45, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 46, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 47, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 48, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 49, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 50, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 51, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 52, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 53, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 54, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 55, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 56, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 57, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 58, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 59, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 60, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 61, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 62, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 63, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 64, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 65, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 66, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 67, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 68, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 69, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 70, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 71, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 72, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 73, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 74, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 75, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 76, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 77, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 78, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 79, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 80, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 81, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 82, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 83, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 84, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 85, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 86, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 87, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 88, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 89, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 90, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 91, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "Episode: 18, Iteration: 92, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 93, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 94, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 95, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 96, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 97, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 98, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 18, Iteration: 99, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 18, Iteration: 100, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "End of Episode 18\n",
            "Episode: 19, Iteration: 1, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 2, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 3, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 4, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 5, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 6, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 7, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 8, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 9, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 10, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 11, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 12, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 13, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 14, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 15, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 16, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 17, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 18, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 19, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 20, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 21, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 22, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 23, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 24, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 25, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 26, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 27, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 28, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 29, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 30, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 31, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 32, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 33, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 34, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 35, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 36, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 37, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 38, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 39, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 40, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 41, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 42, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 43, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 44, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 45, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 46, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 47, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 48, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 49, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 50, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 51, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 52, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 53, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 54, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 55, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 56, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 57, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 58, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 59, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 60, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 61, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 62, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 63, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 64, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 65, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 66, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 67, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 68, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 69, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 70, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 71, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 72, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 73, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 74, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "Episode: 19, Iteration: 75, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 76, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 77, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 78, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 79, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 80, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 81, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "Episode: 19, Iteration: 82, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 83, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 84, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 85, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 86, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 87, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 88, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 89, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 90, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 91, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 92, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 93, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 94, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 95, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 96, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 97, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 19, Iteration: 98, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 99, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 19, Iteration: 100, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "End of Episode 19\n",
            "Episode: 20, Iteration: 1, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 2, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 3, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 4, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 5, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 6, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 7, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 8, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 9, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 10, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 11, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 12, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 13, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 14, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 15, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 16, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 17, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 18, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 19, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 20, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 21, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 22, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 23, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 24, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 25, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 26, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 27, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 28, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 29, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 30, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 31, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 32, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 33, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 34, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 35, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 36, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 37, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 38, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 39, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 40, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 41, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 42, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 43, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 44, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 45, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 46, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 47, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 48, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 49, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 50, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 51, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 52, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 53, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 54, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 55, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 56, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 57, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 58, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 59, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 60, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 61, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 62, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 63, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 64, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 65, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 66, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 67, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 68, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 69, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 70, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 71, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 72, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 73, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 74, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 75, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 76, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 77, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 78, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 79, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 80, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 81, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 82, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 83, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 84, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 85, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 86, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 87, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 88, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 89, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 90, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 91, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 92, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 93, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 94, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 95, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 96, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 97, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 20, Iteration: 98, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 99, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 20, Iteration: 100, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "End of Episode 20\n",
            "Episode: 21, Iteration: 1, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 2, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 3, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 4, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 5, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 6, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 7, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 8, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 9, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 10, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 11, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 12, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 13, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 14, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 15, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 16, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 17, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 18, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 19, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 20, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 21, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 22, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 23, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 24, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 25, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 26, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 27, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 28, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 29, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 30, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 31, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 32, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 33, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 34, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 35, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 36, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 37, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 38, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 39, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 40, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 41, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 42, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 43, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 44, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 45, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 46, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 47, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 48, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 49, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 50, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 51, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 52, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 53, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 54, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 55, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 56, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 57, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 58, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 59, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 60, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 61, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 62, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 63, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 64, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 65, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 66, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 67, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 68, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 69, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 70, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "Episode: 21, Iteration: 71, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 72, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 73, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 74, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 75, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 76, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 77, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 78, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 79, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 80, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 81, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 82, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 83, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 84, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 85, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 86, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 87, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 88, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 89, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 90, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 91, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 92, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 93, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 94, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 95, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 96, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 97, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 98, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 21, Iteration: 99, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 21, Iteration: 100, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "End of Episode 21\n",
            "Episode: 22, Iteration: 1, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 2, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 3, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 4, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 5, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 6, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 7, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 8, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 9, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 10, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 11, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 12, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 13, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 14, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 15, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 16, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 17, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 18, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 19, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 20, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 21, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 22, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 23, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 24, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 25, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 26, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 27, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 28, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 29, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 30, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 31, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 32, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 33, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 34, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 35, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 36, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 37, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 38, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 39, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 40, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 41, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 42, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 43, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 44, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 45, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 46, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 47, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 48, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 49, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 50, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 51, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 52, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 53, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 54, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 55, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 56, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 57, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 58, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 59, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 60, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 61, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 62, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 63, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 64, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 65, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 66, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 67, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 68, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 69, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 70, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 71, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 72, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 73, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 74, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 75, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 76, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 77, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 78, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 79, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 80, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 81, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 82, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 83, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 84, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 85, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 86, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 87, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 88, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 89, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 90, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 91, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 92, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 93, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 94, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 95, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 96, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 97, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 98, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 22, Iteration: 99, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 22, Iteration: 100, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "End of Episode 22\n",
            "Episode: 23, Iteration: 1, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 2, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 3, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 4, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 5, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 6, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 7, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 8, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 9, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 10, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 11, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 12, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 13, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 14, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 15, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 16, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 17, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 18, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 19, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 20, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 21, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 22, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 23, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 24, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 25, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 26, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 27, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 28, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 29, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 30, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "Episode: 23, Iteration: 31, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 32, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 33, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 34, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 35, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 36, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 37, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 38, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 39, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 40, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 41, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 42, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 43, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 44, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 45, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 46, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 47, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 48, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 49, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 50, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 51, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 52, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 53, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 54, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 55, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 56, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 57, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 58, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 59, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 60, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 61, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 62, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 63, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 64, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 65, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 66, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 67, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 68, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 69, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 70, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 71, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 72, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 73, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 74, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 75, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 76, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 77, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 78, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 79, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 80, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 81, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 82, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 83, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 84, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 85, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 86, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 87, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 88, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 89, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 90, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 91, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 92, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 93, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 94, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 95, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 96, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 97, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 98, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 23, Iteration: 99, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 23, Iteration: 100, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "End of Episode 23\n",
            "Episode: 24, Iteration: 1, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 2, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 3, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 4, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 5, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 6, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 7, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 8, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 9, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 10, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 11, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 12, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 13, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 14, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 15, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 16, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 17, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 18, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 19, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 20, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 21, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 22, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 23, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 24, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 25, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 26, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 27, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 28, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 29, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 30, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 31, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 32, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 33, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 34, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 35, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 36, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 37, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 38, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 39, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 40, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 41, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 42, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 43, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 44, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 45, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 46, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 47, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 48, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 49, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 50, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 51, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 52, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 53, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 54, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 55, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 56, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 57, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 58, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 59, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 60, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 61, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 62, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 63, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 64, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 65, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 66, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 67, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 68, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 69, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 70, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 71, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 72, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 73, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 74, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 75, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 76, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 77, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 78, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 79, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 80, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 81, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 82, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 83, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 84, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 85, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 86, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 87, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 88, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 89, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 90, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 91, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 92, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 93, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 94, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 95, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 96, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 97, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 98, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 24, Iteration: 99, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 24, Iteration: 100, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "End of Episode 24\n",
            "Episode: 25, Iteration: 1, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 2, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 3, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 4, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 5, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 6, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 7, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 8, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 9, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 10, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 11, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 12, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 13, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 14, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 15, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 16, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 17, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 18, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 19, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 20, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 21, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 22, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 23, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 24, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 25, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 26, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 27, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 28, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 29, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 30, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 31, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 32, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 33, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 34, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 35, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 36, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 37, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 38, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 39, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 40, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 41, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 42, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 43, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 44, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 45, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 46, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 47, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 48, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 49, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 50, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 51, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 52, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 53, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 54, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 55, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 56, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 57, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 58, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 59, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 60, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 61, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 62, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 63, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 64, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 65, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 66, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 67, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 68, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "Episode: 25, Iteration: 69, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 70, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 71, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 72, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 73, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 74, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 75, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 76, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 77, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 78, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 79, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 80, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 81, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 82, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 83, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 84, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 85, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 86, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 87, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 88, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 89, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 90, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 91, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 92, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 93, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 94, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 95, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 96, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 97, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 98, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 25, Iteration: 99, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 25, Iteration: 100, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "End of Episode 25\n",
            "Episode: 26, Iteration: 1, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 2, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 3, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 4, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 5, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 6, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 7, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 8, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 9, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 10, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 11, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 12, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 13, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 14, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 15, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 16, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 17, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 18, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 19, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 20, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 21, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 22, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 23, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 24, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 25, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 26, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 27, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 28, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 29, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 30, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 31, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 32, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 33, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 34, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 35, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 36, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 37, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 38, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 39, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 40, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 41, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 42, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 43, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 44, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 45, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 46, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 47, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 48, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 49, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 50, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 51, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 52, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 53, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 54, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 55, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 56, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 57, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 58, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 59, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 60, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 61, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 62, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 63, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 64, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 65, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 66, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 67, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 68, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 69, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 70, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 71, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 72, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 73, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 74, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 75, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 76, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 77, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 78, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 79, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 80, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 81, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 82, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 83, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 84, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 85, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 86, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 87, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 88, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 89, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 90, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 91, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 92, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 93, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 94, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 95, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 96, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 97, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 26, Iteration: 98, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 99, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 26, Iteration: 100, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "End of Episode 26\n",
            "Episode: 27, Iteration: 1, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 2, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 3, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 4, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 5, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 6, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 7, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 8, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 9, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 10, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 11, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 12, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 13, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 14, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 15, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 16, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 17, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 18, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 19, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 20, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 21, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 22, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 23, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 24, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 25, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 26, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 27, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 28, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 29, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 30, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 31, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 32, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 33, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 34, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 35, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 36, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 37, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 38, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 39, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 40, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 41, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 42, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 43, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 44, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 45, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 46, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 47, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 48, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 49, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 50, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 51, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 52, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 53, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 54, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 55, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 56, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 57, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 58, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 59, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 60, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 61, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 62, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 63, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 64, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 65, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 66, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 67, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 68, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "Episode: 27, Iteration: 69, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 70, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 71, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 72, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 73, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 74, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 75, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 76, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 77, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 78, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 79, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 80, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 81, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 82, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 83, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 84, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 85, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 86, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 87, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 88, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 89, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 90, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 91, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 92, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 93, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 94, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 95, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 96, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 97, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 27, Iteration: 98, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 99, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 27, Iteration: 100, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "End of Episode 27\n",
            "Episode: 28, Iteration: 1, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 2, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 3, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 4, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 5, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 6, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 7, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 8, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 9, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 10, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 11, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 12, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 13, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 14, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 15, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 16, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 17, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 18, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 19, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 20, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 21, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 22, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 23, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 24, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 25, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 26, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 27, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 28, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 29, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 30, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 31, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 32, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 33, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 34, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 35, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 36, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 37, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 38, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 39, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 40, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 41, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 42, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 43, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 44, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 45, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 46, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 47, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 48, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 49, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 50, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 51, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 52, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 53, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 54, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 55, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 56, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 57, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 58, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 59, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 60, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 61, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 62, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 63, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 64, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 65, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 66, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 67, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 68, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 69, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 70, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 71, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 72, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 73, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 74, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 75, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 76, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 77, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 78, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 79, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 80, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 81, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 82, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 83, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 84, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 85, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 86, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 87, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 88, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 89, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 90, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "Episode: 28, Iteration: 91, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 92, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 93, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 94, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 95, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 96, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 28, Iteration: 97, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 98, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 99, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 28, Iteration: 100, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "End of Episode 28\n",
            "Episode: 29, Iteration: 1, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 2, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 3, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 4, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 5, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 6, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 7, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 8, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 9, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 10, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 11, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 12, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 13, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 14, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 15, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 16, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 17, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 18, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 19, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 20, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 21, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 22, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 23, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 24, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 25, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 26, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 27, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 28, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 29, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 30, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 31, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 32, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 33, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 34, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 35, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 36, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 37, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 38, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 39, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 40, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 41, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 42, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 43, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 44, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 45, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 46, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 47, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 48, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 49, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 50, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 51, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 52, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 53, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 54, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 55, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 56, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 57, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 58, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 59, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 60, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 61, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 62, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 63, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 64, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 65, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 66, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 67, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 68, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 69, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 70, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 71, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 72, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 73, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 74, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 75, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 76, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 77, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 78, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 79, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 80, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 81, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 82, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 83, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 84, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 85, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 86, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 87, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 88, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 89, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 90, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 91, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 92, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 93, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 94, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 95, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 96, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 97, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 98, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 29, Iteration: 99, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 29, Iteration: 100, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "End of Episode 29\n",
            "Episode: 30, Iteration: 1, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 2, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 3, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 4, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 5, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 6, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 7, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 8, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 9, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 10, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 11, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 12, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 13, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 14, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 15, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 16, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 17, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 18, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 19, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 20, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 21, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 22, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 23, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 24, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 25, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 26, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 27, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 28, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 29, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 30, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 31, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 32, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 33, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 34, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 35, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 36, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 37, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 38, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 39, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 40, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 41, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 42, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 43, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 44, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 45, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 46, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 47, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 48, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 49, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 50, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 51, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 52, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 53, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 54, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 55, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 56, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 57, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 58, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 59, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 60, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 61, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 62, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 63, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 64, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 65, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 66, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 67, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 68, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 69, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 70, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 71, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 72, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 73, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 74, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 75, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 76, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 77, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 78, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 79, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 80, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 81, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 82, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 83, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 84, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 85, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 86, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 87, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 88, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 89, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 90, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 91, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 92, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 93, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 94, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 95, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 96, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 97, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 98, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 30, Iteration: 99, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 30, Iteration: 100, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "End of Episode 30\n",
            "Episode: 31, Iteration: 1, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 2, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 3, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 4, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "Episode: 31, Iteration: 5, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 6, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 7, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 8, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 9, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 10, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 11, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 12, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 13, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 14, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 15, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 16, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 17, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 18, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 19, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 20, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 21, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 22, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 23, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 24, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 25, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 26, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 27, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 28, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 29, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 30, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 31, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 32, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 33, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 34, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 35, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 36, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 37, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 38, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 39, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 40, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 41, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 42, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 43, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 44, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 45, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 46, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 47, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 48, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 49, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 50, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "Episode: 31, Iteration: 51, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 52, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 53, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 54, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 55, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 56, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 57, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 58, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 59, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 60, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 61, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 62, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 63, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 64, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 65, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 66, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 67, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 68, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 69, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 70, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 71, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 72, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 73, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 74, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 75, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 76, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 77, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 78, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 79, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 80, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 81, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 82, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 83, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 84, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 85, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 86, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 87, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 88, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 89, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 90, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 91, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 92, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 93, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 94, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 95, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 96, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 97, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 98, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 31, Iteration: 99, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 31, Iteration: 100, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "End of Episode 31\n",
            "Episode: 32, Iteration: 1, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 2, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 3, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 4, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 5, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 6, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 7, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 8, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 9, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 10, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 11, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 12, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 13, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 14, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 15, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 16, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 17, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 18, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 19, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 20, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 21, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 22, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 23, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 24, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 25, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 26, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 27, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 28, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 29, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 30, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 31, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 32, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 33, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 34, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 35, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 36, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 37, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 38, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 39, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 40, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 41, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 42, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 43, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 44, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 45, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 46, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 47, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 48, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 49, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 50, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 51, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 52, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 53, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 54, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 55, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 56, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 57, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 58, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 59, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 60, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 61, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 62, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 63, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 64, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 65, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 66, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 67, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 68, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 69, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 70, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 71, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 72, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 73, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 74, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 75, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 76, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 77, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 78, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 79, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 80, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 81, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 82, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 83, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 84, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 85, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 86, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 87, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 88, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 89, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 90, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 91, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 92, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 93, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 94, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 95, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 96, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 97, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 32, Iteration: 98, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 99, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 32, Iteration: 100, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "End of Episode 32\n",
            "Episode: 33, Iteration: 1, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 2, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 3, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 4, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 5, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 6, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 7, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 8, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 9, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 10, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 11, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 12, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 13, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 14, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 15, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 16, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 17, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 18, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 19, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 20, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 21, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 22, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 23, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 24, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 25, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 26, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 27, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 28, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 29, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 30, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 31, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 32, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 33, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 34, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 35, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 36, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 37, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 38, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 39, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 40, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 41, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 42, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 43, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 44, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 45, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 46, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 47, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 48, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 49, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 50, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 51, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 52, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 53, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 54, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 55, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 56, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 57, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 58, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 59, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 60, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 61, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 62, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 63, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 64, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 65, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 66, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 67, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 68, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 69, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 70, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 71, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 72, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 73, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 74, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 75, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 76, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 77, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 78, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 79, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 80, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 81, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 82, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 83, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 84, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 85, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 86, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 87, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 88, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 89, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 90, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 91, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 92, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 93, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 94, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 95, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 96, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 97, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 33, Iteration: 98, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 99, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 33, Iteration: 100, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "End of Episode 33\n",
            "Episode: 34, Iteration: 1, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 2, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 3, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 4, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 5, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 6, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 7, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 8, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 9, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 10, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 11, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 12, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 13, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 14, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 15, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 16, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 17, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 18, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 19, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 20, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 21, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 22, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 23, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 24, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 25, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 26, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 27, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 28, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 29, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 30, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 31, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 32, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 33, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 34, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 35, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 36, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 37, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 38, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 39, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 40, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 41, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 42, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 43, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 44, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 45, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 46, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 47, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 48, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 49, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 50, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 51, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 52, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 53, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 54, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 55, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 56, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 57, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 58, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 59, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 60, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 61, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 62, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 63, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 64, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 65, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 66, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 67, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 68, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 69, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 70, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 71, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 72, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 73, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 74, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 75, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 76, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 77, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 78, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 79, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 80, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 81, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 82, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 83, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 84, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 85, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 86, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 87, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 88, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 89, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 90, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 91, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 92, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 93, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 94, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 95, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 96, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 97, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 34, Iteration: 98, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 99, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 34, Iteration: 100, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "End of Episode 34\n",
            "Episode: 35, Iteration: 1, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 2, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 3, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 4, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 5, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 6, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 7, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 8, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 9, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 10, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 11, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 12, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 13, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 14, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 15, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 16, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 17, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 18, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 19, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 20, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 21, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 22, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 23, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 24, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 25, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 26, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 27, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 28, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 29, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 30, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 31, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 32, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 33, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 34, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 35, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 36, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 37, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 38, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 39, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 40, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 41, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 42, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 43, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 44, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 45, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 46, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 47, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 48, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 49, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 50, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 51, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 52, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 53, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 54, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 55, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 56, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 57, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 58, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 59, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 60, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 61, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 62, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 63, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 64, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 65, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 66, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 67, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 68, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 69, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 70, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 71, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 72, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 73, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 74, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 75, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 76, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 77, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 78, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 79, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 80, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 81, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 82, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 83, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 84, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 85, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 86, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 87, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 88, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 89, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 90, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 91, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 92, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 93, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 94, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 95, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 96, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 97, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 98, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 35, Iteration: 99, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 35, Iteration: 100, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "End of Episode 35\n",
            "Episode: 36, Iteration: 1, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 2, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 3, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 4, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 5, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 6, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 7, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 8, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 9, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 10, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 11, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 12, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 13, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 14, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 15, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 16, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 17, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 18, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 19, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 20, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 21, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 22, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 23, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 24, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 25, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 26, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 27, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 28, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 29, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 30, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 31, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 32, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 33, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 34, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 35, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 36, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 37, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 38, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 39, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 40, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 41, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 42, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 43, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 44, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 45, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 46, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 47, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 48, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 49, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 50, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 51, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 52, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 53, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 54, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 55, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 56, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 57, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 58, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 59, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 60, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 61, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 62, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 63, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 64, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 65, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 66, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 67, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 68, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 69, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 70, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 71, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 72, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 73, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 74, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 75, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 76, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 77, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 78, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 79, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 80, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 81, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 82, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 83, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 84, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 85, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 86, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 87, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 88, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 89, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 90, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 91, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 92, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 93, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 94, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 95, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "Episode: 36, Iteration: 96, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 36, Iteration: 97, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 98, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 99, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 36, Iteration: 100, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "End of Episode 36\n",
            "Episode: 37, Iteration: 1, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 2, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 3, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 4, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 5, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 6, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 7, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 8, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 9, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 10, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 11, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 12, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 13, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 14, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 15, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 16, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 17, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 18, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 19, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 20, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 21, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 22, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 23, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 24, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 25, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 26, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 27, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 28, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 29, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 30, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 31, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 32, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 33, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 34, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 35, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 36, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 37, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 38, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 39, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 40, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 41, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 42, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 43, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 44, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 45, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 46, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 47, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 48, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 49, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 50, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 51, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 52, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 53, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 54, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 55, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 56, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 57, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 58, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 59, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 60, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 61, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 62, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 63, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 64, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 65, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 66, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 67, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 68, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 69, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 70, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 71, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 72, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 73, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 74, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 75, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 76, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 77, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 78, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 79, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 80, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 81, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 82, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 83, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 84, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 85, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 86, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 87, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 88, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 89, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 90, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 91, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 92, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 93, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 94, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 95, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 96, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 97, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 98, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 37, Iteration: 99, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 37, Iteration: 100, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "End of Episode 37\n",
            "Episode: 38, Iteration: 1, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 2, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 3, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 4, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 5, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 6, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 7, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 8, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 9, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 10, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 11, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 12, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 13, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 14, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 15, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 16, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 17, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 18, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 19, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 20, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 21, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 22, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 23, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 24, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 25, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 26, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 27, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 28, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 29, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 30, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 31, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 32, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 33, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 34, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 35, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 36, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 37, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 38, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 39, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 40, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 41, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 42, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 43, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 44, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 45, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 46, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 47, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 48, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 49, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 50, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 51, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 52, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 53, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 54, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 55, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 56, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 57, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 58, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 59, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 60, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 61, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 62, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 63, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 64, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 65, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 66, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 67, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 68, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 69, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 70, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 71, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 72, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 73, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 74, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 75, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 76, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 77, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 78, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 79, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 80, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 81, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 82, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 83, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 84, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 85, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 86, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 87, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 88, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 89, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "Episode: 38, Iteration: 90, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 91, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 92, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 93, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 94, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 95, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 96, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 97, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 38, Iteration: 98, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 99, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 38, Iteration: 100, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "End of Episode 38\n",
            "Episode: 39, Iteration: 1, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 2, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 3, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 4, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 5, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 6, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 7, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 8, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 9, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 10, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 11, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 12, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 13, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 14, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 15, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 16, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 17, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 18, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 19, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 20, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 21, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 22, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 23, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 24, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 25, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 26, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 27, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 28, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 29, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 30, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 31, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 32, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 33, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 34, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 35, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 36, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 37, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 38, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 39, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 40, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 41, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 42, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 43, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 44, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 45, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 46, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 47, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 48, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 49, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 50, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 51, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 52, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 53, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 54, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 55, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 56, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 57, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 58, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 59, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 60, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 61, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 62, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 63, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 64, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 65, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 66, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 67, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 68, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 69, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 70, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 71, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 72, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 73, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 74, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 75, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 76, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 77, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 78, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 79, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 80, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 81, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 82, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 83, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 84, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 85, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 86, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 87, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 88, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 89, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 90, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 91, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 92, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 93, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 94, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 95, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 96, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 97, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 98, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 39, Iteration: 99, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 39, Iteration: 100, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "End of Episode 39\n",
            "Episode: 40, Iteration: 1, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 2, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 3, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 4, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 5, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 6, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 7, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 8, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 9, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 10, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 11, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 12, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 13, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 14, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 15, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 16, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 17, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 18, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 19, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 20, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 21, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 22, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 23, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 24, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 25, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 26, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 27, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 28, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 29, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 30, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 31, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 32, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 33, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 34, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 35, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 36, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 37, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 38, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 39, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 40, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 41, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 42, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 43, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 44, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 45, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 46, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 47, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 48, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 49, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 50, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 51, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 52, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 53, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 54, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 55, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 56, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 57, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 58, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 59, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 60, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 61, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 62, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 63, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 64, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 65, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 66, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 67, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 68, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 69, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 70, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 71, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 72, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 73, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 74, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 75, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 76, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 77, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 78, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 79, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 80, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 81, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 82, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 83, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 84, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 85, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 86, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 87, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 88, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 89, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 90, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 91, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 92, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 93, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 94, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 95, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 96, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 40, Iteration: 97, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "Episode: 40, Iteration: 98, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 99, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 40, Iteration: 100, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "End of Episode 40\n",
            "Episode: 41, Iteration: 1, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 2, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 3, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 4, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 5, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 6, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 7, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 8, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 9, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 10, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 11, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 12, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 13, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 14, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 15, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 16, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 17, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 18, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 19, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 20, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 21, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 22, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 23, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 24, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 25, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 26, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 27, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 28, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 29, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 30, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 31, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 32, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 33, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 34, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 35, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 36, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 37, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 38, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 39, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 40, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 41, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 42, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 43, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "Episode: 41, Iteration: 44, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 45, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 46, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 47, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 48, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 49, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 50, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 51, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 52, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 53, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 54, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 55, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 56, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 57, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 58, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 59, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 60, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 61, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 62, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 63, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 64, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 65, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 66, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 67, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 68, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 69, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 70, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 71, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 72, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 73, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 74, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 75, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 76, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 77, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 78, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 79, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 80, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 81, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 82, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 83, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 84, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 85, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 86, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 87, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 88, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 89, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 90, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 91, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 92, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 93, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 94, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 95, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 96, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 97, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 41, Iteration: 98, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 99, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 41, Iteration: 100, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "End of Episode 41\n",
            "Episode: 42, Iteration: 1, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 2, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 3, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 4, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 5, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 6, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 7, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 8, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 9, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 10, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 11, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 12, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 13, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 14, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 15, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 16, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 17, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 18, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 19, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 20, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 21, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 22, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 23, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 24, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 25, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 26, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 27, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 28, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 29, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 30, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 31, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 32, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 33, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 34, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 35, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 36, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 37, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 38, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 39, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 40, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 41, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 42, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 43, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 44, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 45, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 46, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 47, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 48, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 49, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 50, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 51, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 52, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 53, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 54, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 55, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 56, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 57, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 58, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 59, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 60, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 61, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 62, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 63, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 64, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 65, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 66, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 67, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "Episode: 42, Iteration: 68, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 69, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 70, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 71, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 72, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 73, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 74, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 75, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 76, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 77, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 78, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 79, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 80, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 81, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 82, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 83, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 84, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 85, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 86, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 87, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 88, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 89, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 90, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 91, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 92, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 93, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 94, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 95, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 96, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 97, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 42, Iteration: 98, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 99, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 42, Iteration: 100, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "End of Episode 42\n",
            "Episode: 43, Iteration: 1, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 2, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 3, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 4, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 5, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 6, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 7, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 8, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 9, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 10, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 11, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 12, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 13, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 14, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 15, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 16, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 17, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 18, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 19, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 20, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 21, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 22, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 23, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 24, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 25, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 26, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 27, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 28, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 29, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 30, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 31, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 32, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 33, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 34, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 35, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 36, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 37, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 38, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 39, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 40, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 41, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 42, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 43, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 44, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 45, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 46, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 47, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 48, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 49, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 50, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 51, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 52, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 53, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 54, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 55, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 56, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 57, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 58, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 59, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 60, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 61, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 62, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 63, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 64, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 65, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 66, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 67, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 68, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 69, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 70, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 71, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 72, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 73, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 74, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 75, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 76, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 77, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 78, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 79, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 80, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 81, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 82, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 83, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 84, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 85, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 86, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 87, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 88, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 89, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 90, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 91, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 92, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 93, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 94, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 95, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 96, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 97, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 98, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 43, Iteration: 99, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 43, Iteration: 100, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "End of Episode 43\n",
            "Episode: 44, Iteration: 1, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 2, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 3, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 4, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 5, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 6, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 7, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 8, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 9, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 10, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 11, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 12, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 13, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 14, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 15, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 16, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 17, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 18, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 19, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 20, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 21, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 22, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 23, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 24, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 25, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 26, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 27, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 28, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 29, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 30, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 31, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 32, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 33, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 34, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 35, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 36, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 37, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 38, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 39, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 40, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 41, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 42, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 43, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 44, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 45, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 46, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 47, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 48, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 49, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 50, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 51, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 52, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 53, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 54, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 55, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 56, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 57, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 58, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 59, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 60, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 61, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 62, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 63, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 64, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 65, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 66, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 67, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 68, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 69, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 70, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 71, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 72, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 73, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 74, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 75, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 76, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 77, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 78, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 79, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 80, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 81, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 82, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 83, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 84, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 85, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 86, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 87, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 88, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 89, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 90, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 91, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 92, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 93, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 94, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 95, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 96, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 97, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 44, Iteration: 98, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 99, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 44, Iteration: 100, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "End of Episode 44\n",
            "Episode: 45, Iteration: 1, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 2, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 3, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 4, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 5, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 6, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 7, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 8, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 9, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 10, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 11, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 12, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 13, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 14, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 15, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 16, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 17, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 18, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 19, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 20, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 21, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 22, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 23, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 24, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 25, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 26, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 27, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 28, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 29, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 30, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 31, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 32, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 33, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 34, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 35, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 36, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 37, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 38, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 39, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 40, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 41, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 42, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 43, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 44, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 45, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 46, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 47, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 48, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 49, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 50, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 51, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 52, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 53, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 54, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 55, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 56, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 57, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 58, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 59, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 60, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 61, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 62, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 63, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 64, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 65, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 66, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 67, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 68, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 69, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 70, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 71, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 72, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 73, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 74, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 75, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 76, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 77, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 78, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 79, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 80, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 81, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 82, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 83, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 84, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 85, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 86, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 87, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 88, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 89, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 90, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 91, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 92, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 93, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 94, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 95, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 96, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 97, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 98, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 45, Iteration: 99, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 45, Iteration: 100, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "End of Episode 45\n",
            "Episode: 46, Iteration: 1, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 2, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 3, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 4, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 5, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 6, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 7, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 8, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 9, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 10, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 11, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 12, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 13, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 14, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 15, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 16, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 17, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 18, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 19, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 20, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 21, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 22, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 23, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 24, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 25, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 26, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 27, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 28, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 29, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 30, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 31, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 32, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 33, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 34, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 35, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 36, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 37, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 38, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 39, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 40, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 41, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 42, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 43, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 44, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 45, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 46, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 47, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 48, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 49, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 50, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 51, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 52, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 53, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 54, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 55, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 56, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 57, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 58, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 59, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 60, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 61, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 62, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 63, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 64, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 65, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 66, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 67, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 68, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 69, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 70, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 71, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 72, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 73, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 74, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 75, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 76, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 77, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 78, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 79, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 80, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 81, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 82, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 83, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 84, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 85, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 86, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 87, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 88, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 89, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 90, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 91, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 92, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 93, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 94, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 95, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 96, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 97, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 98, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 46, Iteration: 99, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 46, Iteration: 100, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "End of Episode 46\n",
            "Episode: 47, Iteration: 1, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 2, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 3, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 4, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 5, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 6, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 7, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 8, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 9, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 10, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 11, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 12, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 13, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 14, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 15, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 16, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 17, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 18, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 19, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 20, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 21, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 22, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 23, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 24, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 25, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 26, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 27, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 28, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 29, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 30, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 31, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 32, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 33, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 34, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 35, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "Episode: 47, Iteration: 36, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 37, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 38, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 39, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 40, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 41, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 42, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 43, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 44, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 45, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 46, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 47, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 48, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 49, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 50, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 51, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 52, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 53, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 54, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 55, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 56, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 57, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 58, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 59, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 60, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 61, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 62, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 63, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 64, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 65, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 66, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 67, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 68, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 69, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 70, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 71, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 72, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 73, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 74, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 75, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 76, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 77, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 78, Current State: E, Action: 1, Reward: -100, Next State: O\n",
            "Episode: 47, Iteration: 79, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 80, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 81, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 82, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 83, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 84, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 85, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 86, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 87, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 88, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 89, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 90, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 91, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 92, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 93, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 94, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 95, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 96, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 97, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 47, Iteration: 98, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 99, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 47, Iteration: 100, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "End of Episode 47\n",
            "Episode: 48, Iteration: 1, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 2, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 3, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 4, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 5, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 6, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 7, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 8, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 9, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 10, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 11, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 12, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 13, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 14, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 15, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 16, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 17, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 18, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 19, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 20, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 21, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 22, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 23, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 24, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 25, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 26, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 27, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 28, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 29, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 30, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 31, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 32, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 33, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 34, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 35, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 36, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 37, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 38, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 39, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 40, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 41, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 42, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 43, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 44, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 45, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 46, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 47, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 48, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 49, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 50, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 51, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 52, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 53, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 54, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 55, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 56, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 57, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 58, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 59, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 60, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 61, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 62, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 63, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 64, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 65, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 66, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 67, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 68, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 69, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 70, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 71, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 72, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 73, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 74, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 75, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 76, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 77, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 78, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 79, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 80, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 81, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 82, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 83, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 84, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 85, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 86, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 87, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 88, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 89, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 90, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 91, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 92, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 93, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 94, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 95, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 96, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 97, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 48, Iteration: 98, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 99, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 48, Iteration: 100, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "End of Episode 48\n",
            "Episode: 49, Iteration: 1, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 2, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 3, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 4, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 5, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 6, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 7, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 8, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 9, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 10, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 11, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 12, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 13, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 14, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 15, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 16, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 17, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 18, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 19, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 20, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 21, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 22, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 23, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 24, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 25, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 26, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 27, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 28, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 29, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 30, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 31, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 32, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 33, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 34, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 35, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 36, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 37, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 38, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 39, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 40, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 41, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 42, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 43, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 44, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 45, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 46, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 47, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 48, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 49, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 50, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 51, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 52, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 53, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 54, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 55, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 56, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 57, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 58, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 59, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 60, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 61, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 62, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 63, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 64, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 65, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 66, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 67, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 68, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 69, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 70, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 71, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 72, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 73, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 74, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 75, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 76, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 77, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 78, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 79, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 80, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 81, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 82, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 83, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 84, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 85, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 86, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 87, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 88, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 89, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 90, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 91, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 92, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 93, Current State: E, Action: 1, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 94, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 95, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 96, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 97, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 98, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 49, Iteration: 99, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 49, Iteration: 100, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "End of Episode 49\n",
            "Episode: 50, Iteration: 1, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 2, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 3, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 4, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 5, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 6, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 7, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 8, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 9, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 10, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 11, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 12, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 13, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 14, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 15, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 16, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 17, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 18, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 19, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 20, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 21, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 22, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 23, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 24, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 25, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 26, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 27, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 28, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 29, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 30, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 31, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 32, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 33, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 34, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 35, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 36, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 37, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 38, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 39, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 40, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 41, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 42, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 43, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 44, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 45, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 46, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 47, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 48, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 49, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 50, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 51, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 52, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 53, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 54, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 55, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 56, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 57, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 58, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 59, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 60, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 61, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 62, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 63, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 64, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 65, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 66, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 67, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 68, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 69, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 70, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 71, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 72, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 73, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 74, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 75, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 76, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 77, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 78, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 79, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 80, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 81, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 82, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 83, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 84, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 85, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 86, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 87, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 88, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 89, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 90, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 91, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 92, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 93, Current State: S, Action: 0, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 94, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 95, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 96, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 97, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "Episode: 50, Iteration: 98, Current State: E, Action: 3, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 99, Current State: E, Action: 0, Reward: -1, Next State: E\n",
            "Episode: 50, Iteration: 100, Current State: S, Action: 2, Reward: -1, Next State: S\n",
            "End of Episode 50\n",
            "\n",
            "Learned Q-Values:\n",
            "[[ -9.99999924 -40.48239361  -9.99999924  -9.99999925]\n",
            " [  0.           0.           0.           0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20)In a game of chess, your opponent wants to carry over mate as soon as possible but you enhance the way of deep - Q- learning method of handling the game, explain why and how will you win through a Python Program?"
      ],
      "metadata": {
        "id": "DVo3L9-aRybj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the chess environment (simplified board)\n",
        "class ChessEnvironment:\n",
        "    def __init__(self):\n",
        "        self.board = np.array([\n",
        "            ['R', 'N', 'B', 'Q', 'K', 'B', 'N', 'R'],\n",
        "            ['P', 'P', 'P', 'P', 'P', 'P', 'P', 'P'],\n",
        "            [' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],\n",
        "            [' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],\n",
        "            [' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],\n",
        "            [' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '],\n",
        "            ['p', 'p', 'p', 'p', 'p', 'p', 'p', 'p'],\n",
        "            ['r', 'n', 'b', 'q', 'k', 'b', 'n', 'r']\n",
        "        ])\n",
        "        self.current_player = 'white'\n",
        "\n",
        "    def is_checkmate(self, player):\n",
        "        king = 'K' if player == 'white' else 'k'\n",
        "        return np.all(self.board != king)\n",
        "\n",
        "    def make_move(self, move):\n",
        "        row1, col1, row2, col2 = move\n",
        "        self.board[row2, col2] = self.board[row1, col1]\n",
        "        self.board[row1, col1] = ' '\n",
        "        self.current_player = 'white' if self.current_player == 'black' else 'black'\n",
        "\n",
        "# Deep Q-Learning agent (random move for illustration)\n",
        "class DQLAgent:\n",
        "    def __init__(self):\n",
        "        # Initialize any necessary variables for the agent\n",
        "        pass\n",
        "\n",
        "    def choose_move(self, state):\n",
        "        # For illustration, choose a random move\n",
        "        possible_moves = [(r1, c1, r2, c2) for r1 in range(8) for c1 in range(8)\n",
        "                          for r2 in range(8) for c2 in range(8)]\n",
        "        return possible_moves[np.random.choice(len(possible_moves))]\n",
        "\n",
        "# Initialize the chess environment and DQL agent\n",
        "chess_env = ChessEnvironment()\n",
        "dql_agent = DQLAgent()\n",
        "\n",
        "# Training loop (simplified random moves)\n",
        "for episode in range(10):\n",
        "    while not chess_env.is_checkmate(chess_env.current_player):\n",
        "        state = chess_env.board\n",
        "        move = dql_agent.choose_move(state)\n",
        "        print(f\"[Move {episode + 1}]\")\n",
        "        print(f\"[Current Player: {chess_env.current_player}]\")\n",
        "        print(chess_env.board)\n",
        "        print()\n",
        "        chess_env.make_move(move)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxLlyEQKR0CY",
        "outputId": "99d11504-0376-48e5-e62c-3f370492cb7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Move 1]\n",
            "[Current Player: white]\n",
            "[['R' 'N' 'B' 'Q' 'K' 'B' 'N' 'R']\n",
            " ['P' 'P' 'P' 'P' 'P' 'P' 'P' 'P']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " ['p' 'p' 'p' 'p' 'p' 'p' 'p' 'p']\n",
            " ['r' 'n' 'b' 'q' 'k' 'b' 'n' 'r']]\n",
            "\n",
            "[Move 1]\n",
            "[Current Player: black]\n",
            "[['R' 'N' 'B' 'Q' 'K' 'B' 'N' 'R']\n",
            " ['P' 'P' 'P' 'P' 'P' 'P' 'P' 'P']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " ['p' 'p' 'p' 'p' 'p' 'p' 'p' 'p']\n",
            " ['r' 'n' 'b' 'q' 'k' 'b' 'n' 'r']]\n",
            "\n",
            "[Move 1]\n",
            "[Current Player: white]\n",
            "[['R' 'N' 'B' 'Q' 'K' 'B' 'N' 'R']\n",
            " ['P' 'P' 'P' 'P' 'P' 'P' 'P' 'P']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' 'k' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " ['p' 'p' 'p' 'p' 'p' 'p' 'p' 'p']\n",
            " ['r' 'n' 'b' 'q' ' ' 'b' 'n' 'r']]\n",
            "\n",
            "[Move 1]\n",
            "[Current Player: black]\n",
            "[['R' 'N' 'B' 'Q' 'K' 'B' 'N' 'R']\n",
            " ['P' 'P' 'P' ' ' 'P' 'P' 'P' 'P']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' 'k' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " ['p' 'p' 'p' 'p' 'p' 'p' 'p' 'p']\n",
            " ['r' 'n' 'b' 'q' ' ' 'b' 'n' 'r']]\n",
            "\n",
            "[Move 1]\n",
            "[Current Player: white]\n",
            "[['R' 'N' 'B' 'Q' 'K' 'B' 'N' 'R']\n",
            " ['P' 'P' 'P' ' ' 'P' ' ' 'P' 'P']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' 'k' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " ['p' 'p' 'p' 'p' 'p' 'p' 'p' 'p']\n",
            " ['r' 'n' 'b' 'q' ' ' 'b' 'n' 'r']]\n",
            "\n",
            "[Move 1]\n",
            "[Current Player: black]\n",
            "[['R' 'N' 'B' 'Q' 'K' 'B' 'N' 'R']\n",
            " ['P' 'P' 'P' ' ' 'P' ' ' 'P' 'P']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' 'k' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " ['p' 'p' 'p' 'p' ' ' 'p' 'p' 'p']\n",
            " ['r' 'n' 'b' 'q' ' ' 'b' 'n' 'r']]\n",
            "\n",
            "[Move 1]\n",
            "[Current Player: white]\n",
            "[['R' 'N' 'B' 'Q' 'K' 'B' 'N' 'R']\n",
            " ['P' 'P' 'P' ' ' 'P' ' ' 'P' 'P']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' 'k' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " ['p' 'p' 'p' 'p' ' ' 'p' 'p' 'p']\n",
            " ['r' 'n' 'b' 'q' ' ' 'b' 'n' 'r']]\n",
            "\n",
            "[Move 1]\n",
            "[Current Player: black]\n",
            "[['R' 'N' 'B' 'Q' 'K' 'B' 'N' 'R']\n",
            " ['P' 'P' 'P' ' ' 'P' ' ' 'P' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' 'k' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " ['p' 'p' 'p' 'p' ' ' 'P' 'p' 'p']\n",
            " ['r' 'n' 'b' 'q' ' ' 'b' 'n' 'r']]\n",
            "\n",
            "[Move 1]\n",
            "[Current Player: white]\n",
            "[['R' 'N' 'B' 'Q' 'K' 'B' 'N' 'R']\n",
            " ['P' 'P' 'P' ' ' 'P' ' ' 'P' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' 'k' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " ['p' 'p' 'p' 'p' ' ' 'P' 'p' 'p']\n",
            " ['r' 'n' 'b' 'q' ' ' 'b' 'n' 'r']]\n",
            "\n",
            "[Move 1]\n",
            "[Current Player: black]\n",
            "[['R' 'N' 'B' 'Q' 'K' 'B' 'N' 'R']\n",
            " ['P' 'P' 'P' ' ' 'P' ' ' 'P' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' 'k' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " ['p' 'p' 'p' 'p' ' ' 'P' 'p' 'p']\n",
            " ['r' 'n' ' ' 'q' ' ' 'b' 'n' 'r']]\n",
            "\n",
            "[Move 1]\n",
            "[Current Player: white]\n",
            "[['R' 'N' 'B' 'Q' 'K' 'B' 'N' 'R']\n",
            " ['P' 'P' 'P' ' ' 'P' ' ' 'P' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' 'k' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " ['p' 'p' 'p' 'p' ' ' 'P' 'p' 'p']\n",
            " ['r' 'n' ' ' 'q' ' ' 'b' 'n' 'r']]\n",
            "\n",
            "[Move 1]\n",
            "[Current Player: black]\n",
            "[['R' 'N' 'B' 'Q' 'K' 'B' 'N' 'R']\n",
            " ['P' 'P' 'P' ' ' 'P' ' ' 'P' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' 'k' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " ['p' 'p' 'p' 'p' ' ' 'P' 'p' 'p']\n",
            " ['r' 'n' ' ' ' ' ' ' 'b' 'n' 'q']]\n",
            "\n",
            "[Move 1]\n",
            "[Current Player: white]\n",
            "[['R' 'N' 'B' 'Q' 'K' 'B' 'N' 'R']\n",
            " ['P' 'P' 'P' ' ' 'P' ' ' 'P' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' 'k' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " ['p' 'p' 'p' 'p' ' ' 'P' 'p' 'p']\n",
            " ['r' 'n' ' ' ' ' ' ' 'b' ' ' 'q']]\n",
            "\n",
            "[Move 1]\n",
            "[Current Player: black]\n",
            "[['R' 'N' 'B' 'Q' 'K' 'B' 'N' 'R']\n",
            " ['P' 'P' 'P' ' ' 'P' ' ' 'P' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' 'k' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " ['p' 'p' 'p' 'p' ' ' 'P' 'p' 'p']\n",
            " ['r' 'n' ' ' ' ' ' ' 'b' ' ' 'q']]\n",
            "\n",
            "[Move 1]\n",
            "[Current Player: white]\n",
            "[['R' 'N' 'B' 'Q' 'K' 'B' 'N' 'R']\n",
            " ['P' 'P' 'P' ' ' 'P' ' ' 'P' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' 'k' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " ['p' 'p' 'p' 'p' ' ' 'P' 'p' 'p']\n",
            " ['r' 'n' ' ' ' ' ' ' 'b' ' ' 'q']]\n",
            "\n",
            "[Move 1]\n",
            "[Current Player: black]\n",
            "[['R' 'N' 'B' 'Q' 'K' 'B' 'N' 'R']\n",
            " ['P' 'P' 'P' ' ' 'P' ' ' 'P' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' 'k' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " ['p' 'p' 'p' 'p' ' ' 'P' 'p' 'p']\n",
            " ['r' 'n' ' ' ' ' ' ' 'b' ' ' 'q']]\n",
            "\n",
            "[Move 1]\n",
            "[Current Player: white]\n",
            "[['R' 'N' 'B' 'Q' 'K' 'B' 'N' 'R']\n",
            " ['P' 'P' 'P' ' ' 'P' ' ' 'P' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' 'k' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " ['p' 'p' 'p' 'p' ' ' 'P' 'p' 'p']\n",
            " [' ' 'n' ' ' ' ' ' ' 'b' ' ' 'q']]\n",
            "\n",
            "[Move 1]\n",
            "[Current Player: black]\n",
            "[['R' 'N' 'B' 'Q' 'K' 'B' 'N' 'R']\n",
            " ['P' 'P' 'P' ' ' 'P' ' ' 'P' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' 'k' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " ['p' 'p' 'p' 'p' ' ' 'P' 'p' 'p']\n",
            " [' ' 'n' ' ' ' ' ' ' 'b' ' ' 'q']]\n",
            "\n",
            "[Move 1]\n",
            "[Current Player: white]\n",
            "[['R' 'N' 'B' 'Q' 'K' 'B' 'N' 'R']\n",
            " ['P' 'P' 'P' ' ' 'P' ' ' 'P' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' 'k' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " ['p' 'p' 'p' 'p' ' ' 'P' 'p' 'p']\n",
            " [' ' 'n' ' ' ' ' ' ' 'b' ' ' 'q']]\n",
            "\n",
            "[Move 1]\n",
            "[Current Player: black]\n",
            "[['R' 'N' 'B' 'Q' 'K' 'B' 'N' 'R']\n",
            " ['P' 'P' 'P' ' ' 'P' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' 'k' ' ' ' ' ' ' ' ' ' ']\n",
            " ['P' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " ['p' 'p' 'p' 'p' ' ' 'P' 'p' 'p']\n",
            " [' ' 'n' ' ' ' ' ' ' 'b' ' ' 'q']]\n",
            "\n",
            "[Move 1]\n",
            "[Current Player: white]\n",
            "[['R' 'N' 'B' 'Q' 'K' 'B' 'N' 'R']\n",
            " ['P' 'P' 'P' ' ' 'P' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " ['P' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ' ' ' ' ' ' ' ' ']\n",
            " ['p' 'p' 'p' 'p' ' ' 'P' 'p' 'p']\n",
            " [' ' 'n' ' ' ' ' ' ' 'b' ' ' 'q']]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "21)Consider you are the manager of a Finance Company, the target of the month has not been achieved and you are in trouble. You come to know that your Q-Learning System not performing well as the numbers of customers have increased, the correction decision would be increasing the layers of the DQN. So explain how you will enhance DQN and transform it into DDQN."
      ],
      "metadata": {
        "id": "mrEEP2JsR97i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, num_states, num_actions, epsilon=1.0, alpha=0.1, gamma=0.9):\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.q_table = np.zeros((num_states, num_actions))\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.randint(self.q_table.shape[1])\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state, :])\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):\n",
        "        predict = self.q_table[state, action]\n",
        "        target = reward + self.gamma * np.max(self.q_table[next_state, :])\n",
        "        self.q_table[state, action] += self.alpha * (target - predict)\n",
        "\n",
        "def train_q_learning_agent(agent, num_episodes):\n",
        "    for episode in range(num_episodes):\n",
        "        # Simulate the environment and obtain state, action, reward, next_state\n",
        "        state = 0  # Placeholder for state, replace with actual state\n",
        "        # Simulate the environment and obtain next_state, reward\n",
        "        next_state = 1  # Placeholder for next_state, replace with actual next_state\n",
        "        reward = 0  # Placeholder for reward, replace with actual reward\n",
        "\n",
        "        # Update Q-values based on rewards and next states\n",
        "        action = agent.choose_action(state)\n",
        "        agent.update_q_table(state, action, reward, next_state)\n",
        "\n",
        "        # Decay epsilon over episodes (you can adjust the decay rate)\n",
        "        agent.epsilon *= 0.99\n",
        "\n",
        "        # Display progress every 10 episodes\n",
        "        if episode % 10 == 0:\n",
        "            print(f\"Episode: {episode}/{num_episodes}, Epsilon: {agent.epsilon:.2f}\")\n",
        "\n",
        "# Example usage\n",
        "num_states = 100  # Replace with the actual number of states in your environment\n",
        "num_actions = 4  # Replace with the actual number of actions in your environment\n",
        "q_agent = QLearningAgent(num_states, num_actions)\n",
        "\n",
        "train_q_learning_agent(q_agent, num_episodes=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zzrbt4NRSCFb",
        "outputId": "ec069118-0a93-4f8b-879e-7ac7b77af37c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0/100, Epsilon: 0.99\n",
            "Episode: 10/100, Epsilon: 0.90\n",
            "Episode: 20/100, Epsilon: 0.81\n",
            "Episode: 30/100, Epsilon: 0.73\n",
            "Episode: 40/100, Epsilon: 0.66\n",
            "Episode: 50/100, Epsilon: 0.60\n",
            "Episode: 60/100, Epsilon: 0.54\n",
            "Episode: 70/100, Epsilon: 0.49\n",
            "Episode: 80/100, Epsilon: 0.44\n",
            "Episode: 90/100, Epsilon: 0.40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22) You are driving a bus in Simulation environment, a discrepancy of less quality policies are returning you a low value points in your simulation Quality which makes them to choose low optimal strategies, based on the necessity you decide to choose DDPG for inducing optimality. Prove it through Coding."
      ],
      "metadata": {
        "id": "b4xmTIZWSOHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gym\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "# Define the Actor and Critic neural networks\n",
        "class Actor(tf.keras.Model):\n",
        "    def __init__(self, action_dim, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(400, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(300, activation='relu')\n",
        "        self.output_layer = tf.keras.layers.Dense(action_dim, activation='tanh')\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def call(self, state):\n",
        "        x = self.dense1(state)\n",
        "        x = self.dense2(x)\n",
        "        actions = self.output_layer(x)\n",
        "        return actions * self.max_action\n",
        "\n",
        "class Critic(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Critic, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(400, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(300, activation='relu')\n",
        "        self.output_layer = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, state, action):\n",
        "        x = self.dense1(tf.concat([state, action], axis=-1))\n",
        "        x = self.dense2(x)\n",
        "        q_value = self.output_layer(x)\n",
        "        return q_value\n",
        "\n",
        "# Define the DDPG agent\n",
        "class DDPGAgent:\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        self.actor = Actor(action_dim, max_action)\n",
        "        self.target_actor = Actor(action_dim, max_action)\n",
        "        self.actor_optimizer = tf.keras.optimizers.Adam(0.001)\n",
        "        self.critic = Critic()\n",
        "        self.target_critic = Critic()\n",
        "        self.critic_optimizer = tf.keras.optimizers.Adam(0.002)\n",
        "\n",
        "        self.memory = deque(maxlen=100000)\n",
        "        self.batch_size = 64\n",
        "        self.discount = 0.99\n",
        "        self.tau = 0.001\n",
        "\n",
        "    def select_action(self, state):\n",
        "        return self.actor(np.expand_dims(state, axis=0))\n",
        "\n",
        "    def train(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return 0, 0\n",
        "\n",
        "        # Sample a random mini-batch from the replay buffer\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = map(np.array, zip(*batch))\n",
        "\n",
        "        target_actions = self.target_actor(next_state_batch)\n",
        "        target_q_values = self.target_critic(next_state_batch, target_actions)\n",
        "        target_q_values = reward_batch + self.discount * target_q_values * (1 - done_batch)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_values = self.critic(state_batch, action_batch)\n",
        "            critic_loss = tf.losses.mean_squared_error(target_q_values, q_values)\n",
        "        critic_grads = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
        "        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic.trainable_variables))\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions = self.actor(state_batch)\n",
        "            actor_loss = -tf.reduce_mean(self.critic(state_batch, actions))\n",
        "        actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
        "        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor.trainable_variables))\n",
        "\n",
        "        for target, source in zip(self.target_critic.trainable_variables, self.critic.trainable_variables):\n",
        "            target.assign(self.tau * source + (1 - self.tau) * target)\n",
        "        for target, source in zip(self.target_actor.trainable_variables, self.actor.trainable_variables):\n",
        "            target.assign(self.tau * source + (1 - self.tau) * target)\n",
        "\n",
        "        return actor_loss, critic_loss\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "# Main training loop\n",
        "def train_ddpg_agent():\n",
        "    env = gym.make(\"Pendulum-v1\")\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    max_action = env.action_space.high[0]\n",
        "\n",
        "    agent = DDPGAgent(state_dim, action_dim, max_action)\n",
        "\n",
        "    num_episodes = 10\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        actor_loss, critic_loss = 0, 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(state)\n",
        "            action_array = np.squeeze(action, axis=0)  # Convert action tensor to numpy array\n",
        "            next_state, reward, done, _ = env.step(action_array)\n",
        "            agent.remember(state, action_array, reward, next_state, done)\n",
        "            actor_loss, critic_loss = agent.train()\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        print(f\"Episode: {episode + 1}, Total Reward: {total_reward}, Actor Loss: {actor_loss}, Critic Loss: {critic_loss}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_ddpg_agent()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Itt3QeWSPQs",
        "outputId": "faa2f635-78aa-4bc9-e604-25df2e5972f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1, Total Reward: -1294.292987276634, Actor Loss: 6.372119903564453, Critic Loss: [8.674746  8.752578  8.622879  8.75043   8.709968  8.623032  8.675247\n",
            " 9.004166  9.035341  8.770231  8.886501  8.655541  8.680864  8.830716\n",
            " 8.621484  8.795013  8.718196  8.763958  8.627129  8.685095  8.985707\n",
            " 8.735335  8.820887  9.090378  8.621361  8.669157  8.887114  8.8906975\n",
            " 8.782696  9.717642  8.620874  8.882037  8.674864  8.627398  8.622261\n",
            " 8.712341  8.99054   8.621078  8.6203    8.648502  8.94435   8.962923\n",
            " 8.629255  8.622932  8.663622  8.79123   8.709411  8.686787  8.628561\n",
            " 8.768293  8.664322  8.683781  8.769678  8.654082  8.740658  8.847041\n",
            " 8.89395   8.650173  8.64673   8.906821  8.663234  8.949308  8.764791\n",
            " 8.634681 ]\n",
            "Episode: 2, Total Reward: -1385.7698909332216, Actor Loss: 7.159601211547852, Critic Loss: [7.4652414 7.5945888 7.387774  7.4872627 7.6185975 7.5002513 7.406433\n",
            " 7.4265633 7.6629605 7.5880527 7.332816  7.6127496 7.66588   7.4762163\n",
            " 7.3390694 7.5013695 7.3782597 7.4962873 7.7211037 7.474953  7.5120945\n",
            " 7.602408  7.3574443 7.3402834 7.512257  7.4837136 7.4590096 7.3734236\n",
            " 7.5000334 7.4723763 7.4924364 7.4834805 7.448252  7.38661   7.5264044\n",
            " 7.384074  7.384716  7.330348  7.482362  7.6164894 7.493596  7.6025767\n",
            " 7.671647  7.439719  7.575592  7.494167  7.5854435 7.67809   7.52944\n",
            " 7.6660776 7.51367   7.304081  7.6865587 7.530997  7.474863  7.498212\n",
            " 7.5341816 7.375801  7.4660225 7.4537315 7.398036  7.5963306 7.455756\n",
            " 7.4462547]\n",
            "Episode: 3, Total Reward: -1138.6871557443862, Actor Loss: 7.704880714416504, Critic Loss: [8.046789  8.181752  8.168983  8.272113  8.051827  8.062369  8.031795\n",
            " 8.02705   8.021645  8.249791  8.188625  8.281916  8.1247635 8.074944\n",
            " 8.07638   8.134827  8.273237  8.203884  8.051919  8.038763  8.032212\n",
            " 8.138994  8.08031   8.240714  8.150674  8.022627  8.011314  8.030753\n",
            " 8.230286  8.050816  8.0950365 8.5371685 8.195932  8.161436  8.369316\n",
            " 8.097247  8.45597   8.169168  8.108915  8.528055  8.066324  8.16279\n",
            " 8.17659   8.389621  8.070019  8.167862  8.196661  8.090763  8.431848\n",
            " 8.165218  8.150676  8.195984  8.060712  8.125847  8.195095  8.014633\n",
            " 8.071634  8.276058  8.236172  8.217625  8.153906  8.057087  8.052199\n",
            " 8.140788 ]\n",
            "Episode: 4, Total Reward: -1384.0132769732181, Actor Loss: 8.749445915222168, Critic Loss: [8.772211  8.574453  8.534296  8.579364  8.522356  8.612064  8.540165\n",
            " 8.521281  8.942851  8.526084  8.652741  8.586566  8.822649  8.707918\n",
            " 9.613485  8.561136  9.06612   8.623304  8.577896  8.882284  8.674093\n",
            " 9.136873  8.848129  8.843653  8.520721  8.569977  8.5981245 8.666283\n",
            " 8.689741  8.808983  8.673437  8.576063  8.617229  8.697933  8.547432\n",
            " 8.815452  8.607292  8.568385  8.74901   8.776428  9.143103  8.721047\n",
            " 8.561003  8.527445  8.648743  8.692056  8.532114  8.931552  8.771198\n",
            " 8.589296  8.524002  8.705909  8.664677  8.562956  8.607123  8.522352\n",
            " 8.588729  8.594224  8.845333  8.807447  8.631933  8.634949  8.987793\n",
            " 8.5595875]\n",
            "Episode: 5, Total Reward: -1802.332558530052, Actor Loss: 8.970073699951172, Critic Loss: [8.822248  8.657831  8.916218  8.9212055 8.860871  8.80422   8.759052\n",
            " 8.92775   8.675352  8.897095  8.6842165 8.680632  8.808111  9.047833\n",
            " 9.082391  8.650324  8.832348  8.841949  8.695955  8.761583  8.805265\n",
            " 8.6542015 8.84136   8.935596  8.752309  8.771254  8.84019   8.732247\n",
            " 8.762724  8.86023   8.999113  8.657888  8.858122  8.846619  8.892251\n",
            " 8.850562  8.765097  8.900213  8.718269  8.752386  8.874408  8.656122\n",
            " 8.788649  8.641451  8.976681  8.791125  8.703004  8.772476  8.777565\n",
            " 8.770246  8.843683  8.740627  8.790765  8.759785  8.864948  8.705208\n",
            " 8.83831   8.981889  8.772999  9.189104  8.71847   8.999454  8.927866\n",
            " 8.803713 ]\n",
            "Episode: 6, Total Reward: -1609.3866413397436, Actor Loss: 9.742729187011719, Critic Loss: [6.341052  6.325513  6.3569703 6.350358  6.3455276 6.3279276 6.350334\n",
            " 6.3494616 6.3797884 6.3422546 6.316718  6.5551653 6.3578167 6.337058\n",
            " 6.348708  6.3164024 6.382056  6.4005127 6.332528  6.362258  6.347807\n",
            " 6.3227205 6.331516  6.3219576 6.36671   6.3576612 6.2984548 6.317547\n",
            " 6.3193455 6.302593  6.344536  6.3485694 6.3209205 6.302513  6.317283\n",
            " 6.3174267 6.3445435 6.299423  6.335802  6.3224525 6.315295  6.400869\n",
            " 6.3259544 6.346506  6.335456  6.3127217 6.3651857 6.3223343 6.3203316\n",
            " 6.4022455 6.318059  6.342656  6.4309607 6.3630514 6.475495  6.358854\n",
            " 6.296291  6.3698273 6.3514533 6.41463   6.3801794 6.3876348 6.3936305\n",
            " 6.324918 ]\n",
            "Episode: 7, Total Reward: -1776.0824558109555, Actor Loss: 10.936840057373047, Critic Loss: [4.33436   4.3315353 4.2646976 4.3521547 4.3898487 4.3507595 4.322502\n",
            " 4.243024  4.4576015 4.3403664 4.2620487 4.2735605 4.304789  4.33282\n",
            " 4.347663  4.3128576 4.361107  4.2565975 4.4972258 4.299445  4.4504504\n",
            " 4.379276  4.3209267 4.3773394 4.325996  4.3533115 4.325855  4.3879204\n",
            " 4.3126335 4.3044333 4.251701  4.3288183 4.363382  4.3792076 4.3444014\n",
            " 4.3845663 4.3325357 4.305601  4.305579  4.323067  4.3227496 4.270878\n",
            " 4.310931  4.428523  4.35297   4.2619    4.4416986 4.369758  4.5049057\n",
            " 4.2939034 4.3067684 4.2917356 4.321356  4.313315  4.413501  4.2997103\n",
            " 4.525684  4.362111  4.3202696 4.2764864 4.4437356 4.338032  4.2778482\n",
            " 4.324067 ]\n",
            "Episode: 8, Total Reward: -1858.525309321993, Actor Loss: 12.18475341796875, Critic Loss: [5.137719  5.528613  5.391694  5.2999477 5.5157566 5.2482986 5.4071045\n",
            " 5.369009  5.3241453 5.376753  5.338533  5.2780585 5.422247  5.0464087\n",
            " 5.304783  5.2468824 5.334501  5.195923  5.287449  5.3536577 5.4261937\n",
            " 5.504409  5.193006  5.3584356 5.2896233 5.293498  6.083991  5.3381767\n",
            " 5.331825  5.378396  5.1448536 5.341859  5.341446  5.2031593 5.3688912\n",
            " 5.354033  5.3217154 5.6508307 5.15947   5.3895564 5.3158207 5.4036913\n",
            " 5.419839  5.34984   5.3713055 5.347564  5.3157854 5.350142  5.3251925\n",
            " 5.500288  5.4046636 5.426652  5.4109526 5.1500273 5.2124434 5.272613\n",
            " 5.386772  5.4634056 5.2578554 5.397171  5.293746  5.233969  5.5341053\n",
            " 5.319564 ]\n",
            "Episode: 9, Total Reward: -1204.381159016458, Actor Loss: 13.058967590332031, Critic Loss: [7.0208874 7.0005484 7.001742  7.005339  7.0019336 7.015764  7.000593\n",
            " 7.0007315 7.036455  7.00274   7.0005536 7.0081263 7.0030365 7.0006895\n",
            " 7.0018454 7.001466  7.156504  7.0244846 7.016004  7.0007167 7.1076846\n",
            " 7.017071  7.0024652 7.022254  7.0217733 7.0047307 7.0229015 7.000881\n",
            " 7.074352  7.0005946 7.0024624 7.001409  7.0006075 7.0344553 7.0325522\n",
            " 7.0023446 7.0036244 7.0036206 7.039422  7.0044575 7.011366  7.0005407\n",
            " 7.0369797 7.072889  7.004852  7.002313  7.0009065 7.0021167 7.00095\n",
            " 7.004635  7.0032196 7.0095515 7.002138  7.000633  7.0029163 7.008836\n",
            " 7.000638  7.007103  7.006959  7.0007987 7.0169115 7.0182214 7.005949\n",
            " 7.000629 ]\n",
            "Episode: 10, Total Reward: -1659.6784850841593, Actor Loss: 14.89620590209961, Critic Loss: [5.152705  5.2005615 5.2185593 5.149268  5.116433  5.434902  5.3461237\n",
            " 5.218262  5.123547  5.109638  5.175495  5.1227617 5.2015853 5.209592\n",
            " 5.2293444 5.195712  5.257123  5.1373777 5.2726293 5.18793   5.1554394\n",
            " 5.167493  5.2158675 5.225344  5.1526794 5.2037206 5.1413655 5.1959686\n",
            " 5.2670393 5.243391  5.2068    5.262102  5.205071  5.196117  5.169989\n",
            " 5.1887245 5.2601805 5.2487917 5.270894  5.4388447 5.1059275 5.252826\n",
            " 5.260273  5.213086  5.144904  5.1577845 5.1290684 5.2055836 5.19687\n",
            " 5.1049585 5.1923428 5.198309  5.375705  5.2102857 5.2132444 5.2580786\n",
            " 5.216374  5.3660107 5.192899  5.1611376 5.190669  5.160657  5.2339\n",
            " 5.211774 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23)You run Google Maps, a discrepancy of less quality policies are returning to customers which make them to choose low optimal strategies, CEO advises you to choose PPO for inducing optimality. Prove it through Coding"
      ],
      "metadata": {
        "id": "qOH_YeTzTM8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# Author: Dr. M. Prakash\n",
        "\n",
        "# Reinforcement Learning - Proximal Policy Optimization (PPO)\n",
        "\n",
        "# Define a simple policy network\n",
        "class PolicyNetwork(tf.keras.Model):\n",
        "    def __init__(self, num_actions):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(64, activation='relu')\n",
        "        self.action_head = tf.keras.layers.Dense(num_actions, activation='softmax')\n",
        "\n",
        "    def call(self, state):\n",
        "        x = self.dense1(state)\n",
        "        x = self.dense2(x)\n",
        "        action_probs = self.action_head(x)\n",
        "        return action_probs\n",
        "\n",
        "# Define the PPO agent\n",
        "class PPOAgent:\n",
        "    def __init__(self, state_dim, action_dim, num_actions):\n",
        "        self.policy_network = PolicyNetwork(num_actions)\n",
        "        self.policy_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "        self.epochs = 10\n",
        "        self.clip_epsilon = 0.2\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = np.expand_dims(state, axis=0)\n",
        "        action_probs = self.policy_network(state).numpy()\n",
        "        action = np.random.choice(self.num_actions, p=action_probs[0])\n",
        "        return action\n",
        "\n",
        "    def train(self, states, actions, old_action_probs, advantages):\n",
        "        for _ in range(self.epochs):\n",
        "            with tf.GradientTape() as tape:\n",
        "                action_probs = self.policy_network(states)\n",
        "                action_masks = tf.one_hot(actions, self.num_actions)\n",
        "                selected_action_probs = tf.reduce_sum(action_probs * action_masks, axis=1)\n",
        "                ratio = selected_action_probs / old_action_probs\n",
        "                clipped_ratio = tf.clip_by_value(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon)\n",
        "                surrogate_objective = tf.minimum(ratio * advantages, clipped_ratio * advantages)\n",
        "                loss = -tf.reduce_mean(surrogate_objective)\n",
        "\n",
        "            grads = tape.gradient(loss, self.policy_network.trainable_variables)\n",
        "            self.policy_optimizer.apply_gradients(zip(grads, self.policy_network.trainable_variables))\n",
        "\n",
        "# Define the environment and training loop\n",
        "def train_ppo_agent():\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = 1\n",
        "    num_actions = env.action_space.n\n",
        "    agent = PPOAgent(state_dim, action_dim, num_actions)\n",
        "    num_episodes = 10\n",
        "    max_steps_per_episode = 200\n",
        "    gamma = 0.99\n",
        "    batch_size = 32\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        states, actions, rewards, action_probs = [], [], [], []\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "\n",
        "        for t in range(max_steps_per_episode):\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            action_probs.append(agent.policy_network(np.expand_dims(state, axis=0)).numpy()[0, action])\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Compute advantages\n",
        "        discounted_rewards = []\n",
        "        advantage = 0\n",
        "\n",
        "        for r in rewards[::-1]:\n",
        "            advantage = r + gamma * advantage\n",
        "            discounted_rewards.insert(0, advantage)\n",
        "\n",
        "        # Normalize advantages\n",
        "        discounted_rewards = (discounted_rewards - np.mean(discounted_rewards)) / (np.std(discounted_rewards) + 1e-8)\n",
        "\n",
        "        # Training\n",
        "        states = np.array(states)\n",
        "        actions = np.array(actions)\n",
        "        old_action_probs = np.array(action_probs)\n",
        "        advantages = np.array(discounted_rewards)\n",
        "        indices = np.arange(len(states))\n",
        "\n",
        "        for _ in range(len(states) // batch_size):\n",
        "            batch_indices = np.random.choice(indices, batch_size, replace=False)\n",
        "            batch_states = states[batch_indices]\n",
        "            batch_actions = actions[batch_indices]\n",
        "            batch_old_action_probs = old_action_probs[batch_indices]\n",
        "            batch_advantages = advantages[batch_indices]\n",
        "\n",
        "            agent.train(batch_states, batch_actions, batch_old_action_probs, batch_advantages)\n",
        "\n",
        "        print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_ppo_agent()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ESkO5HNTOH6",
        "outputId": "f0f9ba11-7268-4ed9-d0f5-45138af9d4f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1, Total Reward: 17.0\n",
            "Episode: 2, Total Reward: 43.0\n",
            "Episode: 3, Total Reward: 15.0\n",
            "Episode: 4, Total Reward: 44.0\n",
            "Episode: 5, Total Reward: 36.0\n",
            "Episode: 6, Total Reward: 16.0\n",
            "Episode: 7, Total Reward: 39.0\n",
            "Episode: 8, Total Reward: 15.0\n",
            "Episode: 9, Total Reward: 17.0\n",
            "Episode: 10, Total Reward: 18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24)You are instructed by your mentor to build a stop clock which will be running asynchronously showing variation of different timing around the world. Now, you need to find which of two methods will be suitable for the development whether A2C or A3C give the Optimal policy designing framework."
      ],
      "metadata": {
        "id": "XGZq6tg-TZve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "\n",
        "class A2CAgent:\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = 0.99  # Discount factor for future rewards\n",
        "        self.actor_critic = self.build_actor_critic()\n",
        "        self.actor_optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "    def build_actor_critic(self):\n",
        "        input_state = Input(shape=(self.state_dim,))\n",
        "        dense1 = Dense(32, activation='relu')(input_state)\n",
        "        dense2 = Dense(32, activation='relu')(dense1)\n",
        "        action_head = Dense(self.action_dim, activation='softmax')(dense2)\n",
        "        critic_head = Dense(1)(dense2)\n",
        "\n",
        "        model = tf.keras.Model(inputs=input_state, outputs=[action_head, critic_head])\n",
        "        return model\n",
        "\n",
        "    def select_action(self, state):\n",
        "        action_probs, _ = self.actor_critic.predict(state)\n",
        "        action = np.random.choice(len(action_probs[0]), p=action_probs[0])\n",
        "        return action\n",
        "\n",
        "    def train(self, states, actions, rewards, next_states, dones):\n",
        "        with tf.GradientTape() as tape:\n",
        "            action_probs, values = self.actor_critic(states)\n",
        "            action_masks = tf.one_hot(actions, len(action_probs[0]))\n",
        "            selected_action_probs = tf.reduce_sum(action_probs * action_masks, axis=1)\n",
        "            advantages = self.compute_advantages(rewards, values, dones)\n",
        "            actor_loss = -tf.reduce_sum(tf.math.log(selected_action_probs) * advantages)\n",
        "            critic_loss = tf.reduce_sum(tf.square(rewards - values))\n",
        "\n",
        "            total_loss = actor_loss + critic_loss\n",
        "\n",
        "        actor_gradients = tape.gradient(total_loss, self.actor_critic.trainable_variables)\n",
        "        self.actor_optimizer.apply_gradients(zip(actor_gradients, self.actor_critic.trainable_variables))\n",
        "\n",
        "    def compute_advantages(self, rewards, values, dones):\n",
        "        advantages = np.zeros_like(rewards, dtype=np.float32)\n",
        "        last_advantage = 0\n",
        "        for t in reversed(range(len(rewards) - 1)):\n",
        "            mask = 1.0 - dones[t]\n",
        "            delta = rewards[t] + self.gamma * values[t + 1] * mask - values[t]\n",
        "            advantages[t] = delta + self.gamma * last_advantage * mask\n",
        "            last_advantage = advantages[t]\n",
        "        return advantages\n",
        "\n",
        "class StopwatchEnv:\n",
        "    def __init__(self):\n",
        "        self.time_elapsed = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.time_elapsed = 0\n",
        "        return [self.time_elapsed]\n",
        "\n",
        "    def step(self, action):\n",
        "        self.time_elapsed += action\n",
        "        done = False\n",
        "        if self.time_elapsed >= 60:\n",
        "            self.time_elapsed = 0\n",
        "            done = True\n",
        "        return [self.time_elapsed], 1, done\n",
        "\n",
        "def train_a2c_agent(agent, env, num_episodes=10):\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(np.array([state]))\n",
        "            next_state, reward, done = env.step(action)\n",
        "            agent.train(np.array([state]),\n",
        "                        np.array([action]),\n",
        "                        np.array([reward]),\n",
        "                        np.array([next_state]),\n",
        "                        np.array([done]))\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        print(f'Episode {episode + 1}/{num_episodes} finished. Total reward: {total_reward}')\n",
        "\n",
        "def main():\n",
        "    env = StopwatchEnv()\n",
        "    agent = A2CAgent(state_dim=1, action_dim=60)\n",
        "    train_a2c_agent(agent, env)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpKZS5wRTbGo",
        "outputId": "c863fc8b-d91c-49c5-fc99-56ff109e2a86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 207ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Episode 1/10 finished. Total reward: 3\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Episode 2/10 finished. Total reward: 2\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Episode 3/10 finished. Total reward: 2\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Episode 4/10 finished. Total reward: 3\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Episode 5/10 finished. Total reward: 2\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Episode 6/10 finished. Total reward: 2\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Episode 7/10 finished. Total reward: 2\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Episode 8/10 finished. Total reward: 2\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Episode 9/10 finished. Total reward: 4\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "Episode 10/10 finished. Total reward: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25)You are a Stock Market advisor, now there is a need to develop a learning engine which will advice you get maximum Profit investment through Probabilistic values of the historical data processing, Use Vanilla Policy Gradient for structuring the highest return Policies."
      ],
      "metadata": {
        "id": "SCvx3-RMTmQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "\n",
        "# Define the Vanilla Policy Gradient Agent\n",
        "class VPGAgent:\n",
        "    def __init__(self, state_dim, action_dim, learning_rate=0.001):\n",
        "        self.policy_network = self.build_policy_network(state_dim, action_dim)\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "    def build_policy_network(self, state_dim, action_dim):\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(32, activation='relu', input_shape=(state_dim,)),\n",
        "            tf.keras.layers.Dense(16, activation='relu'),\n",
        "            tf.keras.layers.Dense(action_dim, activation='linear')\n",
        "        ])\n",
        "        return model\n",
        "\n",
        "    def select_action(self, state):\n",
        "        action_probs = self.policy_network.predict(np.array([state]))\n",
        "        action_probs = np.squeeze(action_probs)  # Ensure the array is flattened\n",
        "        action_probs = np.clip(action_probs, 1e-8, 1.0 - 1e-8)  # Clip probabilities to avoid numerical instability\n",
        "        action_probs /= np.sum(action_probs)  # Normalize probabilities to sum to 1\n",
        "        action = np.random.choice(len(action_probs), p=action_probs)\n",
        "        return action\n",
        "\n",
        "    def train(self, states, actions, advantages):\n",
        "        with tf.GradientTape() as tape:\n",
        "            action_probs = self.policy_network(np.array(states))\n",
        "            action_masks = tf.one_hot(actions, len(action_probs[0]))\n",
        "            selected_action_probs = tf.reduce_sum(action_probs * action_masks, axis=1)\n",
        "            loss = -tf.reduce_sum(tf.math.log(selected_action_probs + 1e-8) * advantages)\n",
        "        grads = tape.gradient(loss, self.policy_network.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.policy_network.trainable_variables))\n",
        "\n",
        "# Define the stock market environment\n",
        "class StockMarketEnv:\n",
        "    def __init__(self, price_data):\n",
        "        self.price_data = price_data\n",
        "        self.current_step = 0\n",
        "        self.initial_balance = 10000  # Initial investment balance\n",
        "        self.balance = self.initial_balance\n",
        "        self.stock_units = 0\n",
        "        self.max_steps = len(price_data) - 1\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        self.balance = self.initial_balance\n",
        "        self.stock_units = 0\n",
        "        return [self.balance, self.stock_units]\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.current_step >= self.max_steps:\n",
        "            return [self.balance, self.stock_units], 0, True\n",
        "\n",
        "        current_price = self.price_data[self.current_step]\n",
        "        next_price = self.price_data[self.current_step + 1]\n",
        "\n",
        "        if action == 1:  # Buy\n",
        "            if self.balance >= current_price:\n",
        "                self.stock_units += 1\n",
        "                self.balance -= current_price\n",
        "        elif action == 0:  # Sell\n",
        "            if self.stock_units > 0:\n",
        "                self.stock_units -= 1\n",
        "                self.balance += current_price\n",
        "\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Calculate reward based on portfolio value\n",
        "        portfolio_value = self.balance + (self.stock_units * next_price)\n",
        "        reward = portfolio_value - self.initial_balance\n",
        "        done = (self.current_step == self.max_steps)\n",
        "\n",
        "        return [portfolio_value, self.stock_units], reward, done\n",
        "\n",
        "\n",
        "# Training function for the VPG agent\n",
        "def train_vpg_agent(agent, env, num_episodes=1000):\n",
        "    state_dim = 2  # State: [portfolio_value, stock_units]\n",
        "    action_dim = 2  # Actions: [Buy (1), Sell (0)]\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        # Print episode number and total reward only after the episode ends\n",
        "        print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Generate sample price data (replace with actual stock data)\n",
        "    price_data = np.random.uniform(50, 150, size=100)\n",
        "    env = StockMarketEnv(price_data)\n",
        "    agent = VPGAgent(2, 2)\n",
        "    train_vpg_agent(agent, env, num_episodes=5)  # Adjust the number of episodes as needed\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Mn2tvc9TnsT",
        "outputId": "85610b50-9baf-439c-a230-71decd5e6b47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 143ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "Episode: 1, Total Reward: 1186.4292267907203\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "Episode: 2, Total Reward: 17501.624994007747\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "Episode: 3, Total Reward: 5800.8100625113075\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 121ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Episode: 4, Total Reward: 8476.698464464695\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Episode: 5, Total Reward: 6360.781201538086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "26)In a maze game is, the agent keeps on moving over, The System decides to tough the levels, how can a system induce a Duel DQN to enhance its game play strategies the compiler used for the game is python."
      ],
      "metadata": {
        "id": "IIhA5_CpUBXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "\n",
        "# Define Dueling DQN Model\n",
        "class DuelingDQN(tf.keras.Model):\n",
        "    def __init__(self, num_actions):\n",
        "        super(DuelingDQN, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(128, activation='relu')\n",
        "        self.dense_value = tf.keras.layers.Dense(1)\n",
        "        self.dense_advantage = tf.keras.layers.Dense(num_actions)\n",
        "\n",
        "    def call(self, state):\n",
        "        x = self.dense1(state)\n",
        "        value = self.dense_value(x)\n",
        "        advantage = self.dense_advantage(x)\n",
        "        return value + (advantage - tf.reduce_mean(advantage, axis=1, keepdims=True))\n",
        "\n",
        "# Define Replay Memory\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "\n",
        "    def push(self, transition):\n",
        "        self.memory.append(transition)\n",
        "        if len(self.memory) > self.capacity:\n",
        "            self.memory.pop(0)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.memory), batch_size)\n",
        "        batch = [self.memory[i] for i in indices]\n",
        "        return batch\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "gamma = 0.99\n",
        "epsilon_start = 1.0\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.995\n",
        "target_update_frequency = 100\n",
        "\n",
        "# Environment\n",
        "env = gym.make('CartPole-v1')  # Replace 'YourMazeGameEnv-v0' with the actual environment name\n",
        "\n",
        "# Model and Target Model\n",
        "num_actions = env.action_space.n\n",
        "model = DuelingDQN(num_actions)\n",
        "target_model = DuelingDQN(num_actions)\n",
        "target_model.set_weights(model.get_weights())\n",
        "\n",
        "# Optimizer and Loss\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "huber_loss = tf.keras.losses.Huber()\n",
        "\n",
        "# Replay Memory\n",
        "replay_memory = ReplayMemory(capacity=10000)\n",
        "\n",
        "# Exploration-Exploitation\n",
        "epsilon = epsilon_start\n",
        "\n",
        "# Training Loop\n",
        "num_episodes = 10\n",
        "batch_size = 32\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, -1])\n",
        "    total_reward = 0\n",
        "\n",
        "    while True:\n",
        "        # Exploration-Exploitation\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            q_values = model.predict(state)\n",
        "            action = np.argmax(q_values)\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_state = np.reshape(next_state, [1, -1])\n",
        "        total_reward += reward\n",
        "\n",
        "        # Store the transition in the replay memory\n",
        "        replay_memory.push((state, action, reward, next_state, int(done)))  # Convert done to an integer\n",
        "\n",
        "        # Sample a random batch from the replay memory\n",
        "        if len(replay_memory.memory) > batch_size:\n",
        "            batch = replay_memory.sample(batch_size)\n",
        "            states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "            states = np.vstack(states)\n",
        "            next_states = np.vstack(next_states)\n",
        "\n",
        "            # Double DQN update\n",
        "            target_q_values_next = target_model.predict(next_states)\n",
        "            max_actions = np.argmax(model.predict(next_states), axis=1)\n",
        "            target_q_values_next = target_q_values_next[np.arange(batch_size), max_actions]\n",
        "\n",
        "            targets = rewards + gamma * (1 - np.array(dones)) * target_q_values_next  # Convert dones to a NumPy array\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                q_values = model(states)\n",
        "                action_masks = tf.one_hot(actions, num_actions)\n",
        "                selected_q_values = tf.reduce_sum(q_values * action_masks, axis=1)\n",
        "                loss = huber_loss(targets, selected_q_values)\n",
        "\n",
        "            gradients = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        # Update target model\n",
        "        if episode % target_update_frequency == 0:\n",
        "            target_model.set_weights(model.get_weights())\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Decay exploration rate\n",
        "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
        "\n",
        "    print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
        "\n",
        "# Evaluate the trained model\n",
        "# Add code for evaluation based on your requirements\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua0YUNsOUDbP",
        "outputId": "0c662a3b-5159-4c32-ebfb-2f62a240fc0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1, Total Reward: 13.0\n",
            "Episode: 2, Total Reward: 13.0\n",
            "1/1 [==============================] - 1s 662ms/step\n",
            "1/1 [==============================] - 0s 347ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 138ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "Episode: 3, Total Reward: 11.0\n",
            "1/1 [==============================] - 0s 133ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 110ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 130ms/step\n",
            "1/1 [==============================] - 0s 105ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 115ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 220ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Episode: 4, Total Reward: 26.0\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "Episode: 5, Total Reward: 13.0\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 118ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "Episode: 6, Total Reward: 15.0\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "Episode: 7, Total Reward: 10.0\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 127ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "Episode: 8, Total Reward: 25.0\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "Episode: 9, Total Reward: 25.0\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Episode: 10, Total Reward: 14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "27) You run Book my Show application, an requirement of making the efficient mapping of user interests to be recommended is the need of the hour, Your higher officials suggest to increase the depth of the Q- Network but you defend them by saying Duelling would help. Prove them by providing the code of lines."
      ],
      "metadata": {
        "id": "5CsuZnaFUhH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define Dueling DQN Model\n",
        "class DuelingDQN(tf.keras.Model):\n",
        "    def __init__(self, num_actions, state_dim):\n",
        "        super(DuelingDQN, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(128, activation='relu')\n",
        "        self.dense_value = tf.keras.layers.Dense(1)\n",
        "        self.dense_advantage = tf.keras.layers.Dense(num_actions)\n",
        "        self.build((None, state_dim))  # Initialize the model's weights\n",
        "\n",
        "    def call(self, state):\n",
        "        x = self.dense1(state)\n",
        "        value = self.dense_value(x)\n",
        "        advantage = self.dense_advantage(x)\n",
        "        return value + (advantage - tf.reduce_mean(advantage, axis=1, keepdims=True))\n",
        "\n",
        "# Define Replay Memory\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "\n",
        "    def push(self, transition):\n",
        "        self.memory.append(transition)\n",
        "        if len(self.memory) > self.capacity:\n",
        "            self.memory.pop(0)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.memory), batch_size)\n",
        "        batch = [self.memory[i] for i in indices]\n",
        "        return batch\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "gamma = 0.99\n",
        "epsilon_start = 1.0\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.995\n",
        "\n",
        "# Model\n",
        "num_actions = 10  # Number of recommendation items\n",
        "state_dim = 50  # Dimensionality of user interests\n",
        "model = DuelingDQN(num_actions, state_dim)\n",
        "target_model = DuelingDQN(num_actions, state_dim)\n",
        "target_model.set_weights(model.get_weights())\n",
        "\n",
        "# Optimizer and Loss\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "huber_loss = tf.keras.losses.Huber()\n",
        "\n",
        "# Exploration-Exploitation\n",
        "epsilon = epsilon_start\n",
        "\n",
        "# Training Loop\n",
        "num_episodes = 100\n",
        "batch_size = 32\n",
        "replay_memory = ReplayMemory(capacity=10000)\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    # Generate a random user interest profile (state)\n",
        "    state = np.random.rand(1, state_dim).astype(np.float32)\n",
        "\n",
        "    # Exploration-Exploitation\n",
        "    if np.random.rand() < epsilon:\n",
        "        action = np.random.randint(num_actions)\n",
        "    else:\n",
        "        q_values = model.predict(state)\n",
        "        action = np.argmax(q_values)\n",
        "\n",
        "    # Simulate a recommendation and get the reward (could be based on user feedback)\n",
        "    reward = np.random.rand()\n",
        "\n",
        "    # Store the transition in replay memory\n",
        "    replay_memory.push((state, action, reward, state, False))  # Assume next state is the same for simplicity\n",
        "\n",
        "    # Sample a random batch from the replay memory\n",
        "    if len(replay_memory.memory) > batch_size:\n",
        "        batch = replay_memory.sample(batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states = np.vstack(states)\n",
        "        next_states = np.vstack(next_states)\n",
        "\n",
        "        # Double DQN update\n",
        "        target_q_values_next = target_model.predict(next_states)\n",
        "        max_actions = np.argmax(model.predict(next_states), axis=1)\n",
        "        target_q_values_next = target_q_values_next[np.arange(batch_size), max_actions]\n",
        "\n",
        "        targets = rewards + gamma * (1 - np.array(dones)) * target_q_values_next\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            q_values = model(states)\n",
        "            action_masks = tf.one_hot(actions, num_actions)\n",
        "            selected_q_values = tf.reduce_sum(q_values * action_masks, axis=1)\n",
        "            loss = huber_loss(targets, selected_q_values)\n",
        "\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    # Update target model\n",
        "    if episode % 10 == 0:\n",
        "        target_model.set_weights(model.get_weights())\n",
        "\n",
        "    # Decay exploration rate\n",
        "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
        "\n",
        "    print(f\"Episode: {episode + 1}, Action: {action}, Reward: {reward}\")\n",
        "\n",
        "# The trained model can be used for making recommendations based on user interests.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrPygpqsUiXN",
        "outputId": "05f2fa60-eb1f-4e4b-cb41-c639ec6859bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1, Action: 0, Reward: 0.9463472441008585\n",
            "Episode: 2, Action: 3, Reward: 0.8531824846758225\n",
            "Episode: 3, Action: 2, Reward: 0.5117288855875964\n",
            "Episode: 4, Action: 8, Reward: 0.788129804974642\n",
            "Episode: 5, Action: 1, Reward: 0.15531748979403193\n",
            "Episode: 6, Action: 9, Reward: 0.43843828816879815\n",
            "Episode: 7, Action: 9, Reward: 0.5458211608773773\n",
            "Episode: 8, Action: 1, Reward: 0.8551249961013745\n",
            "Episode: 9, Action: 9, Reward: 0.7524291755535721\n",
            "Episode: 10, Action: 9, Reward: 0.6020321095388621\n",
            "Episode: 11, Action: 7, Reward: 0.11505417479658964\n",
            "Episode: 12, Action: 9, Reward: 0.3152235979594027\n",
            "Episode: 13, Action: 5, Reward: 0.4417154376960869\n",
            "Episode: 14, Action: 1, Reward: 0.48143319724360756\n",
            "Episode: 15, Action: 7, Reward: 0.19377340493428408\n",
            "Episode: 16, Action: 7, Reward: 0.1864867952313155\n",
            "Episode: 17, Action: 7, Reward: 0.4714127007790013\n",
            "Episode: 18, Action: 5, Reward: 0.6658970399230779\n",
            "Episode: 19, Action: 0, Reward: 0.700313120252223\n",
            "Episode: 20, Action: 2, Reward: 0.7095692533995515\n",
            "Episode: 21, Action: 1, Reward: 0.13249945497765336\n",
            "Episode: 22, Action: 5, Reward: 0.6389266027205645\n",
            "Episode: 23, Action: 7, Reward: 0.4957909043885711\n",
            "Episode: 24, Action: 3, Reward: 0.7011547451288183\n",
            "Episode: 25, Action: 3, Reward: 0.9039126944040827\n",
            "Episode: 26, Action: 1, Reward: 0.8162330950291847\n",
            "Episode: 27, Action: 2, Reward: 0.34756856797136226\n",
            "Episode: 28, Action: 8, Reward: 0.8792795387896473\n",
            "Episode: 29, Action: 9, Reward: 0.7130716032206149\n",
            "Episode: 30, Action: 3, Reward: 0.30688151169583766\n",
            "1/1 [==============================] - 0s 231ms/step\n",
            "Episode: 31, Action: 8, Reward: 0.9375723781237132\n",
            "Episode: 32, Action: 9, Reward: 0.11878076057726017\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "Episode: 33, Action: 6, Reward: 0.09323852345946337\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "Episode: 34, Action: 0, Reward: 0.21417551106258537\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "Episode: 35, Action: 0, Reward: 0.10597261359442367\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "Episode: 36, Action: 2, Reward: 0.660492126187936\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "Episode: 37, Action: 3, Reward: 0.6784613543689745\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "Episode: 38, Action: 7, Reward: 0.06668435885131885\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "Episode: 39, Action: 3, Reward: 0.755925322653515\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "Episode: 40, Action: 5, Reward: 0.6690329436652005\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "Episode: 41, Action: 1, Reward: 0.8084631178818411\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Episode: 42, Action: 2, Reward: 0.23131210603769647\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "Episode: 43, Action: 5, Reward: 0.8457534851190922\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Episode: 44, Action: 7, Reward: 0.990225351115833\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Episode: 45, Action: 0, Reward: 0.03963633274157119\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Episode: 46, Action: 8, Reward: 0.4878276373646713\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Episode: 47, Action: 4, Reward: 0.3636630680758304\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Episode: 48, Action: 9, Reward: 0.06129093230552485\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Episode: 49, Action: 3, Reward: 0.6268636738560146\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Episode: 50, Action: 4, Reward: 0.760647057720195\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Episode: 51, Action: 3, Reward: 0.5143928223043586\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Episode: 52, Action: 8, Reward: 0.24765671618261054\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Episode: 53, Action: 5, Reward: 0.15659615675960625\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Episode: 54, Action: 8, Reward: 0.022290108869357783\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Episode: 55, Action: 3, Reward: 0.08020785334688096\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Episode: 56, Action: 6, Reward: 0.06950027798969727\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Episode: 57, Action: 3, Reward: 0.05907484646081562\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "Episode: 58, Action: 1, Reward: 0.7342596568227077\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Episode: 59, Action: 4, Reward: 0.027410151077079892\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Episode: 60, Action: 2, Reward: 0.9574866380239345\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "Episode: 61, Action: 7, Reward: 0.15703172160515233\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "Episode: 62, Action: 1, Reward: 0.3382220534816036\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "Episode: 63, Action: 9, Reward: 0.7109418976411156\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Episode: 64, Action: 1, Reward: 0.046002248962115355\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Episode: 65, Action: 1, Reward: 0.17540086194579418\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Episode: 66, Action: 0, Reward: 0.36518607517175905\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "Episode: 67, Action: 8, Reward: 0.4990950167541587\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Episode: 68, Action: 5, Reward: 0.572188506896101\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Episode: 69, Action: 7, Reward: 0.378774250612286\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Episode: 70, Action: 2, Reward: 0.602198777736917\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Episode: 71, Action: 6, Reward: 0.3130178964119207\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Episode: 72, Action: 5, Reward: 0.08036788811806617\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "Episode: 73, Action: 6, Reward: 0.6002644195811465\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Episode: 74, Action: 2, Reward: 0.7763209428538468\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Episode: 75, Action: 4, Reward: 0.5872098361332331\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Episode: 76, Action: 3, Reward: 0.2850299998188257\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "Episode: 77, Action: 8, Reward: 0.6396264054891063\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Episode: 78, Action: 5, Reward: 0.002968886297009643\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "Episode: 79, Action: 9, Reward: 0.9579189217070426\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "Episode: 80, Action: 9, Reward: 0.7788420913469629\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "Episode: 81, Action: 5, Reward: 0.7200065550483767\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "Episode: 82, Action: 5, Reward: 0.292445933344438\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Episode: 83, Action: 3, Reward: 0.9278464802854414\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "Episode: 84, Action: 4, Reward: 0.5179463025330073\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "Episode: 85, Action: 4, Reward: 0.7629043298834743\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "Episode: 86, Action: 3, Reward: 0.1620835945990844\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "Episode: 87, Action: 9, Reward: 0.15947628405617753\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "Episode: 88, Action: 3, Reward: 0.5944364315549816\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "Episode: 89, Action: 1, Reward: 0.9306076658693161\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "Episode: 90, Action: 9, Reward: 0.8066463062716498\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "Episode: 91, Action: 3, Reward: 0.998106665821737\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "Episode: 92, Action: 8, Reward: 0.9296465615932236\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "Episode: 93, Action: 4, Reward: 0.1831433141615152\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "Episode: 94, Action: 3, Reward: 0.4554239743308789\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "Episode: 95, Action: 8, Reward: 0.030245679687029403\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "Episode: 96, Action: 0, Reward: 0.25952135849916436\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Episode: 97, Action: 1, Reward: 0.9500307424775488\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "Episode: 98, Action: 0, Reward: 0.764332426010589\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Episode: 99, Action: 3, Reward: 0.860263085984388\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "Episode: 100, Action: 5, Reward: 0.915198910730161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "28)Consider two robots are running on respective tracks. Each of the robots is built with the various Q-learning models say, Robot 1 is induced with DQN, and Robot 2 is powered by DDQN and Robot 3 with DDDQN. On using a python script reveal which Robot would outperforms the other in terms of efficiency by attaining maximal reward at less number of computational steps."
      ],
      "metadata": {
        "id": "_SMdJABXUz0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "from collections import deque\n",
        "import random\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, model_type):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95  # discount factor\n",
        "        self.epsilon = 1.0  # exploration-exploitation trade-off\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.epsilon_min = 0.01\n",
        "        self.learning_rate = 0.001\n",
        "        self.model_type = model_type\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        model = Sequential()\n",
        "\n",
        "        if self.model_type == 'DQN':\n",
        "            model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "            model.add(Dense(24, activation='relu'))\n",
        "            model.add(Dense(self.action_size, activation='linear'))\n",
        "\n",
        "        elif self.model_type == 'DDQN' or self.model_type == 'DDDQN':\n",
        "            model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "            model.add(Dense(24, activation='relu'))\n",
        "\n",
        "            value_stream = Dense(1, activation='linear')\n",
        "            advantage_stream = Dense(self.action_size, activation='linear')\n",
        "\n",
        "            if self.model_type == 'DDQN':\n",
        "                advantage_stream.trainable = False\n",
        "            elif self.model_type == 'DDDQN':\n",
        "                advantage_stream.trainable = True\n",
        "\n",
        "            model.add(value_stream)\n",
        "            model.add(advantage_stream)\n",
        "\n",
        "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        state = np.array(state, dtype=np.float32)\n",
        "        action = np.array(action, dtype=np.int32)\n",
        "        reward = np.array(reward, dtype=np.float32)\n",
        "        next_state = np.array(next_state, dtype=np.float32)\n",
        "        done = np.array(done, dtype=np.float32)\n",
        "\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(self.action_size)\n",
        "        act_values = self.model.predict(tf.convert_to_tensor(state, dtype=tf.float32))\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
        "\n",
        "        states = np.vstack(states).astype(np.float32)\n",
        "        actions = np.array(actions, dtype=np.int32)\n",
        "        rewards = np.array(rewards, dtype=np.float32)\n",
        "        next_states = np.vstack(next_states).astype(np.float32)\n",
        "        dones = np.array(dones, dtype=np.float32)\n",
        "\n",
        "        targets = self.model.predict(states)\n",
        "\n",
        "        if self.model_type == 'DQN':\n",
        "            targets[range(batch_size), actions.flatten()] = rewards + self.gamma * np.max(self.model.predict(next_states), axis=1) * (1 - dones.flatten())\n",
        "        elif self.model_type == 'DDQN':\n",
        "            best_actions = np.argmax(self.model.predict(next_states), axis=1)\n",
        "            targets[range(batch_size), actions.flatten()] = rewards + self.gamma * self.model.predict(next_states)[range(batch_size), best_actions] * (1 - dones.flatten())\n",
        "        elif self.model_type == 'DDDQN':\n",
        "            targets[range(batch_size), actions.flatten()] = rewards + self.gamma * np.max(self.model.predict(next_states), axis=1) * (1 - dones.flatten())\n",
        "\n",
        "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)\n",
        "\n",
        "def train_agent(agent, env, episodes=10, batch_size=32):\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, [1, state_size])\n",
        "        total_reward = 0\n",
        "\n",
        "        for step in range(50):  # Set a reasonable maximum number of steps\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_state = np.reshape(next_state, [1, state_size])\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        if len(agent.memory) > batch_size:\n",
        "            agent.replay(batch_size)\n",
        "\n",
        "        print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = gym.make('CartPole-v1')  # You can change the environment if needed\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size = env.action_space.n\n",
        "\n",
        "    # Train DQN\n",
        "    dqn_agent = DQNAgent(state_size, action_size, model_type='DQN')\n",
        "    train_agent(dqn_agent, env, episodes=10)\n",
        "\n",
        "    # Train DDQN\n",
        "    ddqn_agent = DQNAgent(state_size, action_size, model_type='DDQN')\n",
        "    train_agent(ddqn_agent, env, episodes=10)\n",
        "\n",
        "    # Train DDDQN\n",
        "    dddqn_agent = DQNAgent(state_size, action_size, model_type='DDDQN')\n",
        "    train_agent(dddqn_agent, env, episodes=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pLDWoI_U14V",
        "outputId": "d6149f11-f139-4b7d-a9a6-665d92374b71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "Episode: 1, Total Reward: 38.0\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "Episode: 2, Total Reward: 11.0\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Episode: 3, Total Reward: 20.0\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Episode: 4, Total Reward: 37.0\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "Episode: 5, Total Reward: 32.0\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "1/1 [==============================] - 0s 98ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Episode: 6, Total Reward: 21.0\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Episode: 7, Total Reward: 21.0\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "Episode: 8, Total Reward: 18.0\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "Episode: 9, Total Reward: 50.0\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "Episode: 10, Total Reward: 26.0\n",
            "Episode: 1, Total Reward: 15.0\n",
            "1/1 [==============================] - 0s 215ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "Episode: 2, Total Reward: 29.0\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "Episode: 3, Total Reward: 12.0\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "Episode: 4, Total Reward: 12.0\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "Episode: 5, Total Reward: 14.0\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "Episode: 6, Total Reward: 37.0\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "Episode: 7, Total Reward: 28.0\n",
            "1/1 [==============================] - 0s 244ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "Episode: 8, Total Reward: 15.0\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "Episode: 9, Total Reward: 50.0\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "Episode: 10, Total Reward: 22.0\n",
            "Episode: 1, Total Reward: 14.0\n",
            "1/1 [==============================] - 0s 248ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "Episode: 2, Total Reward: 23.0\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "Episode: 3, Total Reward: 32.0\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "Episode: 4, Total Reward: 11.0\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Episode: 5, Total Reward: 20.0\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Episode: 6, Total Reward: 47.0\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Episode: 7, Total Reward: 11.0\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "Episode: 8, Total Reward: 15.0\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "Episode: 9, Total Reward: 26.0\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Episode: 10, Total Reward: 27.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "29)Consider you are playing a game of Mice and Cheese, The Mice is getting only single pile of cheese instead of the group of cheese. The System decides to enhance its reward maximization technique to enhance its game play strategies. Enhance a Q-table and train the mice to get the Big pile of cheese."
      ],
      "metadata": {
        "id": "t8K2VSY1VJ7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Environment parameters\n",
        "num_states = 5  # Number of states representing the positions of the mouse\n",
        "num_actions = 2  # Number of possible actions: move left or move right\n",
        "epsilon = 0.1  # Exploration-exploitation trade-off\n",
        "alpha = 0.2  # Learning rate\n",
        "gamma = 0.8  # Discount factor\n",
        "num_episodes = 1000\n",
        "\n",
        "# Initialize Q-table\n",
        "q_table = np.zeros((num_states, num_actions))\n",
        "\n",
        "# Function to choose an action based on epsilon-greedy strategy\n",
        "def choose_action(state):\n",
        "    if np.random.rand() < epsilon:\n",
        "        # Exploration: choose a random action\n",
        "        return np.random.choice(num_actions)\n",
        "    else:\n",
        "        # Exploitation: choose the action with the highest Q-value\n",
        "        return np.argmax(q_table[state, :])\n",
        "\n",
        "# Function to update the Q-table based on the Q-learning algorithm\n",
        "def update_q_table(state, action, reward, next_state):\n",
        "    best_next_action = np.argmax(q_table[next_state, :])\n",
        "    q_table[state, action] = (1 - alpha) * q_table[state, action] + \\\n",
        "                            alpha * (reward + gamma * q_table[next_state, best_next_action])\n",
        "\n",
        "# Training loop\n",
        "for episode in range(num_episodes):\n",
        "    # Reset the environment for a new episode\n",
        "    state = 0  # Starting state\n",
        "    total_reward = 0\n",
        "\n",
        "    while state != num_states - 1:  # Continue until the mouse reaches the cheese pile\n",
        "        # Choose an action\n",
        "        action = choose_action(state)\n",
        "\n",
        "        # Take the chosen action and observe the next state and reward\n",
        "        if action == 0:  # Move left\n",
        "            next_state = max(0, state - 1)\n",
        "        else:  # Move right\n",
        "            next_state = min(num_states - 1, state + 1)\n",
        "\n",
        "        # Define the reward structure (you can customize this based on your game rules)\n",
        "        if next_state == num_states - 1:  # Mouse reached the cheese pile\n",
        "            reward = 1\n",
        "        else:\n",
        "            reward = 0\n",
        "\n",
        "        # Update the Q-table\n",
        "        update_q_table(state, action, reward, next_state)\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "\n",
        "        # Accumulate total reward for monitoring\n",
        "        total_reward += reward\n",
        "\n",
        "    if episode % 100 == 0:\n",
        "        print(f\"Episode: {episode}, Total Reward: {total_reward}\")\n",
        "\n",
        "# Print the final Q-table\n",
        "print(\"\\nFinal Q-table:\")\n",
        "print(q_table)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzRYOpCvVLNM",
        "outputId": "dd06f518-6bad-4ad6-8c4b-4680428a4bdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0, Total Reward: 1\n",
            "Episode: 100, Total Reward: 1\n",
            "Episode: 200, Total Reward: 1\n",
            "Episode: 300, Total Reward: 1\n",
            "Episode: 400, Total Reward: 1\n",
            "Episode: 500, Total Reward: 1\n",
            "Episode: 600, Total Reward: 1\n",
            "Episode: 700, Total Reward: 1\n",
            "Episode: 800, Total Reward: 1\n",
            "Episode: 900, Total Reward: 1\n",
            "\n",
            "Final Q-table:\n",
            "[[0.40959996 0.512     ]\n",
            " [0.40959717 0.64      ]\n",
            " [0.51191235 0.8       ]\n",
            " [0.63998573 1.        ]\n",
            " [0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "30)A Scientist in a product development company which produces a stick which helps Visually Challenged People, the company wants to optimize the cost of the stick as the rule based instructions make them higher time complexity, so they ask the Scientist to develop a stick based on the learning strategy for the model which is to be based on the previous experiences present in the video recordings of the dataset. Help the scientist to successfully tweek the code and induce the strategy."
      ],
      "metadata": {
        "id": "2Xg8ZJ90VXiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from collections import deque\n",
        "\n",
        "# Define the 1D stick environment\n",
        "class StickEnvironment(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(StickEnvironment, self).__init__()\n",
        "        self.action_space = gym.spaces.Discrete(2)  # 0: move left, 1: move right\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)\n",
        "        self.position = 0.5  # Initial position of the stick\n",
        "\n",
        "    def step(self, action):\n",
        "        if action == 0:\n",
        "            self.position -= np.random.uniform(0.1, 0.5)  # Move left\n",
        "        elif action == 1:\n",
        "            self.position += np.random.uniform(0.1, 0.5)  # Move right\n",
        "\n",
        "        self.position = np.clip(self.position, 0, 1)  # Clip position to [0, 1]\n",
        "        reward = 0\n",
        "        if 0.4 <= self.position <= 0.6:\n",
        "            reward = 1  # Positive reward for staying in the target region\n",
        "\n",
        "        done = False\n",
        "        return np.array([self.position]), reward, done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.position = 0.5\n",
        "        return np.array([self.position])\n",
        "\n",
        "# Define the RL agent\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95  # discount factor\n",
        "        self.epsilon = 1.0  # exploration-exploitation trade-off\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.epsilon_min = 0.01\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.choice(self.action_size)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = reward + self.gamma * np.amax(self.model.predict(np.array([next_state]))[0])\n",
        "            target_f = self.model.predict(np.array([state]))\n",
        "            target_f[0][int(action)] = target  # Convert action to integer\n",
        "            self.model.fit(np.array([state]), target_f, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "# Initialize the environment and agent\n",
        "env = StickEnvironment()\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "agent = DQNAgent(state_size, action_size)\n",
        "\n",
        "# Training the agent from synthetic dataset\n",
        "def train_agent_from_dataset(agent, dataset, episodes=100):\n",
        "    for episode in range(episodes):\n",
        "        for data_point in dataset:\n",
        "            state, action, reward, next_state, done = data_point\n",
        "            agent.remember(np.array([state]), action, reward, np.array([next_state]), done)\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        agent.replay(11)\n",
        "\n",
        "        # Print the total reward for this episode (not meaningful in this case)\n",
        "        print(f\"Episode: {episode + 1}\")\n",
        "\n",
        "# Generate synthetic dataset\n",
        "num_episodes = 10\n",
        "episode_length = 100\n",
        "synthetic_dataset = []\n",
        "for _ in range(num_episodes):\n",
        "    episode = []\n",
        "    position = 0.5  # Initial position of the stick\n",
        "    for _ in range(episode_length):\n",
        "        action = np.random.choice([0, 1])  # Move left or right\n",
        "        position += (action * 2 - 1) * np.random.uniform(0.1, 0.5)  # Random movement\n",
        "        position = np.clip(position, 0, 1)  # Clip position to [0, 1]\n",
        "        reward = 0\n",
        "        if 0.4 <= position <= 0.6:\n",
        "            reward = 1  # Positive reward for staying in the target region\n",
        "        episode.append((position, action, reward, position, False))\n",
        "    episode[-1] = episode[-1][:4] + (True,)\n",
        "    synthetic_dataset.extend(episode)\n",
        "synthetic_dataset = np.array(synthetic_dataset)\n",
        "\n",
        "# Train the agent from the synthetic dataset\n",
        "train_agent_from_dataset(agent, synthetic_dataset, episodes=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIVaTxvVVZXA",
        "outputId": "0e0819df-8888-4f43-a86f-a5e58f040a9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 264ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Episode: 1\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "Episode: 2\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 111ms/step\n",
            "1/1 [==============================] - 0s 106ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "Episode: 3\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 123ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 101ms/step\n",
            "1/1 [==============================] - 0s 62ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Episode: 4\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Episode: 5\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "Episode: 6\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 109ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "Episode: 7\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "Episode: 8\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Episode: 9\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Episode: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "31)Consider a line following robot are running on a line. The robot’s efficiency is very less so enhance the robot’s steps by trial and error method by inducing a Vanilla Policy Gradient mechanism"
      ],
      "metadata": {
        "id": "5OJZRpsJVvdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the line-following environment\n",
        "class LineFollowingEnvironment:\n",
        "    def __init__(self):\n",
        "        self.state = 0.0  # Initial state\n",
        "        self.target_state = 1.0  # Target state\n",
        "\n",
        "    def step(self, action):\n",
        "        # Simulate the robot's movement based on the action\n",
        "        movement = action - 1  # Actions: 0 (left), 1 (stay), 2 (right)\n",
        "        self.state += 0.1 * movement  # Simulate movement\n",
        "        reward = -abs(self.state - self.target_state)  # Negative distance to the target is the reward\n",
        "        return self.state, reward, False  # Return next state, reward, and whether the episode is done\n",
        "\n",
        "# Define the policy network\n",
        "class PolicyNetwork(tf.keras.Model):\n",
        "    def __init__(self, num_actions):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(16, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(num_actions, activation='softmax')\n",
        "\n",
        "    def call(self, state):\n",
        "        x = self.dense1(state)\n",
        "        return self.dense2(x)\n",
        "\n",
        "# Initialize the environment and the policy network\n",
        "env = LineFollowingEnvironment()\n",
        "policy_network = PolicyNetwork(num_actions=3)  # 3 actions: left, stay, right\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "# Training loop using Vanilla Policy Gradient\n",
        "num_episodes = 100\n",
        "returns = []\n",
        "\n",
        "@tf.function\n",
        "def train_step(states, actions, discounted_returns):\n",
        "    with tf.GradientTape() as tape:\n",
        "        action_probs = policy_network(states)\n",
        "        action_distribution = tfp.distributions.Categorical(probs=action_probs)\n",
        "        action_log_probs = action_distribution.log_prob(actions)\n",
        "        loss = -tf.reduce_sum(action_log_probs * discounted_returns)\n",
        "\n",
        "    gradients = tape.gradient(loss, policy_network.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, policy_network.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    states, actions, rewards = [], [], []\n",
        "\n",
        "    for _ in range(10):  # Maximum steps per episode\n",
        "        state = np.array([[env.state]], dtype=np.float32)\n",
        "        action_probs = policy_network(state)\n",
        "        action = np.random.choice([0, 1, 2], p=action_probs.numpy()[0])\n",
        "        next_state, reward, done = env.step(action)\n",
        "\n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Compute discounted returns\n",
        "    discounted_returns = []\n",
        "    cumulative_return = 0\n",
        "    for r in reversed(rewards):\n",
        "        cumulative_return = r + 0.95 * cumulative_return\n",
        "        discounted_returns.append(cumulative_return)\n",
        "    discounted_returns.reverse()\n",
        "\n",
        "    # Convert lists to TensorFlow tensors\n",
        "    states = tf.concat(states, axis=0)\n",
        "    actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
        "    discounted_returns = tf.convert_to_tensor(discounted_returns, dtype=tf.float32)\n",
        "\n",
        "    # Perform a gradient step\n",
        "    loss = train_step(states, actions, discounted_returns)\n",
        "\n",
        "    # Store the total return for this episode\n",
        "    returns.append(np.sum(rewards))\n",
        "\n",
        "    # Print progress\n",
        "    if episode % 10 == 0:\n",
        "        print(f\"Episode {episode}, Total Return: {np.sum(rewards)}\")\n",
        "\n",
        "# Plot the learning curve\n",
        "plt.plot(returns)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Return')\n",
        "plt.title('Vanilla Policy Gradient Learning Curve')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "qpqLbrucVwo2",
        "outputId": "d4f21577-9ddd-4a98-f2ba-3152fa03856c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Total Return: -11.599999999999998\n",
            "Episode 10, Total Return: -19.1\n",
            "Episode 20, Total Return: -14.5\n",
            "Episode 30, Total Return: -8.2\n",
            "Episode 40, Total Return: -6.8\n",
            "Episode 50, Total Return: -70.49999999999987\n",
            "Episode 60, Total Return: -170.4999999999999\n",
            "Episode 70, Total Return: -270.50000000000125\n",
            "Episode 80, Total Return: -370.5000000000027\n",
            "Episode 90, Total Return: -470.5000000000041\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "32)A robot dog you are building that is to be running on a zig zag lines. The robot’s efficiency is very less as it needs to enhance both actions simulatenously and asynchronously, propose a method how you will induce an algorithm to make it work efficiently (Actor-critic methods)"
      ],
      "metadata": {
        "id": "oTvIJL8AV9uM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# Actor Network\n",
        "class Actor(tf.keras.Model):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Actor, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(32, activation='relu')\n",
        "        self.output_layer = tf.keras.layers.Dense(action_dim, activation='softmax')\n",
        "\n",
        "    def call(self, state):\n",
        "        x = self.dense1(state)\n",
        "        x = self.dense2(x)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "# Critic Network\n",
        "class Critic(tf.keras.Model):\n",
        "    def __init__(self, state_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(32, activation='relu')\n",
        "        self.output_layer = tf.keras.layers.Dense(1, activation='linear')\n",
        "\n",
        "    def call(self, state):\n",
        "        x = self.dense1(state)\n",
        "        x = self.dense2(x)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "# Actor-Critic Agent\n",
        "class ActorCriticAgent:\n",
        "    def __init__(self, state_dim, action_dim, learning_rate=0.001, gamma=0.99):\n",
        "        self.actor = Actor(state_dim, action_dim)\n",
        "        self.critic = Critic(state_dim)\n",
        "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def train_step(self, state, action, reward, next_state, done):\n",
        "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
        "            # Actor forward pass\n",
        "            action_probs = self.actor(state, training=True)\n",
        "            chosen_action_prob = tf.reduce_sum(action_probs * action, axis=1, keepdims=True)\n",
        "\n",
        "            # Critic forward pass\n",
        "            state_value = self.critic(state, training=True)\n",
        "\n",
        "            # Calculate TD error\n",
        "            td_error = reward + (1 - done) * self.gamma * self.critic(next_state) - state_value\n",
        "\n",
        "            # Calculate actor and critic losses\n",
        "            actor_loss = -tf.math.log(chosen_action_prob) * td_error\n",
        "            critic_loss = tf.square(td_error)\n",
        "\n",
        "        # Update actor and critic parameters\n",
        "        actor_gradients = tape1.gradient(actor_loss, self.actor.trainable_variables)\n",
        "        critic_gradients = tape2.gradient(critic_loss, self.critic.trainable_variables)\n",
        "\n",
        "        self.actor_optimizer.apply_gradients(zip(actor_gradients, self.actor.trainable_variables))\n",
        "        self.critic_optimizer.apply_gradients(zip(critic_gradients, self.critic.trainable_variables))\n",
        "\n",
        "# Environment setup\n",
        "env = gym.make('CartPole-v1')\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "# Agent setup\n",
        "agent = ActorCriticAgent(state_dim, action_dim)\n",
        "\n",
        "# Training loop\n",
        "num_episodes = 10\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    episode_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Choose action using the policy from the Actor\n",
        "        action_probs = agent.actor(np.expand_dims(state, axis=0))\n",
        "        action = np.random.choice(action_dim, p=action_probs.numpy()[0])\n",
        "\n",
        "        # Take the chosen action and observe the next state and reward\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # Train the Actor-Critic agent\n",
        "        agent.train_step(np.expand_dims(state, axis=0), tf.one_hot(action, action_dim), reward, np.expand_dims(next_state, axis=0), done)\n",
        "\n",
        "        state = next_state\n",
        "        episode_reward += reward\n",
        "\n",
        "    # Print episode information\n",
        "    print(f\"Episode: {episode + 1}, Reward: {episode_reward}\")\n",
        "\n",
        "# Optionally, you can save the trained model\n",
        "# agent.actor.save_weights(\"actor_weights.h5\")\n",
        "# agent.critic.save_weights(\"critic_weights.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZhDOYlQV-0z",
        "outputId": "a92c6402-5dcb-4aed-b6ef-83bca4fb3490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1, Reward: 42.0\n",
            "Episode: 2, Reward: 45.0\n",
            "Episode: 3, Reward: 21.0\n",
            "Episode: 4, Reward: 52.0\n",
            "Episode: 5, Reward: 23.0\n",
            "Episode: 6, Reward: 56.0\n",
            "Episode: 7, Reward: 38.0\n",
            "Episode: 8, Reward: 12.0\n",
            "Episode: 9, Reward: 44.0\n",
            "Episode: 10, Reward: 14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "33)You are running a Money lending company, now there is a need to develop a learning engine which will advice you get maximum Profit investment through Probabilistic values of the historical data processing of the Customers, where Vanilla Policy Gradient is returning Lower values, so move onto the Advance Gradient policy Methods and decide which gives you the best output"
      ],
      "metadata": {
        "id": "lEwjLGrkWU3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Environment setup\n",
        "env = gym.make('CartPole-v1')  # Replace with the actual environment name\n",
        "\n",
        "# Neural Network Architecture\n",
        "class PolicyNetwork(tf.keras.Model):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(32, activation='relu')\n",
        "        self.output_layer = tf.keras.layers.Dense(action_dim, activation='softmax')\n",
        "\n",
        "    def call(self, state):\n",
        "        x = self.dense1(state)\n",
        "        x = self.dense2(x)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "class ValueNetwork(tf.keras.Model):\n",
        "    def __init__(self, state_dim):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(32, activation='relu')\n",
        "        self.output_layer = tf.keras.layers.Dense(1, activation='linear')\n",
        "\n",
        "    def call(self, state):\n",
        "        x = self.dense1(state)\n",
        "        x = self.dense2(x)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "# TRPO Implementation\n",
        "# TRPO Implementation\n",
        "class TRPOAgent:\n",
        "    def __init__(self, state_dim, action_dim, policy_lr=0.001, value_lr=0.001, gamma=0.99, max_kl=0.01):\n",
        "        self.policy_network = PolicyNetwork(state_dim, action_dim)\n",
        "        self.value_network = ValueNetwork(state_dim)\n",
        "        self.policy_optimizer = tf.keras.optimizers.Adam(learning_rate=policy_lr)\n",
        "        self.value_optimizer = tf.keras.optimizers.Adam(learning_rate=value_lr)\n",
        "        self.gamma = gamma\n",
        "        self.max_kl = max_kl\n",
        "\n",
        "    def compute_discounted_returns(self, rewards):\n",
        "        discounted_returns = []\n",
        "        running_add = 0\n",
        "        for r in rewards[::-1]:\n",
        "            running_add = running_add * self.gamma + r\n",
        "            discounted_returns.insert(0, running_add)\n",
        "        return discounted_returns\n",
        "\n",
        "    def surrogate_loss(self, states, actions, advantages):\n",
        "        policy_probs = self.policy_network(states)\n",
        "        chosen_action_probs = tf.reduce_sum(policy_probs * tf.cast(actions, dtype=tf.float32), axis=1)\n",
        "        old_policy_probs = tf.stop_gradient(chosen_action_probs)\n",
        "\n",
        "        advantages = (advantages - tf.reduce_mean(advantages)) / (tf.math.reduce_std(advantages) + 1e-8)\n",
        "\n",
        "        # Explicitly cast to float32\n",
        "        chosen_action_probs = tf.cast(chosen_action_probs, dtype=tf.float32)\n",
        "        old_policy_probs = tf.cast(old_policy_probs, dtype=tf.float32)\n",
        "\n",
        "        ratio = chosen_action_probs / old_policy_probs\n",
        "        surrogate1 = ratio * advantages\n",
        "        surrogate2 = tf.clip_by_value(ratio, 1 - self.max_kl, 1 + self.max_kl) * advantages\n",
        "\n",
        "        return -tf.reduce_mean(tf.minimum(surrogate1, surrogate2))\n",
        "\n",
        "    def train_policy(self, states, actions, advantages):\n",
        "        # Convert advantages to float32\n",
        "        advantages = tf.convert_to_tensor(advantages, dtype=tf.float32)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self.surrogate_loss(states, actions, advantages)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.policy_network.trainable_variables)\n",
        "        self.policy_optimizer.apply_gradients(zip(gradients, self.policy_network.trainable_variables))\n",
        "\n",
        "    def train_value(self, states, returns):\n",
        "        with tf.GradientTape() as tape:\n",
        "            values = self.value_network(states)\n",
        "            value_loss = tf.reduce_mean(tf.square(returns - values))\n",
        "\n",
        "        gradients = tape.gradient(value_loss, self.value_network.trainable_variables)\n",
        "        self.value_optimizer.apply_gradients(zip(gradients, self.value_network.trainable_variables))\n",
        "\n",
        "    def compute_profit(self, rewards):\n",
        "        return np.sum(rewards)\n",
        "\n",
        "# PPO Implementation\n",
        "class PPOAgent:\n",
        "    def __init__(self, state_dim, action_dim, policy_lr=0.001, value_lr=0.001, gamma=0.99, clip_ratio=0.2, epochs=10):\n",
        "        self.policy_network = PolicyNetwork(state_dim, action_dim)\n",
        "        self.value_network = ValueNetwork(state_dim)\n",
        "        self.policy_optimizer = tf.keras.optimizers.Adam(learning_rate=policy_lr)\n",
        "        self.value_optimizer = tf.keras.optimizers.Adam(learning_rate=value_lr)\n",
        "        self.gamma = gamma\n",
        "        self.clip_ratio = clip_ratio\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def compute_discounted_returns(self, rewards):\n",
        "        discounted_returns = []\n",
        "        running_add = 0\n",
        "        for r in rewards[::-1]:\n",
        "            running_add = running_add * self.gamma + r\n",
        "            discounted_returns.insert(0, running_add)\n",
        "        return discounted_returns\n",
        "\n",
        "    def surrogate_loss(self, states, actions, advantages, old_policy_probs):\n",
        "        policy_probs = self.policy_network(states)\n",
        "        chosen_action_probs = tf.reduce_sum(policy_probs * tf.cast(actions, dtype=tf.float32), axis=1)\n",
        "\n",
        "        ratio = chosen_action_probs / old_policy_probs\n",
        "        surrogate1 = ratio * advantages\n",
        "        surrogate2 = tf.clip_by_value(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages\n",
        "\n",
        "        return -tf.reduce_mean(tf.minimum(surrogate1, surrogate2))\n",
        "\n",
        "    def train_policy(self, states, actions, advantages):\n",
        "        # Convert advantages to float32\n",
        "        advantages = tf.convert_to_tensor(advantages, dtype=tf.float32)\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            with tf.GradientTape() as tape:\n",
        "                policy_probs = self.policy_network(states)\n",
        "                old_policy_probs = tf.stop_gradient(tf.reduce_sum(policy_probs * tf.cast(actions, dtype=tf.float32), axis=1))\n",
        "                loss = self.surrogate_loss(states, actions, advantages, old_policy_probs)\n",
        "\n",
        "            gradients = tape.gradient(loss, self.policy_network.trainable_variables)\n",
        "            self.policy_optimizer.apply_gradients(zip(gradients, self.policy_network.trainable_variables))\n",
        "\n",
        "    def train_value(self, states, returns):\n",
        "        with tf.GradientTape() as tape:\n",
        "            values = self.value_network(states)\n",
        "            value_loss = tf.reduce_mean(tf.square(returns - values))\n",
        "\n",
        "        gradients = tape.gradient(value_loss, self.value_network.trainable_variables)\n",
        "        self.value_optimizer.apply_gradients(zip(gradients, self.value_network.trainable_variables))\n",
        "\n",
        "    def compute_profit(self, rewards):\n",
        "        return np.sum(rewards)\n",
        "\n",
        "# Training loop\n",
        "# Training loop\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "vpg_agent = TRPOAgent(state_dim, action_dim)\n",
        "ppo_agent = PPOAgent(state_dim, action_dim)\n",
        "\n",
        "num_episodes = 10\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    states, actions, rewards_vpg, rewards_ppo = [], [], [], []\n",
        "    state = env.reset()\n",
        "\n",
        "    while True:\n",
        "        # VPG\n",
        "        action_probs_vpg = vpg_agent.policy_network(np.expand_dims(state, axis=0))\n",
        "        action_vpg = np.random.choice(action_dim, p=action_probs_vpg.numpy()[0])\n",
        "\n",
        "        # PPO\n",
        "        action_probs_ppo = ppo_agent.policy_network(np.expand_dims(state, axis=0))\n",
        "        action_ppo = np.random.choice(action_dim, p=action_probs_ppo.numpy()[0])\n",
        "\n",
        "        next_state, reward_vpg, done, _ = env.step(action_vpg)\n",
        "        _, reward_ppo, _, _ = env.step(action_ppo)\n",
        "\n",
        "        states.append(state)\n",
        "        actions.append(tf.one_hot(action_vpg, action_dim))\n",
        "        rewards_vpg.append(reward_vpg)\n",
        "        rewards_ppo.append(reward_ppo)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            # VPG\n",
        "            discounted_returns_vpg = vpg_agent.compute_discounted_returns(rewards_vpg)\n",
        "            advantages_vpg = discounted_returns_vpg - vpg_agent.value_network(np.vstack(states)).numpy().flatten()\n",
        "\n",
        "            vpg_agent.train_value(np.vstack(states), discounted_returns_vpg)\n",
        "            vpg_agent.train_policy(np.vstack(states), tf.convert_to_tensor(actions, dtype=tf.float32), advantages_vpg)\n",
        "\n",
        "            # PPO\n",
        "            discounted_returns_ppo = ppo_agent.compute_discounted_returns(rewards_ppo)\n",
        "            advantages_ppo = discounted_returns_ppo - ppo_agent.value_network(np.vstack(states)).numpy().flatten()\n",
        "\n",
        "            ppo_agent.train_value(np.vstack(states), discounted_returns_ppo)\n",
        "            ppo_agent.train_policy(np.vstack(states), tf.convert_to_tensor(actions, dtype=tf.float32), advantages_ppo)\n",
        "\n",
        "            # Performance Metrics\n",
        "            profit_vpg = vpg_agent.compute_profit(rewards_vpg)\n",
        "            profit_ppo = ppo_agent.compute_profit(rewards_ppo)\n",
        "\n",
        "            print(f\"Episode: {episode + 1}/{num_episodes}\")\n",
        "            print(f\"VPG Profit: {profit_vpg}\")\n",
        "            print(f\"PPO Profit: {profit_ppo}\")\n",
        "\n",
        "            # Learning Curves\n",
        "            episode_range = range(len(rewards_vpg))  # Use the correct range\n",
        "            plt.plot(episode_range, [vpg_agent.compute_profit(r) for r in rewards_vpg], label='VPG')\n",
        "            plt.plot(episode_range, [ppo_agent.compute_profit(r) for r in rewards_ppo], label='PPO')\n",
        "            plt.xlabel('Episode')\n",
        "            plt.ylabel('Profit')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "            # Justification\n",
        "            if profit_ppo > profit_vpg:\n",
        "                print(\"PPO outperformed VPG. Consider moving forward with PPO.\")\n",
        "            else:\n",
        "                print(\"VPG outperformed PPO. Consider further investigation or parameter tuning.\")\n",
        "\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xoVRnWB6WWGy",
        "outputId": "cc7d644f-8815-453b-f1fa-65b95d008d71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/classic_control/cartpole.py:163: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1/10\n",
            "VPG Profit: 16.0\n",
            "PPO Profit: 15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VPG outperformed PPO. Consider further investigation or parameter tuning.\n",
            "Episode: 2/10\n",
            "VPG Profit: 5.0\n",
            "PPO Profit: 5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VPG outperformed PPO. Consider further investigation or parameter tuning.\n",
            "Episode: 3/10\n",
            "VPG Profit: 9.0\n",
            "PPO Profit: 8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VPG outperformed PPO. Consider further investigation or parameter tuning.\n",
            "Episode: 4/10\n",
            "VPG Profit: 8.0\n",
            "PPO Profit: 8.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VPG outperformed PPO. Consider further investigation or parameter tuning.\n",
            "Episode: 5/10\n",
            "VPG Profit: 10.0\n",
            "PPO Profit: 9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VPG outperformed PPO. Consider further investigation or parameter tuning.\n",
            "Episode: 6/10\n",
            "VPG Profit: 10.0\n",
            "PPO Profit: 9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VPG outperformed PPO. Consider further investigation or parameter tuning.\n",
            "Episode: 7/10\n",
            "VPG Profit: 12.0\n",
            "PPO Profit: 12.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VPG outperformed PPO. Consider further investigation or parameter tuning.\n",
            "Episode: 8/10\n",
            "VPG Profit: 7.0\n",
            "PPO Profit: 6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VPG outperformed PPO. Consider further investigation or parameter tuning.\n",
            "Episode: 9/10\n",
            "VPG Profit: 14.0\n",
            "PPO Profit: 13.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VPG outperformed PPO. Consider further investigation or parameter tuning.\n",
            "Episode: 10/10\n",
            "VPG Profit: 10.0\n",
            "PPO Profit: 9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VPG outperformed PPO. Consider further investigation or parameter tuning.\n"
          ]
        }
      ]
    }
  ]
}